# Configuration for Phase 1: Pretrained VQ-VAE experimentation

# Inherit from base and override specific values

vqvae:
  source: "pretrained_hf"
  checkpoint: "dalle-mini/vqgan_imagenet_f16_16384"
  codebook_size: 16384
  downsample_factor: 16

transformer:
  hidden_size: 512  # Smaller for experimentation
  num_layers: 12
  num_heads: 8
  dropout: 0.1
  max_seq_len: 256

text_encoder:
  model_name: "openai/clip-vit-base-patch32"
  max_length: 77
  hidden_size: 512

training:
  batch_size: 16  # Smaller for single GPU experiments
  learning_rate: 3.0e-4
  warmup_steps: 500
  max_steps: 10000
  gradient_accumulation_steps: 2
  mixed_precision: "fp16"
  
  checkpoint_interval: 1000
  eval_interval: 500
  log_interval: 50
  
  cfg_dropout: 0.1

data:
  dataset: "flickr30k"
  image_size: 256
  num_workers: 4

mlflow:
  experiment_name: "latent-gpt-pretrained-vqvae"
  tags:
    phase: "1"
    resolution: "256"
    conditioning: "text_conditional"
    vqvae_source: "pretrained_hf"
    experiment_type: "small_scale"

output_dir: "./outputs/pretrained_experiments"
seed: 42
