# Base configuration for LatentGPT training

# VQ-VAE configuration
vqvae:
  source: "pretrained_hf"  # pretrained_hf, pretrained_taming, or custom
  checkpoint: "dalle-mini/vqgan_imagenet_f16_16384"
  codebook_size: 16384
  downsample_factor: 16

# Transformer configuration
transformer:
  hidden_size: 1024
  num_layers: 24
  num_heads: 16
  ffn_dim: null  # Default: 4 * hidden_size
  dropout: 0.1
  max_seq_len: 256  # 16x16 for 256x256 images with f=16

# Text encoder configuration
text_encoder:
  model_name: "openai/clip-vit-base-patch32"
  max_length: 77
  hidden_size: 512

# Training configuration
training:
  batch_size: 32
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_steps: 1000
  max_steps: 100000
  gradient_accumulation_steps: 1
  mixed_precision: "bf16"
  
  # Checkpointing
  checkpoint_interval: 5000
  eval_interval: 1000
  log_interval: 100
  
  # Classifier-free guidance
  cfg_dropout: 0.1

# Data configuration
data:
  dataset: "flickr30k"
  image_size: 256
  num_workers: 4
  cache_dir: null  # Uses default

# MLflow configuration
mlflow:
  experiment_name: "latent-gpt-pretrained-vqvae"
  tracking_uri: "http://127.0.0.1:5000"
  run_name: null  # Auto-generated
  tags:
    phase: "1"
    resolution: "256"
    conditioning: "text_conditional"
    vqvae_source: "pretrained_hf"

# Paths
output_dir: "./outputs"
seed: 42
