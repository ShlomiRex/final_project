{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d201d27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import urllib\n",
    "import numpy as np\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e269c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5ce8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_sample_image()-> PIL.Image.Image:\n",
    "    url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTZmJy3aSZ1Ix573d2MlJXQowLCLQyIUsPdniOJ7rBsgG4XJb04g9ZFA9MhxYvckeKkVmo&usqp=CAU'\n",
    "    filename = 'racoon.jpg'\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    return PIL.Image.open(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba00d1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_noise_distribution(noise, predicted_noise):\n",
    "    plt.hist(noise.cpu().numpy().flatten(), density = True, alpha = 0.8, label = \"ground truth noise\")\n",
    "    plt.hist(predicted_noise.cpu().numpy().flatten(), density = True, alpha = 0.8, label = \"predicted noise\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937289f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_noise_prediction(noise, predicted_noise):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    f, ax = plt.subplots(1, 2, figsize = (5,5))\n",
    "    ax[0].imshow(reverse_transform(noise))\n",
    "    ax[0].set_title(f\"ground truth noise\", fontsize = 10)\n",
    "    ax[1].imshow(reverse_transform(predicted_noise))\n",
    "    ax[1].set_title(f\"predicted noise\", fontsize = 10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b031430",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class DiffusionModel:\n",
    "    def __init__(self, start_schedule=0.0001, end_schedule=0.02, timesteps = 300):\n",
    "        self.start_schedule = start_schedule\n",
    "        self.end_schedule = end_schedule\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        \"\"\"\n",
    "        if \n",
    "            betas = [0.1, 0.2, 0.3, ...]\n",
    "        then\n",
    "            alphas = [0.9, 0.8, 0.7, ...]\n",
    "            alphas_cumprod = [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n",
    "            \n",
    "        \n",
    "        \"\"\" \n",
    "        self.betas = torch.linspace(start_schedule, end_schedule, timesteps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "        \n",
    "    def forward(self, x_0, t, device):\n",
    "        \"\"\"\n",
    "        x_0: (B, C, H, W)\n",
    "        t: (B,)\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x_0)\n",
    "        sqrt_alphas_cumprod_t = self.get_index_from_list(self.alphas_cumprod.sqrt(), t, x_0.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t, x_0.shape)\n",
    "            \n",
    "        mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n",
    "        variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n",
    "        \n",
    "        return mean + variance, noise.to(device)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def backward(self, x, t, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Calls the model to predict the noise in the image and returns \n",
    "        the denoised image. \n",
    "        Applies noise to this image, if we are not in the last step yet.\n",
    "        \"\"\"\n",
    "        betas_t = self.get_index_from_list(self.betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t, x.shape)\n",
    "        sqrt_recip_alphas_t = self.get_index_from_list(torch.sqrt(1.0 / self.alphas), t, x.shape)\n",
    "        mean = sqrt_recip_alphas_t * (x - betas_t * model(x, t, **kwargs) / sqrt_one_minus_alphas_cumprod_t)\n",
    "        posterior_variance_t = betas_t\n",
    "\n",
    "        if t == 0:\n",
    "            return mean\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "            variance = torch.sqrt(posterior_variance_t) * noise \n",
    "            return mean + variance\n",
    "\n",
    "    @staticmethod\n",
    "    def get_index_from_list(values, t, x_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        \"\"\"\n",
    "        pick the values from vals\n",
    "        according to the indices stored in `t`\n",
    "        \"\"\"\n",
    "        result = values.gather(-1, t.cpu())\n",
    "        \"\"\"\n",
    "        if \n",
    "        x_shape = (5, 3, 64, 64)\n",
    "            -> len(x_shape) = 4\n",
    "            -> len(x_shape) - 1 = 3\n",
    "            \n",
    "        and thus we reshape `out` to dims\n",
    "        (batch_size, 1, 1, 1)\n",
    "        \n",
    "        \"\"\"\n",
    "        return result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498796b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb68152",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SHAPE), # Resize the input image\n",
    "    transforms.ToTensor(), # Convert to torch tensor (scales data into [0,1])\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1), # Scale data between [-1, 1] \n",
    "])\n",
    "\n",
    "\n",
    "reverse_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda t: (t + 1) / 2), # Scale data between [0,1]\n",
    "    transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "    transforms.Lambda(lambda t: t * 255.), # Scale data between [0.,255.]\n",
    "    transforms.Lambda(lambda t: t.cpu().numpy().astype(np.uint8)), # Convert into an uint8 numpy array\n",
    "    transforms.ToPILImage(), # Convert to PIL image\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d3f02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pil_image = get_sample_image()\n",
    "torch_image = transform(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a905d73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diffusion_model = DiffusionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba3c9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NO_DISPLAY_IMAGES = 5\n",
    "torch_image_batch = torch.stack([torch_image] * NO_DISPLAY_IMAGES)\n",
    "t = torch.linspace(0, diffusion_model.timesteps - 1, NO_DISPLAY_IMAGES).long()\n",
    "noisy_image_batch, _ = diffusion_model.forward(torch_image_batch, t, device)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "f, ax = plt.subplots(1, NO_DISPLAY_IMAGES, figsize = (100,100))\n",
    "\n",
    "for idx, image in enumerate(noisy_image_batch):\n",
    "    ax[idx].imshow(reverse_transform(image))\n",
    "    ax[idx].set_title(f\"Iteration: {t[idx].item()}\", fontsize = 100)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dfe406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d6453",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, time_embedding_dims, labels, num_filters = 3, downsample=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_embedding_dims = time_embedding_dims\n",
    "        self.time_embedding = SinusoidalPositionEmbeddings(time_embedding_dims)\n",
    "        self.labels = labels\n",
    "        if labels:\n",
    "            self.label_mlp = nn.Linear(1, channels_out)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        \n",
    "        if downsample:\n",
    "            self.conv1 = nn.Conv2d(channels_in, channels_out, num_filters, padding=1)\n",
    "            self.final = nn.Conv2d(channels_out, channels_out, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(2 * channels_in, channels_out, num_filters, padding=1)\n",
    "            self.final = nn.ConvTranspose2d(channels_out, channels_out, 4, 2, 1)\n",
    "            \n",
    "        self.bnorm1 = nn.BatchNorm2d(channels_out)\n",
    "        self.bnorm2 = nn.BatchNorm2d(channels_out)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channels_out, channels_out, 3, padding=1)\n",
    "        self.time_mlp = nn.Linear(time_embedding_dims, channels_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t, **kwargs):\n",
    "        o = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        o_time = self.relu(self.time_mlp(self.time_embedding(t)))\n",
    "        o = o + o_time[(..., ) + (None, ) * 2]\n",
    "        if self.labels:\n",
    "            label = kwargs.get('labels')\n",
    "            o_label = self.relu(self.label_mlp(label))\n",
    "            o = o + o_label[(..., ) + (None, ) * 2]\n",
    "            \n",
    "        o = self.bnorm2(self.relu(self.conv2(o)))\n",
    "\n",
    "        return self.final(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec1c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, img_channels = 3, time_embedding_dims = 128, labels = False, sequence_channels = (64, 128, 256, 512, 1024)):\n",
    "        super().__init__()\n",
    "        self.time_embedding_dims = time_embedding_dims\n",
    "        sequence_channels_rev = reversed(sequence_channels)\n",
    "        \n",
    "        self.downsampling = nn.ModuleList([Block(channels_in, channels_out, time_embedding_dims, labels) for channels_in, channels_out in zip(sequence_channels, sequence_channels[1:])])\n",
    "        self.upsampling = nn.ModuleList([Block(channels_in, channels_out, time_embedding_dims, labels,downsample=False) for channels_in, channels_out in zip(sequence_channels[::-1], sequence_channels[::-1][1:])])\n",
    "        self.conv1 = nn.Conv2d(img_channels, sequence_channels[0], 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(sequence_channels[0], img_channels, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x, t, **kwargs):\n",
    "        residuals = []\n",
    "        o = self.conv1(x)\n",
    "        for ds in self.downsampling:\n",
    "            o = ds(o, t, **kwargs)\n",
    "            residuals.append(o)\n",
    "        for us, res in zip(self.upsampling, reversed(residuals)):\n",
    "            o = us(torch.cat((o, res), dim=1), t, **kwargs)\n",
    "            \n",
    "        return self.conv2(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e9738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NO_EPOCHS = 2000\n",
    "PRINT_FREQUENCY = 400\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = True\n",
    "\n",
    "unet = UNet(labels=False)\n",
    "unet.to(device)\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae4872",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(NO_EPOCHS):\n",
    "    mean_epoch_loss = []\n",
    "    \n",
    "    batch = torch.stack([torch_image] * BATCH_SIZE)\n",
    "    t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,)).long().to(device)\n",
    "\n",
    "    batch_noisy, noise = diffusion_model.forward(batch, t, device) \n",
    "    predicted_noise = unet(batch_noisy, t)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.nn.functional.mse_loss(noise, predicted_noise) \n",
    "    mean_epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % PRINT_FREQUENCY == 0:\n",
    "        print('---')\n",
    "        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)}\")\n",
    "        if VERBOSE:\n",
    "            with torch.no_grad():\n",
    "                plot_noise_prediction(noise[0], predicted_noise[0])\n",
    "                plot_noise_distribution(noise, predicted_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb8524",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    img = torch.randn((1, 3) + IMAGE_SHAPE).to(device)\n",
    "    for i in reversed(range(diffusion_model.timesteps)):\n",
    "        t = torch.full((1,), i, dtype=torch.long, device=device)\n",
    "        img = diffusion_model.backward(img, t, unet.eval())\n",
    "        if i % 50 == 0:\n",
    "            plt.figure(figsize=(2,2))\n",
    "            plt.imshow(reverse_transform(img[0]))\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d1f78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NO_EPOCHS = 100\n",
    "PRINT_FREQUENCY = 10\n",
    "LR = 0.001\n",
    "VERBOSE = False\n",
    "\n",
    "unet = UNet(labels=True)\n",
    "unet.to(device)\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb4d2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, drop_last=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be6b824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(NO_EPOCHS):\n",
    "    mean_epoch_loss = []\n",
    "    mean_epoch_loss_val = []\n",
    "    for batch, label in trainloader:\n",
    "        t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,)).long().to(device)\n",
    "        batch = batch.to(device)\n",
    "        batch_noisy, noise = diffusion_model.forward(batch, t, device) \n",
    "        predicted_noise = unet(batch_noisy, t, labels = label.reshape(-1,1).float().to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.nn.functional.mse_loss(noise, predicted_noise) \n",
    "        mean_epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    for batch, label in testloader:\n",
    "\n",
    "        t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,)).long().to(device)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        batch_noisy, noise = diffusion_model.forward(batch, t, device) \n",
    "        predicted_noise = unet(batch_noisy, t, labels = label.reshape(-1,1).float().to(device))\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(noise, predicted_noise) \n",
    "        mean_epoch_loss_val.append(loss.item())\n",
    "\n",
    "    if epoch % PRINT_FREQUENCY == 0:\n",
    "        print('---')\n",
    "        print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val Loss {np.mean(mean_epoch_loss_val)}\")\n",
    "        if VERBOSE:\n",
    "            with torch.no_grad():\n",
    "                plot_noise_prediction(noise[0], predicted_noise[0])\n",
    "                plot_noise_distribution(noise, predicted_noise)\n",
    "                \n",
    "        torch.save(unet.state_dict(), f\"epoch: {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(labels=True)\n",
    "unet.load_state_dict(torch.load((\"epoch: 80\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b893eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5cb534",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(classes)\n",
    "NUM_DISPLAY_IMAGES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa26b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(16)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "f, ax = plt.subplots(NUM_CLASSES, NUM_DISPLAY_IMAGES, figsize = (100,100))\n",
    "\n",
    "for c in range(NUM_CLASSES):\n",
    "    imgs = torch.randn((NUM_DISPLAY_IMAGES, 3) + IMAGE_SHAPE).to(device)\n",
    "    for i in reversed(range(diffusion_model.timesteps)):\n",
    "        t = torch.full((1,), i, dtype=torch.long, device=device)\n",
    "        labels = torch.tensor([c] * NUM_DISPLAY_IMAGES).resize(NUM_DISPLAY_IMAGES, 1).float().to(device)\n",
    "        imgs = diffusion_model.backward(x=imgs, t=t, model=unet.eval().to(device), labels = labels)\n",
    "    for idx, img in enumerate(imgs):\n",
    "        ax[c][idx].imshow(reverse_transform(img))\n",
    "        ax[c][idx].set_title(f\"Class: {classes[c]}\", fontsize = 100)\n",
    "        \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-conditioned-image-generation-using-st-35DVCAXA-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
