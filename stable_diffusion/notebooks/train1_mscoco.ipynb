{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42fc9d31",
   "metadata": {},
   "source": [
    "# ðŸ”§ System Diagnostic - Run This First!\n",
    "\n",
    "Before training, we need to verify that the GPU is properly accessible. This cell will check:\n",
    "1. Slurm job allocation\n",
    "2. GPU hardware detection\n",
    "3. CUDA environment variables\n",
    "4. PyTorch CUDA compatibility\n",
    "5. Common issues and solutions\n",
    "\n",
    "**Run the diagnostic cell below FIRST before proceeding with training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0488f15a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " ðŸ” COMPREHENSIVE GPU DIAGNOSTIC FOR HPC CLUSTER\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£  SLURM JOB ALLOCATION\n",
      "--------------------------------------------------------------------------------\n",
      "  Job ID                   : 3032684\n",
      "  Assigned Node(s)         : gpu8\n",
      "  Node ID                  : 0\n",
      "  Total GPUs Allocated     : NOT SET\n",
      "  GPUs on This Node        : 2\n",
      "  GPU IDs Allocated        : 0,7\n",
      "  CPUs on Node             : 2\n",
      "  Memory per Node          : 16384\n",
      "\n",
      "  âœ… Slurm has allocated GPU(s) to this job\n",
      "\n",
      "2ï¸âƒ£  CUDA ENVIRONMENT VARIABLES\n",
      "--------------------------------------------------------------------------------\n",
      "  CUDA_VISIBLE_DEVICES     : 0,1\n",
      "  CUDA_HOME                : /prefix/software/CUDA/11.8.0\n",
      "  CUDA_PATH                : /prefix/software/CUDA/11.8.0\n",
      "  CUDA_ROOT                : /prefix/software/CUDA/11.8.0\n",
      "  LD_LIBRARY_PATH          : /prefix/software/CUDA/11.8.0/nvvm/lib64 (and 2 more)\n",
      "\n",
      "  âœ… CUDA_VISIBLE_DEVICES is set\n",
      "\n",
      "3ï¸âƒ£  GPU HARDWARE DETECTION (nvidia-smi)\n",
      "--------------------------------------------------------------------------------\n",
      "index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "0, NVIDIA A100 80GB PCIe, 470.161.03, 80994 MiB, 0 MiB, 80994 MiB\n",
      "1, NVIDIA A100 80GB PCIe, 470.161.03, 80994 MiB, 0 MiB, 80994 MiB\n",
      "\n",
      "  âœ… GPU hardware detected successfully\n",
      "\n",
      "4ï¸âƒ£  CUDA TOOLKIT VERSION\n",
      "--------------------------------------------------------------------------------\n",
      "  System CUDA: Cuda compilation tools, release 11.8, V11.8.89\n",
      "  CUDA Version: 11.8\n",
      "\n",
      "5ï¸âƒ£  PYTORCH CUDA DETECTION\n",
      "--------------------------------------------------------------------------------\n",
      "  PyTorch Version: 2.7.1+cu118\n",
      "  PyTorch Built with CUDA: 11.8\n",
      "  CUDA Available: True\n",
      "  CUDA Device Count: 2\n",
      "    GPU 0: NVIDIA A100 80GB PCIe\n",
      "      Total Memory: 79.10 GB\n",
      "    GPU 1: NVIDIA A100 80GB PCIe\n",
      "      Total Memory: 79.10 GB\n",
      "\n",
      "  âœ… PyTorch can access GPU(s)!\n",
      "\n",
      "================================================================================\n",
      " ðŸ“‹ SUMMARY & RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "  âœ… Slurm GPU Allocation\n",
      "  âœ… CUDA Environment\n",
      "  âœ… GPU Hardware (nvidia-smi)\n",
      "  âœ… PyTorch CUDA Access\n",
      "\n",
      "ðŸŽ‰ ALL CHECKS PASSED! GPU is ready for training.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive GPU Diagnostic for HPC Cluster\n",
    "This cell diagnoses why PyTorch might show \"device: cpu\" even on GPU nodes\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" ðŸ” COMPREHENSIVE GPU DIAGNOSTIC FOR HPC CLUSTER\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# ===========================================================================\n",
    "# 1. SLURM JOB INFORMATION\n",
    "# ===========================================================================\n",
    "print(\"1ï¸âƒ£  SLURM JOB ALLOCATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "slurm_vars = {\n",
    "    'SLURM_JOB_ID': 'Job ID',\n",
    "    'SLURM_JOB_NODELIST': 'Assigned Node(s)',\n",
    "    'SLURM_NODEID': 'Node ID',\n",
    "    'SLURM_GPUS': 'Total GPUs Allocated',\n",
    "    'SLURM_GPUS_ON_NODE': 'GPUs on This Node',\n",
    "    'SLURM_JOB_GPUS': 'GPU IDs Allocated',\n",
    "    'SLURM_CPUS_ON_NODE': 'CPUs on Node',\n",
    "    'SLURM_MEM_PER_NODE': 'Memory per Node',\n",
    "}\n",
    "\n",
    "slurm_allocated = False\n",
    "for var, desc in slurm_vars.items():\n",
    "    value = os.environ.get(var, 'NOT SET')\n",
    "    print(f\"  {desc:25s}: {value}\")\n",
    "    if var in ['SLURM_GPUS', 'SLURM_GPUS_ON_NODE', 'SLURM_JOB_GPUS']:\n",
    "        if value != 'NOT SET' and value != '0' and value != '':\n",
    "            slurm_allocated = True\n",
    "\n",
    "print()\n",
    "if slurm_allocated:\n",
    "    print(\"  âœ… Slurm has allocated GPU(s) to this job\")\n",
    "else:\n",
    "    print(\"  âš ï¸  WARNING: No GPU allocation detected by Slurm!\")\n",
    "    print(\"     This job may not have requested GPU resources.\")\n",
    "    print()\n",
    "\n",
    "# ===========================================================================\n",
    "# 2. CUDA ENVIRONMENT VARIABLES\n",
    "# ===========================================================================\n",
    "print()\n",
    "print(\"2ï¸âƒ£  CUDA ENVIRONMENT VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "cuda_vars = {\n",
    "    'CUDA_VISIBLE_DEVICES': 'Which GPUs are visible to CUDA',\n",
    "    'CUDA_HOME': 'CUDA installation directory',\n",
    "    'CUDA_PATH': 'CUDA path',\n",
    "    'CUDA_ROOT': 'CUDA root directory',\n",
    "    'LD_LIBRARY_PATH': 'Library path (includes CUDA libs)',\n",
    "}\n",
    "\n",
    "cuda_env_ok = False\n",
    "for var, desc in cuda_vars.items():\n",
    "    value = os.environ.get(var, 'NOT SET')\n",
    "    if var == 'LD_LIBRARY_PATH' and value != 'NOT SET':\n",
    "        # Show only CUDA-related parts\n",
    "        cuda_parts = [p for p in value.split(':') if 'cuda' in p.lower() or 'CUDA' in p]\n",
    "        if cuda_parts:\n",
    "            print(f\"  {var:25s}: {cuda_parts[0]} (and {len(cuda_parts)-1} more)\")\n",
    "        else:\n",
    "            print(f\"  {var:25s}: (no CUDA paths found)\")\n",
    "    else:\n",
    "        print(f\"  {var:25s}: {value}\")\n",
    "    \n",
    "    if var == 'CUDA_VISIBLE_DEVICES' and value != 'NOT SET':\n",
    "        cuda_env_ok = True\n",
    "\n",
    "print()\n",
    "if cuda_env_ok:\n",
    "    print(\"  âœ… CUDA_VISIBLE_DEVICES is set\")\n",
    "else:\n",
    "    print(\"  âš ï¸  WARNING: CUDA_VISIBLE_DEVICES not set!\")\n",
    "    print(\"     GPUs may not be visible to applications.\")\n",
    "    print()\n",
    "\n",
    "# ===========================================================================\n",
    "# 3. GPU HARDWARE DETECTION (nvidia-smi)\n",
    "# ===========================================================================\n",
    "print()\n",
    "print(\"3ï¸âƒ£  GPU HARDWARE DETECTION (nvidia-smi)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free',\n",
    "         '--format=csv'],\n",
    "        capture_output=True, text=True, timeout=5\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout)\n",
    "        print(\"  âœ… GPU hardware detected successfully\")\n",
    "        hardware_ok = True\n",
    "    else:\n",
    "        print(f\"  âŒ nvidia-smi failed with error:\\n{result.stderr}\")\n",
    "        hardware_ok = False\n",
    "except FileNotFoundError:\n",
    "    print(\"  âŒ nvidia-smi command not found!\")\n",
    "    print(\"     GPU drivers may not be installed.\")\n",
    "    hardware_ok = False\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Error running nvidia-smi: {e}\")\n",
    "    hardware_ok = False\n",
    "\n",
    "print()\n",
    "\n",
    "# ===========================================================================\n",
    "# 4. CUDA TOOLKIT VERSION\n",
    "# ===========================================================================\n",
    "print(\"4ï¸âƒ£  CUDA TOOLKIT VERSION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'release' in line.lower():\n",
    "                print(f\"  System CUDA: {line.strip()}\")\n",
    "                # Extract version number\n",
    "                import re\n",
    "                match = re.search(r'release (\\d+\\.\\d+)', line)\n",
    "                if match:\n",
    "                    cuda_version = match.group(1)\n",
    "                    print(f\"  CUDA Version: {cuda_version}\")\n",
    "    else:\n",
    "        print(\"  âš ï¸  nvcc not found (CUDA toolkit may not be in PATH)\")\n",
    "except Exception as e:\n",
    "    print(f\"  âš ï¸  Could not determine CUDA version: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ===========================================================================\n",
    "# 5. PYTORCH CUDA DETECTION\n",
    "# ===========================================================================\n",
    "print(\"5ï¸âƒ£  PYTORCH CUDA DETECTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"  PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"  PyTorch Built with CUDA: {torch.version.cuda}\")\n",
    "    print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"    GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"      Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print()\n",
    "        print(\"  âœ… PyTorch can access GPU(s)!\")\n",
    "        pytorch_ok = True\n",
    "    else:\n",
    "        print()\n",
    "        print(\"  âŒ PyTorch CANNOT access GPU!\")\n",
    "        pytorch_ok = False\n",
    "        \n",
    "        # Diagnose why\n",
    "        print()\n",
    "        print(\"  ðŸ” DIAGNOSIS:\")\n",
    "        \n",
    "        # Check CUDA version mismatch\n",
    "        if torch.version.cuda:\n",
    "            pytorch_cuda = torch.version.cuda\n",
    "            print(f\"     - PyTorch was built for CUDA {pytorch_cuda}\")\n",
    "            \n",
    "            # Try to get system CUDA version\n",
    "            try:\n",
    "                nvcc_result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=5)\n",
    "                if nvcc_result.returncode == 0:\n",
    "                    import re\n",
    "                    match = re.search(r'release (\\d+\\.\\d+)', nvcc_result.stdout)\n",
    "                    if match:\n",
    "                        system_cuda = match.group(1)\n",
    "                        print(f\"     - System has CUDA {system_cuda}\")\n",
    "                        \n",
    "                        # Compare major versions\n",
    "                        pytorch_major = pytorch_cuda.split('.')[0]\n",
    "                        system_major = system_cuda.split('.')[0]\n",
    "                        \n",
    "                        if pytorch_major != system_major:\n",
    "                            print()\n",
    "                            print(f\"     âš ï¸  CUDA VERSION MISMATCH!\")\n",
    "                            print(f\"         PyTorch needs CUDA {pytorch_major}.x\")\n",
    "                            print(f\"         System has CUDA {system_major}.x\")\n",
    "                            print()\n",
    "                            print(f\"     ðŸ’¡ SOLUTION: Reinstall PyTorch with CUDA {system_major}.x support\")\n",
    "                            if system_major == '11':\n",
    "                                print(f\"         pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "                            elif system_major == '12':\n",
    "                                print(f\"         pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Check if CUDA_VISIBLE_DEVICES is set\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES') == '':\n",
    "            print(\"     - CUDA_VISIBLE_DEVICES is empty (no GPUs visible)\")\n",
    "        elif os.environ.get('CUDA_VISIBLE_DEVICES') is None:\n",
    "            print(\"     - CUDA_VISIBLE_DEVICES is not set\")\n",
    "        \n",
    "        # Check if Slurm allocated GPU\n",
    "        if not slurm_allocated:\n",
    "            print(\"     - Slurm did not allocate GPU to this job\")\n",
    "            print(\"       Did you request GPU with --gres=gpu:1?\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"  âŒ PyTorch is not installed!\")\n",
    "    pytorch_ok = False\n",
    "\n",
    "print()\n",
    "\n",
    "# ===========================================================================\n",
    "# 6. SUMMARY & RECOMMENDATIONS\n",
    "# ===========================================================================\n",
    "print(\"=\"*80)\n",
    "print(\" ðŸ“‹ SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "all_checks = {\n",
    "    'Slurm GPU Allocation': slurm_allocated,\n",
    "    'CUDA Environment': cuda_env_ok,\n",
    "    'GPU Hardware (nvidia-smi)': hardware_ok,\n",
    "    'PyTorch CUDA Access': pytorch_ok,\n",
    "}\n",
    "\n",
    "for check, status in all_checks.items():\n",
    "    status_icon = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"  {status_icon} {check}\")\n",
    "\n",
    "print()\n",
    "\n",
    "if all(all_checks.values()):\n",
    "    print(\"ðŸŽ‰ ALL CHECKS PASSED! GPU is ready for training.\")\n",
    "else:\n",
    "    print(\"âš ï¸  ISSUES DETECTED. Review the diagnostic output above.\")\n",
    "    print()\n",
    "    print(\"Common solutions:\")\n",
    "    print()\n",
    "    print(\"1. If 'Slurm GPU Allocation' failed:\")\n",
    "    print(\"   - Make sure you started Jupyter with: bash slurm/start_jupyter.sh gpu7\")\n",
    "    print(\"   - Check job allocation: squeue -u $USER\")\n",
    "    print()\n",
    "    print(\"2. If 'PyTorch CUDA Access' failed but hardware is OK:\")\n",
    "    print(\"   - Most likely CUDA version mismatch\")\n",
    "    print(\"   - Run the PyTorch reinstall cell below\")\n",
    "    print()\n",
    "    print(\"3. If 'GPU Hardware' failed:\")\n",
    "    print(\"   - Job may be on a CPU-only node\")\n",
    "    print(\"   - Cancel and restart with: bash slurm/start_jupyter.sh gpu7\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484027a",
   "metadata": {},
   "source": [
    "# Train Conditional UNet2D on MS COCO with Text Captions + VAE\n",
    "\n",
    "This notebook trains a **text-conditional** diffusion model on MS COCO using:\n",
    "- **Pretrained VAE** (AutoencoderKL from Stable Diffusion) to encode images to latents\n",
    "- **Pretrained CLIP** text encoder for text embeddings\n",
    "- **UNet2DConditionModel** (with cross-attention for text conditioning)\n",
    "- **DDPM scheduler** for training and inference\n",
    "\n",
    "## Key Design Choices\n",
    "- MS COCO images resized to 256x256\n",
    "- Images normalized to [-1,1] for VAE compatibility\n",
    "- VAE encodes to 32x32x4 latents\n",
    "- CLIP encodes text captions to 77x768 embeddings\n",
    "- Only the UNet is trained; VAE and CLIP are frozen\n",
    "- **Text-conditional generation** - generate images from text prompts\n",
    "\n",
    "## Memory Optimizations for GPU\n",
    "- **Batch size**: 16 (adjustable based on GPU memory)\n",
    "- **Image size**: 256x256\n",
    "- **Mixed precision**: Enabled (FP16)\n",
    "- **Cache clearing**: Periodic GPU cache clearing to prevent fragmentation\n",
    "- **DataLoader**: num_workers=2, pin_memory=True for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434c71c",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ Flickr8k Text-to-Image Conditional Diffusion Training\n",
    "\n",
    "## About Flickr8k Dataset\n",
    "\n",
    "1. **Dataset**: Flickr8k - 8,000 images from Flickr\n",
    "2. **Captions**: 5 human-written captions per image (40,000 total captions)\n",
    "3. **Content**: Diverse everyday scenes with people, animals, objects, and activities\n",
    "4. **Image Complexity**: Natural photographs with varied subjects and backgrounds\n",
    "5. **Text Descriptions**: Detailed natural language descriptions of image content\n",
    "6. **Download**: Automatically loaded via HuggingFace datasets\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Run the diagnostic cell to verify GPU access\n",
    "2. Configure training parameters (adjust batch size for your GPU)\n",
    "3. Start training - checkpoints and sample images saved every 500 steps\n",
    "4. Monitor progress through text-to-image samples (with captions displayed!)\n",
    "5. Generate images from custom text prompts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef2508",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e9d64",
   "metadata": {},
   "source": [
    "## Memory Usage Tips\n",
    "\n",
    "If you still encounter OOM errors, try these additional optimizations:\n",
    "\n",
    "1. **Further reduce batch size**: Change `batch_size` to 8 or even 4\n",
    "2. **Reduce image size**: Change `image_size` to 64 (will train faster but lower quality)\n",
    "3. **Enable gradient checkpointing** (uncomment in UNet creation if available)\n",
    "4. **Close other applications** that might be using GPU memory\n",
    "5. **Restart the kernel** to clear any lingering memory allocations\n",
    "\n",
    "Current settings should work on most 8GB GPUs with CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c68263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doshlom4/work/conda/envs/torch114/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLIPTextModel, CLIPTokenizer\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# TensorBoard SummaryWriter\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Accelerate for multi-GPU training\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n",
      "File \u001b[0;32m~/work/conda/envs/torch114/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorboard\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_vendor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensorboard, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m Version(\n\u001b[1;32m      5\u001b[0m     tensorboard\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m      6\u001b[0m ) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.15\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.models import AutoencoderKL\n",
    "from diffusers.models.unets.unet_2d_condition import UNet2DConditionModel\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# TensorBoard SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Accelerate for multi-GPU training\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Metrics: FID and CLIP Score\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.multimodal.clip_score import CLIPScore\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0149ad",
   "metadata": {},
   "source": [
    "## IMPORTANT: CUDA Setup Check\n",
    "\n",
    "If you see \"Device: cpu\" in the output above, it means PyTorch cannot access the GPU. This is usually because:\n",
    "\n",
    "1. **CUDA version mismatch**: Your PyTorch is built for CUDA 12.x but the cluster has CUDA 11.x\n",
    "2. **PyTorch not installed with CUDA support**\n",
    "\n",
    "Run the diagnostic cell below to check and fix this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3055dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Diagnostic and Fix\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU DIAGNOSTIC CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check PyTorch version and CUDA support\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        print(\"\\nâœ… GPU is working correctly!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  PyTorch cannot access GPU!\")\n",
    "        \n",
    "        # Check if nvidia-smi works\n",
    "        try:\n",
    "            result = subprocess.run(['nvidia-smi', '--query-gpu=name,driver_version,memory.total', \n",
    "                                   '--format=csv,noheader'], \n",
    "                                  capture_output=True, text=True, timeout=5)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"\\nGPU hardware detected:\")\n",
    "                print(f\"  {result.stdout.strip()}\")\n",
    "                \n",
    "                # Check CUDA version on system\n",
    "                cuda_result = subprocess.run(['nvcc', '--version'], \n",
    "                                           capture_output=True, text=True, timeout=5)\n",
    "                if cuda_result.returncode == 0:\n",
    "                    print(f\"\\nSystem CUDA:\")\n",
    "                    for line in cuda_result.stdout.split('\\n'):\n",
    "                        if 'release' in line.lower():\n",
    "                            print(f\"  {line.strip()}\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(\"SOLUTION: Reinstall PyTorch with correct CUDA version\")\n",
    "                print(\"=\"*70)\n",
    "                print(\"\\nYour PyTorch was built for a different CUDA version than\")\n",
    "                print(\"what's available on this system.\")\n",
    "                print(\"\\nTo fix this, run the following command:\")\n",
    "                print(\"\\n  pip uninstall torch torchvision torchaudio -y\")\n",
    "                print(\"  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "                print(\"\\nThis will install PyTorch with CUDA 11.8 support (matching your system).\")\n",
    "                print(\"\\nWould you like to automatically fix this? (This will take a few minutes)\")\n",
    "                print(\"If yes, run the cell below.\")\n",
    "            else:\n",
    "                print(\"\\nâŒ GPU hardware not detected by nvidia-smi\")\n",
    "                print(\"This job might not have GPU allocated!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError checking GPU: {e}\")\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"âŒ PyTorch is not installed!\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88700b47",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e388bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def build_transforms(image_size: int = 256) -> transforms.Compose:\n",
    "    \"\"\"Transform Flickr8k images: resize to square, normalize to [-1,1] for VAE.\"\"\"\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def encode_text(text: str, tokenizer, text_encoder, device):\n",
    "    \"\"\"Encode text prompt into embeddings using CLIP.\"\"\"\n",
    "    text_inputs = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_input_ids)[0]\n",
    "    \n",
    "    return text_embeddings\n",
    "\n",
    "\n",
    "def compute_fid_score(real_images, generated_images, device):\n",
    "    \"\"\"\n",
    "    Compute FID score between real and generated images.\n",
    "    \n",
    "    Args:\n",
    "        real_images: Tensor of real images (B, C, H, W) in range [0, 1]\n",
    "        generated_images: Tensor of generated images (B, C, H, W) in range [0, 1]\n",
    "        device: Device to run computation on\n",
    "    \n",
    "    Returns:\n",
    "        FID score as float\n",
    "    \"\"\"\n",
    "    fid = FrechetInceptionDistance(normalize=True).to(device)\n",
    "    \n",
    "    # Convert to uint8 [0, 255] format expected by FID and move to device\n",
    "    real_images_uint8 = (real_images * 255).clamp(0, 255).to(torch.uint8).to(device)\n",
    "    generated_images_uint8 = (generated_images * 255).clamp(0, 255).to(torch.uint8).to(device)\n",
    "    \n",
    "    # Update FID with real and fake images\n",
    "    fid.update(real_images_uint8, real=True)\n",
    "    fid.update(generated_images_uint8, real=False)\n",
    "    \n",
    "    # Compute FID score\n",
    "    fid_score = fid.compute()\n",
    "    \n",
    "    return fid_score.item()\n",
    "\n",
    "\n",
    "def compute_clip_score(images, prompts, device):\n",
    "    \"\"\"\n",
    "    Compute CLIP score between images and their text prompts.\n",
    "    \n",
    "    Args:\n",
    "        images: Tensor of images (B, C, H, W) in range [0, 1]\n",
    "        prompts: List of text prompts\n",
    "        device: Device to run computation on\n",
    "    \n",
    "    Returns:\n",
    "        CLIP score as float\n",
    "    \"\"\"\n",
    "    clip_score_fn = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch32\").to(device)\n",
    "    \n",
    "    # Convert to uint8 [0, 255] format and move to device\n",
    "    images_uint8 = (images * 255).clamp(0, 255).to(torch.uint8).to(device)\n",
    "    \n",
    "    # Compute CLIP score\n",
    "    score = clip_score_fn(images_uint8, prompts)\n",
    "    \n",
    "    return score.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378122a1",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    dataset_root: str\n",
    "    output_dir: str = \"./outputs/train1_flickr8k_text2img\"\n",
    "    batch_size: int = 16\n",
    "    num_epochs: int = 999999  # Run indefinitely until manually stopped\n",
    "    lr: float = 1e-4\n",
    "    num_train_timesteps: int = 1000\n",
    "    image_size: int = 256\n",
    "    seed: int = 42\n",
    "    mixed_precision: bool = True\n",
    "    checkpoint_interval: int = 500  # Save checkpoint every N steps\n",
    "    # UNet size for A100 GPU (scaled up from ~166M to ~400-500M parameters)\n",
    "    # 5 levels: 128 -> 256 -> 512 -> 512 -> 768 channels\n",
    "    unet_block_out_channels: tuple[int, ...] = (128, 256, 512, 512, 768)\n",
    "    layers_per_block: int = 3  # Increased from 2 to 3 for deeper model\n",
    "    # Text conditioning\n",
    "    classifier_free_guidance_prob: float = 0.1  # 10% unconditional training for CFG\n",
    "\n",
    "\n",
    "# Configure training parameters - OPTIMIZED FOR FLICKR8K TEXT-TO-IMAGE\n",
    "config = TrainConfig(\n",
    "    dataset_root=\"../../datasets\",\n",
    "    output_dir=\"./outputs/train1_flickr8k_text2img\",\n",
    "    batch_size=16,  # Adjust based on GPU memory\n",
    "    num_epochs=999999,  # Run indefinitely until manually stopped\n",
    "    lr=1e-4,\n",
    "    image_size=256,  # Full resolution for high-quality images\n",
    "    mixed_precision=True,  # Enable mixed precision to save memory\n",
    "    checkpoint_interval=500,  # Save checkpoint every 500 steps\n",
    "    classifier_free_guidance_prob=0.1,  # Enable classifier-free guidance\n",
    ")\n",
    "\n",
    "print(f\"Device: {get_device()}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Epochs: Unlimited (will run until manually stopped)\")\n",
    "print(f\"Learning rate: {config.lr}\")\n",
    "print(f\"Image size: {config.image_size}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"Checkpoint interval: {config.checkpoint_interval} steps\")\n",
    "print(f\"Classifier-free guidance: {config.classifier_free_guidance_prob * 100}% unconditional\")\n",
    "print(f\"\\nDataset: Flickr8k (Text-Conditional Image Generation)\")\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter with unique timestamped run name\n",
    "from datetime import datetime\n",
    "run_name = datetime.now().strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "tb_log_dir = os.path.join(config.output_dir, \"tensorboard\", run_name)\n",
    "writer = SummaryWriter(log_dir=tb_log_dir)\n",
    "print(f\"TensorBoard logs will be written to: {tb_log_dir}\")\n",
    "print(f\"Run name: {run_name}\")\n",
    "\n",
    "# Initialize Accelerator for multi-GPU training\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=\"fp16\" if config.mixed_precision else \"no\",\n",
    "    log_with=\"tensorboard\",\n",
    "    project_dir=config.output_dir,\n",
    ")\n",
    "print(f\"\\nðŸš€ Accelerator initialized:\")\n",
    "print(f\"  Device: {accelerator.device}\")\n",
    "print(f\"  Number of processes: {accelerator.num_processes}\")\n",
    "print(f\"  Mixed precision: {accelerator.mixed_precision}\")\n",
    "print(f\"  Is main process: {accelerator.is_main_process}\")\n",
    "print(f\"  Process index: {accelerator.process_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db2cba",
   "metadata": {},
   "source": [
    "## 4. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(device: torch.device, config: TrainConfig):\n",
    "    # Pretrained VAE and CLIP text encoder/tokenizer\n",
    "    print(\"Loading pretrained VAE...\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
    "    vae.requires_grad_(False)\n",
    "    vae.eval()\n",
    "    vae.to(device)\n",
    "\n",
    "    print(\"Loading CLIP text encoder...\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    text_encoder.requires_grad_(False)\n",
    "    text_encoder.eval()\n",
    "    text_encoder.to(device)\n",
    "\n",
    "    print(\"Creating UNet2DConditionModel...\")\n",
    "    # Conditional UNet operating in latent space (4 channels)\n",
    "    # Model sized for A100 GPU: 5-level architecture with 3 layers per block\n",
    "    # Expected ~400-500M parameters for better A100 utilization\n",
    "    unet = UNet2DConditionModel(\n",
    "        sample_size=config.image_size // 8,  # 32 for 256x256\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        layers_per_block=config.layers_per_block,\n",
    "        block_out_channels=config.unet_block_out_channels,\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",              # 128 channels, no attention\n",
    "            \"CrossAttnDownBlock2D\",     # 256 channels, text cross-attention\n",
    "            \"CrossAttnDownBlock2D\",     # 512 channels, text cross-attention\n",
    "            \"CrossAttnDownBlock2D\",     # 512 channels, text cross-attention\n",
    "            \"CrossAttnDownBlock2D\",     # 768 channels, text cross-attention (new level)\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"CrossAttnUpBlock2D\",       # 768 channels, text cross-attention\n",
    "            \"CrossAttnUpBlock2D\",       # 512 channels, text cross-attention\n",
    "            \"CrossAttnUpBlock2D\",       # 512 channels, text cross-attention\n",
    "            \"CrossAttnUpBlock2D\",       # 256 channels, text cross-attention\n",
    "            \"UpBlock2D\",                # 128 channels, no attention\n",
    "        ),\n",
    "        cross_attention_dim=512,  # CLIP ViT-B/32 hidden size\n",
    "    ).to(device)\n",
    "\n",
    "    num_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "    print(f\"UNet trainable parameters: {num_params:,}\")\n",
    "\n",
    "    return vae, tokenizer, text_encoder, unet\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "device = accelerator.device\n",
    "vae, tokenizer, text_encoder, unet = create_models(device, config)\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292e237",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23366d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(config: TrainConfig) -> DataLoader:\n",
    "    \"\"\"Load Flickr8k dataset with captions using HuggingFace datasets.\"\"\"\n",
    "    import random\n",
    "    \n",
    "    # Use /home/doshlom4/work for cache as requested\n",
    "    cache_dir = \"/home/doshlom4/work/huggingface_cache\"\n",
    "    tmp_dir = \"/home/doshlom4/work/tmp\"\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "    \n",
    "    # Set ALL relevant environment variables BEFORE importing datasets\n",
    "    os.environ['HF_HOME'] = cache_dir\n",
    "    os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "    os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "    os.environ['TMPDIR'] = tmp_dir\n",
    "    os.environ['TEMP'] = tmp_dir\n",
    "    os.environ['TMP'] = tmp_dir\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“ DATASET CACHE CONFIGURATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Cache directory: {cache_dir}\")\n",
    "    print(f\"Dataset: Flickr8k\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    from datasets import load_dataset\n",
    "    from torch.utils.data import Dataset\n",
    "    \n",
    "    print(\"\\nLoading Flickr8k dataset...\")\n",
    "    \n",
    "    # Try Flickr30k first (superset of Flickr8k), then fall back to alternatives\n",
    "    dataset_candidates = [\n",
    "        (\"nlphuji/flickr30k\", \"test\"),  # Smaller test split first\n",
    "        (\"nlphuji/flickr30k\", \"train\"), # Full train split\n",
    "    ]\n",
    "    \n",
    "    ds = None\n",
    "    for dataset_name, split_name in dataset_candidates:\n",
    "        try:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Attempting to load: {dataset_name} (split={split_name})\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # CRITICAL: trust_remote_code=False to avoid deprecated loading scripts\n",
    "            ds = load_dataset(\n",
    "                dataset_name, \n",
    "                split=split_name, \n",
    "                cache_dir=cache_dir,\n",
    "                streaming=False,\n",
    "                trust_remote_code=False  # This prevents the \"Dataset scripts are no longer supported\" error\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… Successfully loaded {dataset_name}\")\n",
    "            print(f\"âœ… Dataset has {len(ds)} examples\")\n",
    "            \n",
    "            # Print first example structure\n",
    "            if len(ds) > 0:\n",
    "                example = ds[0]\n",
    "                print(f\"\\nðŸ“‹ Dataset structure:\")\n",
    "                print(f\"Keys: {list(example.keys())}\")\n",
    "            \n",
    "            print(\"=\"*80)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Failed to load {dataset_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if ds is None:\n",
    "        raise RuntimeError(\"Could not load Flickr8k/Flickr30k dataset\")\n",
    "    \n",
    "    tfms = build_transforms(config.image_size)\n",
    "    \n",
    "    # Dataset wrapper for Flickr with text captions\n",
    "    class FlickrTextImageDataset(Dataset):\n",
    "        def __init__(self, hf_dataset, transform, cfg_prob=0.1):\n",
    "            self.dataset = hf_dataset\n",
    "            self.transform = transform\n",
    "            self.cfg_prob = cfg_prob\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            max_retries = 5\n",
    "            for retry in range(max_retries):\n",
    "                try:\n",
    "                    example = self.dataset[idx % len(self.dataset)]\n",
    "                    \n",
    "                    # Get image\n",
    "                    image = example.get('image', None) or example.get('img', None)\n",
    "                    if image is None:\n",
    "                        idx = (idx + 1) % len(self.dataset)\n",
    "                        continue\n",
    "                        \n",
    "                    if image.mode != 'RGB':\n",
    "                        image = image.convert('RGB')\n",
    "                    image = self.transform(image)\n",
    "                    \n",
    "                    # Get caption\n",
    "                    caption = \"\"\n",
    "                    if 'caption' in example:\n",
    "                        captions = example['caption']\n",
    "                        if isinstance(captions, list) and len(captions) > 0:\n",
    "                            caption = random.choice(captions)\n",
    "                        elif isinstance(captions, str):\n",
    "                            caption = captions\n",
    "                    elif 'captions' in example:\n",
    "                        captions = example['captions']\n",
    "                        if isinstance(captions, list) and len(captions) > 0:\n",
    "                            caption = random.choice(captions)\n",
    "                    elif 'sentences' in example:\n",
    "                        sentences = example['sentences']\n",
    "                        if isinstance(sentences, list) and len(sentences) > 0:\n",
    "                            sent = random.choice(sentences)\n",
    "                            if isinstance(sent, dict):\n",
    "                                caption = sent.get('raw', '') or sent.get('text', '')\n",
    "                            else:\n",
    "                                caption = str(sent)\n",
    "                    \n",
    "                    if not caption:\n",
    "                        caption = \"A photo\"\n",
    "                    \n",
    "                    # For classifier-free guidance: randomly drop text\n",
    "                    if random.random() < self.cfg_prob:\n",
    "                        caption = \"\"\n",
    "                    \n",
    "                    return {'image': image, 'caption': caption}\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading example {idx}: {e}, retrying...\")\n",
    "                    idx = (idx + 1) % len(self.dataset)\n",
    "                    if retry == max_retries - 1:\n",
    "                        raise\n",
    "    \n",
    "    dataset = FlickrTextImageDataset(ds, tfms, config.classifier_free_guidance_prob)\n",
    "    \n",
    "    print(f\"\\nâœ… DataLoader created successfully\")\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    print(f\"Batch size: {config.batch_size}\")\n",
    "    print(f\"Number of batches per epoch: {len(dataset) // config.batch_size}\")\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create dataloader\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING FLICKR8K DATASET\")\n",
    "print(\"=\"*80)\n",
    "dataloader = make_dataloader(config)\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… Dataset loaded successfully\")\n",
    "print(f\"Dataset size: {len(dataloader.dataset)}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6ca12",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61af30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch with captions\n",
    "print(\"Loading sample batch...\")\n",
    "sample_batch = next(iter(dataloader))\n",
    "sample_images = sample_batch['image']\n",
    "sample_captions = sample_batch['caption']\n",
    "\n",
    "print(f\"Batch shape: {sample_images.shape}\")\n",
    "print(f\"\\nSample Flickr8k captions:\")\n",
    "for i, caption in enumerate(sample_captions[:4]):\n",
    "    print(f\"  {i+1}. {caption}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(18, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(sample_images):\n",
    "        img = sample_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img + 1) / 2  # Denormalize from [-1,1] to [0,1]\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        # Add caption as title (truncated)\n",
    "        if i < len(sample_captions):\n",
    "            caption = sample_captions[i][:50] + \"...\" if len(sample_captions[i]) > 50 else sample_captions[i]\n",
    "            ax.set_title(caption, fontsize=7, wrap=True)\n",
    "            \n",
    "plt.suptitle(\"Sample Flickr8k Images with Captions\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del sample_batch, sample_images, sample_captions\n",
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc55e4",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909cd5a",
   "metadata": {},
   "source": [
    "## 7. Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10bb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(output_dir: str):\n",
    "    \"\"\"Find the latest checkpoint in the output directory.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        return None\n",
    "    \n",
    "    # Look for checkpoint files\n",
    "    checkpoint_files = []\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.endswith('.pt') and (filename.startswith('unet_step_') or filename.startswith('unet_epoch_')):\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            checkpoint_files.append(filepath)\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    # Get the most recently modified checkpoint\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)\n",
    "    return latest_checkpoint\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path: str, unet, optimizer=None):\n",
    "    \"\"\"Load checkpoint and return metadata.\"\"\"\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # Handle different checkpoint formats\n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'unet' in checkpoint:\n",
    "            unet.load_state_dict(checkpoint['unet'])\n",
    "        else:\n",
    "            unet.load_state_dict(checkpoint)\n",
    "        \n",
    "        # Load optimizer state if available\n",
    "        if optimizer is not None and 'optimizer' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = {\n",
    "            'global_step': checkpoint.get('global_step', checkpoint.get('step', 0)),\n",
    "            'epoch': checkpoint.get('epoch', 0),\n",
    "            'batch_losses': checkpoint.get('batch_losses', []),\n",
    "            'epoch_losses': checkpoint.get('epoch_losses', []),\n",
    "        }\n",
    "    else:\n",
    "        unet.load_state_dict(checkpoint)\n",
    "        metadata = {'global_step': 0, 'epoch': 0, 'batch_losses': [], 'epoch_losses': []}\n",
    "    \n",
    "    print(f\"Resumed from step {metadata['global_step']}, epoch {metadata['epoch']}\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str, unet, optimizer, global_step: int, epoch: int, \n",
    "                   batch_losses: List[float], epoch_losses: List[float]):\n",
    "    \"\"\"Save checkpoint with complete metadata.\"\"\"\n",
    "    checkpoint = {\n",
    "        'unet': unet.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'global_step': global_step,\n",
    "        'epoch': epoch,\n",
    "        'batch_losses': batch_losses,\n",
    "        'epoch_losses': epoch_losses,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "# Check for existing checkpoints\n",
    "latest_checkpoint = find_latest_checkpoint(config.output_dir)\n",
    "if latest_checkpoint:\n",
    "    print(f\"Found existing checkpoint: {latest_checkpoint}\")\n",
    "    print(\"Training will resume from this checkpoint.\")\n",
    "else:\n",
    "    print(\"No existing checkpoints found. Starting training from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ae37e",
   "metadata": {},
   "source": [
    "### Checkpoint System Features\n",
    "\n",
    "The checkpoint system provides:\n",
    "\n",
    "1. **Automatic Resume**: Detects and loads the latest checkpoint automatically\n",
    "2. **Complete Metadata**: Each checkpoint stores:\n",
    "   - UNet model weights\n",
    "   - Optimizer state (for proper resume)\n",
    "   - Global step count\n",
    "   - Current epoch number\n",
    "   - Complete batch loss history\n",
    "   - Complete epoch loss history\n",
    "3. **Training Plots**: Loss plots saved with every checkpoint\n",
    "4. **Sample Images**: Generated samples (all 10 CIFAR10 classes) saved at each checkpoint to visualize training progress\n",
    "5. **Multiple Checkpoint Types**:\n",
    "   - Step checkpoints: `unet_step_2000.pt` (every 2000 steps)\n",
    "   - Epoch checkpoints: `unet_epoch_1.pt` (after each epoch)\n",
    "   - Final checkpoint: `unet_final.pt` (at completion)\n",
    "   - Sample images: `samples_step_2000.png`, `samples_epoch_1.png`, `samples_final.png`\n",
    "\n",
    "If training is interrupted, simply re-run the training cell and it will automatically resume from the latest checkpoint!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876740c9",
   "metadata": {},
   "source": [
    "### ðŸ“Š Metrics Tracking\n",
    "\n",
    "At every checkpoint, the following metrics are computed and logged to TensorBoard:\n",
    "\n",
    "1. **FID (FrÃ©chet Inception Distance)**:\n",
    "   - Measures the quality and diversity of generated images\n",
    "   - Lower is better (0 = perfect match to real distribution)\n",
    "   - Compares generated images to a batch of real images from the dataset\n",
    "   - Standard metric for evaluating generative models\n",
    "\n",
    "2. **CLIP Score**:\n",
    "   - Measures how well generated images match their text prompts\n",
    "   - Higher is better (max ~100)\n",
    "   - Uses pretrained CLIP model to compare image-text alignment\n",
    "   - Ensures text-conditioning is working properly\n",
    "\n",
    "Both metrics are automatically computed at:\n",
    "- Every checkpoint interval (e.g., every 500 steps)\n",
    "- End of each epoch\n",
    "- Final checkpoint\n",
    "\n",
    "View metrics in TensorBoard under the \"Metrics\" tab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_checkpoint_samples(\n",
    "    unet, vae, text_encoder, tokenizer, device, config, \n",
    "    writer=None, global_step=None, num_samples: int = 16, num_inference_steps: int = 50,\n",
    "    real_images_batch=None\n",
    "):\n",
    "    \"\"\"Generate sample images from text prompts during training checkpoints and log to TensorBoard.\n",
    "    Also computes FID and CLIP score metrics.\n",
    "    Displays each generated image with its caption.\n",
    "    \n",
    "    Args:\n",
    "        real_images_batch: Optional batch of real images for FID computation\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'fid_score' and 'clip_score' if computed, else None\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating {num_samples} text-to-image samples at step {global_step}...\")\n",
    "    \n",
    "    # Predefined interesting prompts for Flickr8k-style visualization\n",
    "    prompts = [\n",
    "        \"A dog running on the beach\",\n",
    "        \"A child playing with a ball\",\n",
    "        \"A person riding a bike\",\n",
    "        \"A cat sitting on a chair\",\n",
    "        \"People walking in a park\",\n",
    "        \"A bird in a tree\",\n",
    "        \"A man with a hat\",\n",
    "        \"A woman smiling\",\n",
    "        \"Children playing outside\",\n",
    "        \"A dog jumping in the air\",\n",
    "        \"A person holding an umbrella\",\n",
    "        \"A group of people standing\",\n",
    "        \"A child with a toy\",\n",
    "        \"A dog with a stick\",\n",
    "        \"A person near water\",\n",
    "        \"Two people talking\",\n",
    "    ]\n",
    "    \n",
    "    # Create scheduler for sampling\n",
    "    scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    # Set models to eval mode\n",
    "    unet.eval()\n",
    "    \n",
    "    # Create figure with more space for captions\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(24, 7))\n",
    "    \n",
    "    # Collect generated images and prompts for metrics\n",
    "    generated_images_list = []\n",
    "    used_prompts = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i >= num_samples:\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            # Get prompt and encode\n",
    "            prompt = prompts[i % len(prompts)]\n",
    "            used_prompts.append(prompt)\n",
    "            text_embeddings = encode_text(prompt, tokenizer, text_encoder, device)\n",
    "            \n",
    "            # Set seed for reproducibility\n",
    "            torch.manual_seed(42 + i)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(42 + i)\n",
    "            \n",
    "            # Init random latents\n",
    "            latents = torch.randn((1, 4, config.image_size // 8, config.image_size // 8), device=device)\n",
    "            \n",
    "            # Denoising loop with text conditioning\n",
    "            for t in scheduler.timesteps:\n",
    "                latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "                \n",
    "                # Predict noise with text conditioning\n",
    "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "                \n",
    "                # Step\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "            \n",
    "            # Decode latents to image\n",
    "            latents = latents / 0.18215\n",
    "            image = vae.decode(latents).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.detach().cpu()\n",
    "            \n",
    "            # Store for metrics\n",
    "            generated_images_list.append(image)\n",
    "            \n",
    "            # Display image with FULL prompt as title\n",
    "            img = image[0].permute(1, 2, 0).numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            # Wrap text for better display\n",
    "            ax.set_title(prompt, fontsize=9, wrap=True, pad=10)\n",
    "    \n",
    "    plt.suptitle(f\"Text-to-Image Samples at Step {global_step}\", fontsize=16, y=0.99)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])  # Make room for suptitle\n",
    "    \n",
    "    # Log as image to TensorBoard with slider support (same tag, different steps)\n",
    "    if writer is not None and global_step is not None:\n",
    "        # Convert figure to numpy array\n",
    "        fig.canvas.draw()\n",
    "        img_array = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
    "        img_array = img_array.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
    "        # Convert RGBA to RGB and then to CHW format for TensorBoard\n",
    "        img_array = img_array[:, :, :3]  # Drop alpha channel\n",
    "        img_array = img_array.transpose(2, 0, 1)\n",
    "        writer.add_image(\"samples/generated_images\", img_array, global_step=global_step)\n",
    "        writer.flush()\n",
    "    \n",
    "    plt.close(fig)\n",
    "    print(f\"âœ… Sample images with captions logged to TensorBoard at step {global_step}\")\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {}\n",
    "    try:\n",
    "        # Stack generated images\n",
    "        generated_images = torch.cat(generated_images_list, dim=0)  # (num_samples, C, H, W)\n",
    "        \n",
    "        # Compute CLIP Score\n",
    "        print(f\"Computing CLIP Score...\")\n",
    "        clip_score = compute_clip_score(generated_images, used_prompts, device)\n",
    "        metrics['clip_score'] = clip_score\n",
    "        print(f\"CLIP Score: {clip_score:.4f}\")\n",
    "        \n",
    "        # Compute FID if real images are provided\n",
    "        if real_images_batch is not None:\n",
    "            print(f\"Computing FID Score...\")\n",
    "            # Ensure real images are in [0, 1] range\n",
    "            real_images = real_images_batch\n",
    "            if real_images.min() < 0:\n",
    "                real_images = (real_images + 1) / 2  # Convert from [-1, 1] to [0, 1]\n",
    "            \n",
    "            # Resize real images to match generated images if needed\n",
    "            if real_images.shape[-1] != config.image_size:\n",
    "                real_images = torch.nn.functional.interpolate(\n",
    "                    real_images, size=(config.image_size, config.image_size), mode='bilinear'\n",
    "                )\n",
    "            \n",
    "            # Take only as many real images as generated\n",
    "            real_images = real_images[:num_samples].cpu()\n",
    "            \n",
    "            fid_score = compute_fid_score(real_images, generated_images, device)\n",
    "            metrics['fid_score'] = fid_score\n",
    "            print(f\"FID Score: {fid_score:.4f}\")\n",
    "        \n",
    "        # Log metrics to TensorBoard\n",
    "        if writer is not None and global_step is not None:\n",
    "            if 'clip_score' in metrics:\n",
    "                writer.add_scalar(\"metrics/clip_score\", metrics['clip_score'], global_step)\n",
    "            if 'fid_score' in metrics:\n",
    "                writer.add_scalar(\"metrics/fid_score\", metrics['fid_score'], global_step)\n",
    "            writer.flush()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compute metrics: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    \n",
    "\n",
    "    # Set UNet back to train mode    return metrics\n",
    "\n",
    "    unet.train()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ff223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: TrainConfig, vae, tokenizer, text_encoder, unet, dataloader, device, accelerator, resume_from_checkpoint: str = None):\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "    # Noise scheduler for training\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=config.num_train_timesteps, beta_schedule=\"squaredcos_cap_v2\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(unet.parameters(), lr=config.lr)\n",
    "    \n",
    "    # Prepare models, optimizer, and dataloader with Accelerator\n",
    "    # Note: VAE and text_encoder are frozen, so we don't prepare them\n",
    "    unet, optimizer, dataloader = accelerator.prepare(unet, optimizer, dataloader)\n",
    "\n",
    "    # Loss tracking and training state\n",
    "    epoch_losses = []\n",
    "    global_step = 0\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Resume from checkpoint if available\n",
    "    if resume_from_checkpoint:\n",
    "        metadata = load_checkpoint(resume_from_checkpoint, accelerator.unwrap_model(unet), optimizer)\n",
    "        global_step = metadata['global_step']\n",
    "        start_epoch = metadata['epoch']\n",
    "        epoch_losses = metadata['epoch_losses']\n",
    "        if accelerator.is_main_process:\n",
    "            print(f\"Resuming training from step {global_step}, epoch {start_epoch}\")\n",
    "\n",
    "    unet.train()\n",
    "    \n",
    "    # Store a batch of real images for FID computation during checkpoints\n",
    "    real_images_for_metrics = None\n",
    "    \n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        epoch_loss_sum = 0.0\n",
    "        epoch_batch_count = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\", disable=not accelerator.is_local_main_process)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Get images and captions from batch\n",
    "            images = batch['image']\n",
    "            captions = batch['caption']\n",
    "            \n",
    "            # Store first batch of real images for FID metrics (only on main process, only once)\n",
    "            if real_images_for_metrics is None and accelerator.is_main_process:\n",
    "                real_images_for_metrics = images.clone().cpu()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Encode images to latents using frozen VAE\n",
    "                latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "                \n",
    "                # Encode text prompts using frozen CLIP\n",
    "                text_embeddings = []\n",
    "                for caption in captions:\n",
    "                    emb = encode_text(caption, tokenizer, text_encoder, accelerator.device)\n",
    "                    text_embeddings.append(emb)\n",
    "                text_embeddings = torch.cat(text_embeddings, dim=0)\n",
    "\n",
    "            # Sample noise and timestep; add noise\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=accelerator.device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Predict noise with text conditioning\n",
    "            # Accelerator handles mixed precision automatically\n",
    "            with accelerator.autocast():\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=text_embeddings).sample\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            # Backward pass with Accelerator\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Record loss\n",
    "            loss_value = loss.item()\n",
    "            epoch_loss_sum += loss_value\n",
    "            epoch_batch_count += 1\n",
    "\n",
    "            global_step += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{loss_value:.4f}\", \"step\": global_step})\n",
    "\n",
    "            # Clear cache every 50 batches to prevent fragmentation\n",
    "            if batch_idx % 50 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Save periodic checkpoints (only on main process)\n",
    "            if global_step % config.checkpoint_interval == 0 and accelerator.is_main_process:\n",
    "                ckpt_path = os.path.join(config.output_dir, f\"unet_step_{global_step}.pt\")\n",
    "                save_checkpoint(ckpt_path, accelerator.unwrap_model(unet), optimizer, global_step, epoch, [], epoch_losses)\n",
    "                print(f\"\\nCheckpoint saved: {ckpt_path}\")\n",
    "                \n",
    "                # Log epoch loss to TensorBoard\n",
    "                if len(epoch_losses) > 0:\n",
    "                    writer.add_scalar(\"training/epoch_loss\", epoch_losses[-1], global_step)\n",
    "                    writer.flush()\n",
    "                \n",
    "                # Generate and log sample images to TensorBoard with metrics\n",
    "                try:\n",
    "                    # Use stored real images for FID computation\n",
    "                    generate_checkpoint_samples(\n",
    "                        accelerator.unwrap_model(unet), vae, text_encoder, tokenizer, accelerator.device, config,\n",
    "                        writer=writer, global_step=global_step, real_images_batch=real_images_for_metrics\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to generate/log sample images: {e}\")\n",
    "                \n",
    "                # Clear cache after sampling\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        # Record epoch average loss\n",
    "        avg_epoch_loss = epoch_loss_sum / epoch_batch_count if epoch_batch_count > 0 else 0.0\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        if accelerator.is_main_process:\n",
    "            print(f\"Epoch {epoch+1}/{config.num_epochs} - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # Save per-epoch checkpoint (only on main process)\n",
    "        if accelerator.is_main_process:\n",
    "            ckpt_path = os.path.join(config.output_dir, f\"unet_epoch_{epoch+1}.pt\")\n",
    "            save_checkpoint(ckpt_path, accelerator.unwrap_model(unet), optimizer, global_step, epoch + 1, [], epoch_losses)\n",
    "            \n",
    "            # Log epoch loss to TensorBoard\n",
    "            writer.add_scalar(\"training/epoch_loss\", avg_epoch_loss, global_step)\n",
    "            writer.flush()\n",
    "            \n",
    "            # Generate and log sample images at end of epoch with metrics\n",
    "            try:\n",
    "                generate_checkpoint_samples(\n",
    "                    accelerator.unwrap_model(unet), vae, text_encoder, tokenizer, accelerator.device, config,\n",
    "                    writer=writer, global_step=global_step, real_images_batch=real_images_for_metrics\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to generate/log sample images at epoch end: {e}\")\n",
    "        \n",
    "        # Clear cache after each epoch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Save final (only on main process)\n",
    "    if accelerator.is_main_process:\n",
    "        final_path = os.path.join(config.output_dir, \"unet_final.pt\")\n",
    "        save_checkpoint(final_path, accelerator.unwrap_model(unet), optimizer, global_step, config.num_epochs, [], epoch_losses)\n",
    "        print(f\"\\nFinal model saved: {final_path}\")\n",
    "        \n",
    "        # Generate final sample images and log to TensorBoard with metrics\n",
    "        try:\n",
    "            generate_checkpoint_samples(\n",
    "                accelerator.unwrap_model(unet), vae, text_encoder, tokenizer, accelerator.device, config,\n",
    "                writer=writer, global_step=global_step, real_images_batch=real_images_for_metrics\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate/log final sample images: {e}\")\n",
    "\n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43dd0c",
   "metadata": {},
   "source": [
    "## 9. Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1713a",
   "metadata": {},
   "source": [
    "## 8.5. Memory Optimization (Clear GPU Cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74842b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache before training to maximize available memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    # Set memory allocation configuration for better fragmentation handling\n",
    "    import os\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    # Print current memory status\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(\"Memory cache cleared and optimized for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoints and resume if available\n",
    "latest_checkpoint = find_latest_checkpoint(config.output_dir)\n",
    "\n",
    "# Train the model (will resume from checkpoint if found)\n",
    "epoch_losses = train(\n",
    "    config, vae, tokenizer, text_encoder, unet, dataloader, device, accelerator,\n",
    "    resume_from_checkpoint=latest_checkpoint\n",
    ")\n",
    "\n",
    "# Close TensorBoard writer to flush remaining logs\n",
    "try:\n",
    "    writer.close()\n",
    "    print(\"TensorBoard writer closed. Logs available at:\", tb_log_dir)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to close TensorBoard writer cleanly: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7bd06f",
   "metadata": {},
   "source": [
    "## 10. Visualize Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d54c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch losses are automatically logged to TensorBoard as scalars during training\n",
    "# View them in TensorBoard under the \"SCALARS\" tab -> \"training/epoch_loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db4313",
   "metadata": {},
   "source": [
    "## 11. Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d34a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(\n",
    "    prompt: str,\n",
    "    num_inference_steps: int = 50,\n",
    "    guidance_scale: float = 7.5,\n",
    "    seed: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"Generate an image from a text prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text description of the image to generate\n",
    "        num_inference_steps: Number of denoising steps\n",
    "        guidance_scale: Classifier-free guidance scale (higher = more prompt adherence)\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    # Encode text prompt\n",
    "    text_embeddings = encode_text(prompt, tokenizer, text_encoder, accelerator.device)\n",
    "    \n",
    "    # For classifier-free guidance, also encode empty prompt\n",
    "    if guidance_scale > 1.0:\n",
    "        uncond_embeddings = encode_text(\"\", tokenizer, text_encoder, accelerator.device)\n",
    "        # Concatenate for batch processing\n",
    "        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "    # Init random latents in latent space\n",
    "    latents = torch.randn((1, 4, config.image_size // 8, config.image_size // 8), device=accelerator.device)\n",
    "\n",
    "    unet.eval()\n",
    "    for t in tqdm(scheduler.timesteps, desc=f\"Generating '{prompt}'\"):\n",
    "        # Prepare latent input\n",
    "        latent_model_input = latents\n",
    "        if guidance_scale > 1.0:\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict noise with text conditioning\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # Perform classifier-free guidance\n",
    "        if guidance_scale > 1.0:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # Step\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    # Decode latents to image\n",
    "    latents = latents / 0.18215\n",
    "    image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu()\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f26d1",
   "metadata": {},
   "source": [
    "## 12. Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images from various Flickr8k-style text prompts\n",
    "test_prompts = [\n",
    "    \"A dog running on the beach\",\n",
    "    \"A child playing with a ball\",\n",
    "    \"A person riding a bike\",\n",
    "    \"A cat sitting on a chair\",\n",
    "    \"People walking in a park\",\n",
    "    \"A bird in a tree\",\n",
    "    \"A man with a hat\",\n",
    "    \"A woman smiling\",\n",
    "    \"Children playing outside\",\n",
    "    \"A dog jumping in the air\",\n",
    "    \"A person holding an umbrella\",\n",
    "    \"A group of people standing\",\n",
    "    \"A child with a toy\",\n",
    "    \"A dog with a stick\",\n",
    "    \"A person near water\",\n",
    "    \"Two people talking\",\n",
    "]\n",
    "\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 20))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(test_prompts):\n",
    "        prompt = test_prompts[i]\n",
    "        print(f\"Generating {i+1}/{len(test_prompts)}: '{prompt}'\")\n",
    "        generated_image = sample(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            seed=42 + i,\n",
    "        )\n",
    "        \n",
    "        # Display image with caption\n",
    "        img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        # Display full caption as title\n",
    "        ax.set_title(prompt, fontsize=10, wrap=True, pad=10)\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Text-to-Image Samples from Flickr8k Model\", fontsize=18, y=0.995)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "# Log figure to TensorBoard\n",
    "try:\n",
    "    writer.add_figure(\"samples/text_to_image_samples\", fig, 0)\n",
    "    writer.flush()\n",
    "    print(\"Text-to-image samples with captions logged to TensorBoard.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to log samples to TensorBoard: {e}\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7e582",
   "metadata": {},
   "source": [
    "## 13. Test Different Guidance Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different guidance scales for a single Flickr8k-style prompt\n",
    "test_prompt = \"A happy dog playing with a ball in a sunny park\"\n",
    "guidance_scales = [0, 3, 5, 7.5, 10, 15]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(guidance_scales), figsize=(24, 5))\n",
    "for ax, gs in zip(axes, guidance_scales):\n",
    "    print(f\"Generating with guidance scale {gs}\")\n",
    "    generated_image = sample(\n",
    "        prompt=test_prompt,\n",
    "        guidance_scale=gs,\n",
    "        num_inference_steps=50,\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Guidance Scale: {gs}\", fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Add the prompt as main title\n",
    "plt.suptitle(f'Prompt: \"{test_prompt}\"', fontsize=14, y=1.02)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "# Log guidance scale comparison to TensorBoard\n",
    "try:\n",
    "    writer.add_figure(\"samples/guidance_scale_comparison\", fig, 0)\n",
    "    writer.flush()\n",
    "    print(\"Guidance scale comparison with caption logged to TensorBoard.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to log guidance comparison: {e}\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7cd20",
   "metadata": {},
   "source": [
    "## 14. Save Individual Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and log a single high-quality sample from a custom Flickr8k-style prompt\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# CUSTOMIZE YOUR PROMPT HERE\n",
    "custom_prompt = \"A happy dog playing with a ball in a sunny park\"\n",
    "\n",
    "print(f\"Generating image for prompt: '{custom_prompt}'\")\n",
    "\n",
    "generated_image = sample(\n",
    "    prompt=custom_prompt,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Log to TensorBoard\n",
    "try:\n",
    "    writer.add_image(\"samples/custom\", generated_image[0], 0)\n",
    "    writer.flush()\n",
    "    print(\"Custom sample logged to TensorBoard.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to log custom sample: {e}\")\n",
    "\n",
    "# Display with caption\n",
    "plt.figure(figsize=(10, 10))\n",
    "img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "plt.imshow(img)\n",
    "plt.title(f'Prompt: \"{custom_prompt}\"', fontsize=14, wrap=True, pad=20)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
