{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76054edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "import tempfile\n",
    "from win10toast import ToastNotifier\n",
    "import os\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torch import nn\n",
    "import math\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import PIL\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d9e3e",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[TOC](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8854a6",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [TOC](#toc1_)    \n",
    "- [TODO: List of things to do in this project](#toc2_)    \n",
    "- [Datasets](#toc3_)    \n",
    "- [Noise schedulers (linear, cosine)](#toc4_)    \n",
    "  - [Linear](#toc4_1_)    \n",
    "  - [Cosine](#toc4_2_)    \n",
    "- [U-Net](#toc5_)    \n",
    "- [Time embeddings (Sinusoidal)](#toc6_)    \n",
    "- [Helper functions](#toc7_)    \n",
    "  - [Transformations](#toc7_1_)    \n",
    "  - [Plot histograms + KL-Divergence](#toc7_2_)    \n",
    "- [Final Model (forward, backward diffusion)](#toc8_)    \n",
    "- [Setup model + parameters](#toc9_)    \n",
    "  - [Forward diffusion](#toc9_1_)    \n",
    "  - [Reverse diffusion](#toc9_2_)    \n",
    "- [Train](#toc10_)    \n",
    "- [Metrics (FID, IS)](#toc11_)    \n",
    "- [Loss functions](#toc12_)    \n",
    "- [Sampling](#toc13_)    \n",
    "- [Training loop](#toc14_)    \n",
    "- [Generation test](#toc15_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96eecb",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[TODO: List of things to do in this project](#toc0_)\n",
    "\n",
    "* See the notebook code on the forward AND backward process in the model: https://github.com/dtransposed/code_videos/blob/main/01_Diffusion_Models_Tutorial/Diffusion%20Model.ipynb\n",
    "* The model should remove noise (backward diffusion). I need to use \"no grad\" - i didn't use it\n",
    "* Plot samples images (actually generate images from random noise) before are after each epoch training. Save it to mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74082d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "LR = 0.001\n",
    "\n",
    "EPOCHS = 15\n",
    "IMG_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "T = 300\n",
    "EXPERIMENT_NAME = \"Training 2\"\n",
    "\n",
    "mlflow.end_run() # just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28cc4ed",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Datasets](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), # Randomly flip horizontally so we have slightly different images every epoch\n",
    "    transforms.Pad(2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train = torchvision.datasets.FashionMNIST(root=\"../data\", download=True, transform=data_transform, train=True)\n",
    "\n",
    "test = torchvision.datasets.FashionMNIST(root=\"../data\", download=True, transform=data_transform, train=False)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "iter_test_dataloader = iter(test_dataloader) # For testing, plotting, validating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70958d22",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Noise schedulers (linear, cosine)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618aa839",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Linear](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf095cb",
   "metadata": {},
   "source": [
    "$\\alpha_t$ controls how much noise we add at timestep $t$.\n",
    "\n",
    "$\\beta_t$ controls the variance of the noise added at timestep $t$.\n",
    "\n",
    "1) $\\alpha_t = 1 - \\beta_t$ the noise strength changes at each timestep $t$ (depends on $\\beta_t$)\n",
    "2) $\\bar{\\alpha_t} = \\prod_{s=1}^{t} a_s$\n",
    "\n",
    "Equations (1) and (2) is from the paper \"Denoising Diffusion Probabilistic Models\"\n",
    "\n",
    "3) $x_t = \\sqrt{\\bar{\\alpha_t}} x_0 + \\sqrt{1 - \\bar{\\alpha_t}} \\epsilon$ - the forward diffusion process $q(x_t | x_0)$\n",
    "\n",
    "Equation (3): instead of iteratively adding noise, we do it in one single step (from $x_0$ we get to $x_t$). The $\\epsilon$ is random Gaussian noise. I explain this equation in detail in the latex / PDF document (`single_step_forward_process.pdf`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNoiseScheduler(nn.Module):\n",
    "    def __init__(self, timesteps: int = 1000, beta_start: float = 1e-4, beta_end: float = 0.02):\n",
    "        super().__init__()\n",
    "\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        # Start and end are the beta values for the linear noise schedule that we linearly interpolate between (hence linear scheduler)\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alphas = 1.0 - self.betas # Equation 1\n",
    "        self.alpha_hat = torch.cumprod(self.alphas, dim=0)  # Equation 2\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.betas = self.betas.to(device)\n",
    "        self.alphas = self.alphas.to(device)\n",
    "        self.alpha_hat = self.alpha_hat.to(device)\n",
    "        return self\n",
    "\n",
    "    def forward(self, x0: Tensor, t: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Apply noise at level \"t\" to an image \"x0\".\n",
    "        It applies the noise in a single step because of the Gaussian property of markov-chain of additive noise.\n",
    "\n",
    "        x0: [B, C, H, W]\n",
    "        t: [B, ]\n",
    "        \n",
    "        Returns:\n",
    "            x_t: the noisy image at timestep t\n",
    "            noise: the noise that was added\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x0)  # random Gaussian noise\n",
    "\n",
    "        # Get the alpha_hat values for each t in the batch (shape: [B, 1, 1, 1] for broadcasting)\n",
    "        alpha_hat_t = self.alpha_hat[t.long()].view(-1, 1, 1, 1)\n",
    "        sqrt_alpha_hat_t = torch.sqrt(alpha_hat_t)\n",
    "        sqrt_one_minus_alpha_hat_t = torch.sqrt(1 - alpha_hat_t)\n",
    "\n",
    "        x_t = sqrt_alpha_hat_t * x0 + sqrt_one_minus_alpha_hat_t * noise\n",
    "        return x_t, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, lbls = next(iter_test_dataloader)\n",
    "linear_noise_sched = LinearNoiseScheduler()\n",
    "\n",
    "num_of_imgs = 16\n",
    "\n",
    "imgs = imgs[0].repeat(imgs.shape[0], 1, 1, 1) # Batch of the same first image\n",
    "imgs = imgs[:num_of_imgs]\n",
    "\n",
    "# different noise iterations\n",
    "t = [25 * i for i in range(num_of_imgs)]\n",
    "t = Tensor(t)\n",
    "\n",
    "# apply noise\n",
    "noisy_imgs, noise = linear_noise_sched.forward(imgs, t)\n",
    "\n",
    "# Plot original + noisy images\n",
    "fig, axes = plt.subplots(1, num_of_imgs, figsize=(32, 32))\n",
    "\n",
    "# first row\n",
    "for i in range(num_of_imgs):\n",
    "    axes[i].imshow(rearrange(noisy_imgs[i], \"b h w -> h w b\"), cmap=\"gray\")\n",
    "    axes[i].set_title(\"t=\" + str(t[i].item()))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981a2e0f",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Cosine](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineNoiseScheduler(nn.Module):\n",
    "    \"\"\"\n",
    "    s = 0.008 is set by the authors of the paper (section 3.2)\n",
    "    \"\"\"\n",
    "    def __init__(self, timesteps = 1000, s=0.008):\n",
    "        self.timesteps = timesteps\n",
    "        self.s = s\n",
    "        self.betas = self._cosine_beta_schedule()\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def _cosine_beta_schedule(self):\n",
    "        steps = self.timesteps + 1\n",
    "        t = torch.linspace(0, self.timesteps, steps) / self.timesteps\n",
    "        alphas_bar = torch.cos(((t + self.s) / (1 + self.s)) * math.pi * 0.5) ** 2 # Formula 2\n",
    "        alphas_bar = alphas_bar / alphas_bar[0] # formula 1\n",
    "        betas = 1 - (alphas_bar[1:] / alphas_bar[:-1])\n",
    "        return torch.clamp(betas, max=0.999)\n",
    "\n",
    "    def forward(self, x0: Tensor, t: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Adds \"t\" noise to \"x0\"\n",
    "\n",
    "        x0: [B, C, H, W]\n",
    "        t: [B, ]\n",
    "        \n",
    "        Returns:\n",
    "            x_t: the noisy image at timestep t\n",
    "            noise: the noise that was added\n",
    "        \"\"\"\n",
    "        alpha_bar = self.alpha_cumprod[t.long()].view(-1, 1, 1, 1)\n",
    "        noise = torch.randn_like(x0)\n",
    "        return torch.sqrt(alpha_bar) * x0 + torch.sqrt(1 - alpha_bar) * noise, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, lbls = next(iter_test_dataloader)\n",
    "cosine_noise_sched = CosineNoiseScheduler()\n",
    "\n",
    "num_of_imgs = 16\n",
    "\n",
    "imgs = imgs[0].repeat(imgs.shape[0], 1, 1, 1) # Batch of the same first image\n",
    "imgs = imgs[:num_of_imgs]\n",
    "\n",
    "# different noise iterations\n",
    "t = [25 * i for i in range(num_of_imgs)]\n",
    "t = Tensor(t)\n",
    "\n",
    "# apply noise\n",
    "noisy_imgs, noise = cosine_noise_sched.forward(imgs, t)\n",
    "\n",
    "# Plot original + noisy images\n",
    "fig, axes = plt.subplots(1, num_of_imgs, figsize=(32, 32))\n",
    "\n",
    "# first row\n",
    "for i in range(num_of_imgs):\n",
    "    axes[i].imshow(rearrange(noisy_imgs[i], \"b h w -> h w b\"), cmap=\"gray\")\n",
    "    axes[i].set_title(\"t=\" + str(t[i].item()))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40ebf3",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[U-Net](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From the paper:\n",
    "\"It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) [i.e. DoubleConvolution module] and a 2x2 max pooling operation with stride 2 for downsampling [i.e. DownSampling module].\"\n",
    "\"\"\"\n",
    "class DoubleConvolution(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "class DownSampling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConvolution(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        down = self.conv(x)\n",
    "        pool = self.pool(down)\n",
    "\n",
    "        return down, pool\n",
    "\n",
    "\"\"\"\n",
    "From the paper: \n",
    "\"Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU\"\n",
    "\"\"\"\n",
    "class UpSampling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConvolution(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.dconv1 = DownSampling(in_channels, 64)\n",
    "        self.dconv2 = DownSampling(64, 128)\n",
    "        self.dconv3 = DownSampling(128, 256)\n",
    "        self.dconv4 = DownSampling(256, 512)\n",
    "\n",
    "        self.bottle_neck = DoubleConvolution(512, 1024)\n",
    "\n",
    "        self.uconv1 = UpSampling(1024, 512)\n",
    "        self.uconv2 = UpSampling(512, 256)\n",
    "        self.uconv3 = UpSampling(256, 128)\n",
    "        self.uconv4 = UpSampling(128, 64)\n",
    "\n",
    "        self.out = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "    \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        d1,p1 = self.dconv1(x)\n",
    "        d2,p2 = self.dconv2(p1)\n",
    "        d3,p3 = self.dconv3(p2)\n",
    "        d4,p4 = self.dconv4(p3)\n",
    "\n",
    "        b = self.bottle_neck(p4)\n",
    "\n",
    "        u1 = self.uconv1(b, d4)\n",
    "        u2 = self.uconv2(u1, d3)\n",
    "        u3 = self.uconv3(u2, d2)\n",
    "        u4 = self.uconv4(u3, d1)\n",
    "\n",
    "        out = self.out(u4)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d1e6f",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Time embeddings (Sinusoidal)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        assert self.embedding_dim % 2 == 0, \"embedding_dim must be even\"\n",
    "    \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    def forward(self, timesteps: Tensor) -> Tensor:\n",
    "        # Create the frequency spectrum\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        exponent = -math.log(10000.0) / (half_dim - 1)\n",
    "        freq = torch.exp(torch.arange(half_dim, dtype=torch.float32) * exponent)\n",
    "        # freq = freq.to(self.device)\n",
    "\n",
    "        # Expand timesteps for broadcasting\n",
    "        timesteps = timesteps.float().unsqueeze(1)  # (N, 1)\n",
    "        args = timesteps * freq.unsqueeze(0)        # (N, half_dim)\n",
    "\n",
    "        # Concatenate sin and cos\n",
    "        embedding = torch.cat([torch.sin(args), torch.cos(args)], dim=1)  # (N, embedding_dim)\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeds = 100 # Number of vectors\n",
    "embedding_dim = 256 # Vector size\n",
    "\n",
    "semb = SinusoidalEmbedding(embedding_dim)\n",
    "\n",
    "time_embeddings = torch.empty(num_embeds, embedding_dim)\n",
    "for i in range(num_embeds):\n",
    "    embd = semb.forward(Tensor([i]))\n",
    "    time_embeddings[i] = embd\n",
    "\n",
    "print(\"Time embedding shape: \" + str(time_embeddings.shape))\n",
    "txt = r\"\"\"\n",
    "# Concatenate sin and cos\n",
    "embedding = torch.cat([torch.sin(args), torch.cos(args)], dim=1)  # (N, embedding_dim)\n",
    "\"\"\"\n",
    "print(\"Left half: sinus. Right half: cosine. See line: \\n\\t\" + txt)\n",
    "\n",
    "plt.figure(figsize=(10, 1))\n",
    "plt.imshow(time_embeddings, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Value')\n",
    "# plt.axis('off')  # Optional: hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee549c",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Helper functions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c01034",
   "metadata": {},
   "source": [
    "## <a id='toc7_1_'></a>[Transformations](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04697cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_plot(x: Tensor) -> PIL.Image.Image:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        x: A single image or multiple images [B, C, H, W]\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image.Image to plot.\n",
    "    \"\"\"\n",
    "    x = x.detach().cpu()\n",
    "    if x.ndim == 4:\n",
    "        x = rearrange(x, \"b c h w -> b h w c\")\n",
    "    elif x.ndim == 3:\n",
    "        x = rearrange(x, \"c h w -> h w c\")\n",
    "    else:\n",
    "        raise ValueError(\"Expected tensor of 3 or 4 dimensions\")\n",
    "    \n",
    "    return x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1040ca41",
   "metadata": {},
   "source": [
    "## <a id='toc7_2_'></a>[Plot histograms + KL-Divergence](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54336d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_gaussians(mu1, std1, mu2, std2):\n",
    "    \"\"\"KL divergence between two Gaussians N(mu1, std1^2) || N(mu2, std2^2)\"\"\"\n",
    "    var1, var2 = std1**2, std2**2\n",
    "    return torch.log(std2 / std1) + (var1 + (mu1 - mu2)**2) / (2 * var2) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce07a44",
   "metadata": {},
   "source": [
    "Plot noise histograms with normal to determine our noise prediction with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9cf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_noise_histogram_with_normal(\n",
    "        first_tensor: Tensor, \n",
    "        second_tensor: Tensor, \n",
    "        title: str = \"Noise Histograms Comparison\",\n",
    "        first_tensor_label: str = \"Tensor 1 (blue)\",\n",
    "        second_tensor_label: str = \"Tensor 2 (orange)\",\n",
    "        bins: int = 20,\n",
    "        normalize: bool = False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Plots histograms and fitted normal distributions for two tensors.\n",
    "    \n",
    "    Parameters:\n",
    "        first_tensor (Tensor): First tensor (e.g. noise image), shape (B, C, H, W)\n",
    "        second_tensor (Tensor): Second tensor to compare with, same shape\n",
    "        title (str): Title for the plot\n",
    "        first_tensor_label (str): Legend label of first histogram\n",
    "        second_tensor_label (str): Legend label of second histogram\n",
    "        bins: Number of bins for the histogram\n",
    "        normalize: If True, will normalize tensors to [0, 1] range\n",
    "    \n",
    "    Returns:\n",
    "        kl_divergence (Tensor): The KL-Divergence between two distributions (first_tensor, second_tensor)\n",
    "    \"\"\"\n",
    "    # Normalize\n",
    "    if normalize:\n",
    "        first_tensor = (first_tensor - first_tensor.min()) / (first_tensor.max() - first_tensor.min())\n",
    "        second_tensor = (second_tensor - second_tensor.min()) / (second_tensor.max() - second_tensor.min())\n",
    "\n",
    "    # Flatten and convert to numpy\n",
    "    first_values = first_tensor.flatten().detach().cpu().numpy()\n",
    "    second_values = second_tensor.flatten().detach().cpu().numpy()\n",
    "\n",
    "    # Plot histograms\n",
    "    plt.hist(first_values, bins=bins, density=True, alpha=0.5, label=first_tensor_label, color='skyblue')\n",
    "    plt.hist(second_values, bins=bins, density=True, alpha=0.5, label=second_tensor_label, color='orange')\n",
    "\n",
    "    # Normal distributions\n",
    "    for values, color in zip(\n",
    "        [first_values, second_values],\n",
    "        ['blue', 'darkorange']\n",
    "    ):\n",
    "        mu, std = np.mean(values), np.std(values)\n",
    "\n",
    "        x = np.linspace(values.min(), values.max(), 100)\n",
    "        p = norm.pdf(x, mu, std)\n",
    "        plt.plot(x, p, color=color, linewidth=2, label=f'PDF (μ={mu:.2f}, σ={std:.2f})')\n",
    "\n",
    "    # Formatting\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    mu1 = np.mean(first_values)\n",
    "    std1 = np.std(first_values)\n",
    "    mu2 = np.mean(second_values)\n",
    "    std2 = np.std(second_values)\n",
    "    kl_divergence = kl_gaussians(Tensor([mu1]), Tensor([std1]), Tensor([mu2]), Tensor([std2]))\n",
    "    return kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise1 = torch.randn(1, 1, 32, 32)\n",
    "noise2 = torch.randn(1, 1, 32, 32) * 0.5 + 1.0  # different mean and std\n",
    "kl_divergence = plot_noise_histogram_with_normal(noise1, noise2)\n",
    "print(\"We have high KL-Divergence: \" + str(kl_divergence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3773af",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise1 = torch.randn(1, 1, 32, 32)\n",
    "noise2 = torch.randn(1, 1, 32, 32) + 0.5  # different mean and std\n",
    "kl_divergence = plot_noise_histogram_with_normal(noise1, noise2)\n",
    "print(\"We have low KL-Divergence: \" + str(kl_divergence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6409d",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Final Model (forward, backward diffusion)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12035ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, noise_scheduler: str = \"linear\", input_channels: int = 1, num_classes: int = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Noise scheduler selection\n",
    "        if noise_scheduler == \"linear\":\n",
    "            self.noise_scheduler = LinearNoiseScheduler()\n",
    "        elif noise_scheduler == \"cosine\":\n",
    "            self.noise_scheduler = CosineNoiseScheduler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown noise scheduler: {noise_scheduler}\")\n",
    "        \n",
    "        # U-Net selection\n",
    "        self.unet = UNet(in_channels=input_channels, num_classes=num_classes)\n",
    "\n",
    "        # Time embeddings\n",
    "        self.time_embedding_dim = 256\n",
    "        self.sinusoidalEmbedding = SinusoidalEmbedding(self.time_embedding_dim) # TODO: Do something with time embeddings\n",
    "\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.noise_scheduler.to(device)\n",
    "        self.unet.to(device)\n",
    "        self.sinusoidalEmbedding.to(device)\n",
    "        return self\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor):\n",
    "        \"\"\"\n",
    "        Forward diffusion (adding noise). Uses the selected noise scheduler.\n",
    "\n",
    "        x: (B, C, H, W)\n",
    "        t: (B, )\n",
    "        \"\"\"\n",
    "        return self.noise_scheduler.forward(x, t)\n",
    "\n",
    "\n",
    "    # Don't collect gradient information (no need to calculate loss here)\n",
    "    # This is inference / sampling, not training\n",
    "    @torch.no_grad()\n",
    "    def backward(self, x, t):\n",
    "        \"\"\"\n",
    "        Calls the model to predict the noise in the image and returns the denoised image. \n",
    "        Applies noise to this image, if we are not in the last step yet.\n",
    "        \"\"\"\n",
    "        betas_t = self.get_index_from_list(self.betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.get_index_from_list(torch.sqrt(1. - self.alphas_cumprod), t, x.shape)\n",
    "        sqrt_recip_alphas_t = self.get_index_from_list(torch.sqrt(1.0 / self.alphas), t, x.shape)\n",
    "        mean = sqrt_recip_alphas_t * (x - betas_t * self.forward(x, t) / sqrt_one_minus_alphas_cumprod_t)\n",
    "        posterior_variance_t = betas_t\n",
    "\n",
    "        if t == 0:\n",
    "            return mean\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "            variance = torch.sqrt(posterior_variance_t) * noise \n",
    "            return mean + variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0fb01",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[Setup model + parameters](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57822cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_classes=1) # MNIST is 1 channel. We predict on 1 channels instead of 3.\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30036e9a",
   "metadata": {},
   "source": [
    "## <a id='toc9_1_'></a>[Forward diffusion](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_imgs = 5\n",
    "\n",
    "imgs, lbls = next(iter_test_dataloader)\n",
    "t = Tensor([i * 10 for i in range(num_of_imgs)])\n",
    "\n",
    "imgs = torch.stack([imgs[0]] * num_of_imgs)\n",
    "\n",
    "print(imgs.shape)\n",
    "print(t.shape)\n",
    "\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "noisy_imgs, noise = model.forward(imgs, t)\n",
    "\n",
    "for img in noisy_imgs:\n",
    "    plt.imshow(to_plot(img), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23146fa4",
   "metadata": {},
   "source": [
    "## <a id='toc9_2_'></a>[Reverse diffusion](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77be0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, lbls = next(iter_test_dataloader)\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "t = torch.randint(0, T, (BATCH_SIZE, ))\n",
    "\n",
    "noisy_imgs, noise = model.forward(imgs, t)\n",
    "noise_pred = model.backward(imgs, None)\n",
    "\n",
    "print(\"t = \" + str(t[0].item()))\n",
    "\n",
    "plt.imshow(to_plot(imgs[0]), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(to_plot(noisy_imgs[0]), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(to_plot(noise_pred[0]), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "a = noise[0]\n",
    "b = noise_pred[0]\n",
    "\n",
    "print(a.shape, b.shape)\n",
    "plot_noise_histogram_with_normal(a, b, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d0b15",
   "metadata": {},
   "source": [
    "# <a id='toc10_'></a>[Train](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_plot_inference_and_fid_is_metrics(model, epoch, imgs, t, log_artifact: bool = True):\n",
    "    \"\"\"\n",
    "    Plots the inference. Plots the original image, noisy image, first term, second term, predicted noise, and noisy image minus predicted noise.\n",
    "    \"\"\"\n",
    "    x_noisy, noise = model.noise_scheduler(imgs, t)\n",
    "    noise_pred = model.backward(x_noisy, t)\n",
    "\n",
    "    # Plot original image, noisy image, noise, and predicted noise\n",
    "    idx = 0\n",
    "    original_image = imgs[idx].cpu()\n",
    "    noisy_image = x_noisy[idx].cpu()\n",
    "    noise_idx = noise[idx].cpu()\n",
    "    # first_term_image = first_term[idx].cpu()\n",
    "    # second_term_image = second_term[idx].cpu()\n",
    "    predicted_noise_image = noise_pred[idx].cpu().detach()\n",
    "    t_idx = t[idx].cpu().item()\n",
    "    # minus = noisy_image - second_term_image\n",
    "    noisy_minus_pred = noisy_image - predicted_noise_image\n",
    "\n",
    "    num_of_subplots = 5\n",
    "    subplot_index = 0\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_of_subplots, figsize=(32, 32))\n",
    "    axes[subplot_index].imshow(rearrange(original_image, \"c h w -> h w c\"), cmap=\"gray\")\n",
    "    axes[subplot_index].set_title(\"Original\")\n",
    "    subplot_index += 1\n",
    "\n",
    "    axes[subplot_index].imshow(rearrange(noisy_image, \"c h w -> h w c\"), cmap=\"gray\")\n",
    "    axes[subplot_index].set_title(\"Noisy (t=\"+str(t_idx)+\")\")\n",
    "    subplot_index += 1\n",
    "\n",
    "    axes[subplot_index].imshow(rearrange(noise_idx, \"c h w -> h w c\"), cmap=\"gray\")\n",
    "    axes[subplot_index].set_title(\"Noise\")\n",
    "    subplot_index += 1\n",
    "\n",
    "    # axes[subplot_index].imshow(rearrange(first_term_image, \"c h w -> h w c\"), cmap=\"gray\")\n",
    "    # axes[subplot_index].set_title(\"First term\")\n",
    "    # subplot_index += 1\n",
    "\n",
    "    # axes[subplot_index].imshow(rearrange(second_term_image, \"c h w -> h w c\"), cmap=\"gray\")\n",
    "    # axes[subplot_index].set_title(\"Second term\")\n",
    "    # subplot_index += 1\n",
    "\n",
    "    axes[subplot_index].imshow(rearrange(predicted_noise_image, \"c h w -> h w c\"), cmap=\"gray\")\n",
    "    axes[subplot_index].set_title(\"Predicted\")\n",
    "    subplot_index += 1\n",
    "\n",
    "    # axes[subplot_index].imshow(rearrange(minus, \"c h w -> h w c\"), cmap=\"gray\")\n",
    "    # axes[subplot_index].set_title(\"(Noisy-Second)\")\n",
    "    # subplot_index += 1\n",
    "\n",
    "    axes[subplot_index].imshow(rearrange(noisy_minus_pred, \"c h w -> h w c\"), cmap=\"gray\")\n",
    "    axes[subplot_index].set_title(\"(Noisy-Pred)\")\n",
    "    subplot_index += 1\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.25)  # Increase gap between columns\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    if log_artifact:\n",
    "        img_name_prefix = EXPERIMENT_NAME + \"_\" + str(epoch) + \"_\"\n",
    "        with tempfile.NamedTemporaryFile(prefix=img_name_prefix, suffix=\".png\", delete=False) as tmp:\n",
    "            fig.savefig(tmp.name)\n",
    "            mlflow.log_artifact(tmp.name, artifact_path=\"images\")\n",
    "        os.remove(tmp.name)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)  # Clean up the figure\n",
    "\n",
    "imgs, lbls = next(iter(test_dataloader))\n",
    "imgs = imgs.to(device)\n",
    "t = torch.randint(0, 100, (BATCH_SIZE,), device=device).long()\n",
    "log_plot_inference_and_fid_is_metrics(model, 0, imgs, t, log_artifact=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ca268",
   "metadata": {},
   "source": [
    "# <a id='toc11_'></a>[Metrics (FID, IS)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0997c09",
   "metadata": {},
   "source": [
    "FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9016550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fid_score(original_images, generated_images):\n",
    "    \"\"\"\n",
    "    FID evaluates diversity of generated images (generated_images) with in a reference dataset (original_images)\n",
    "    \"\"\"\n",
    "    # Convert grayscale (1-channel) images to 3-channel\n",
    "    if original_images.shape[1] == 1:\n",
    "        original_images = original_images.repeat(1, 3, 1, 1)\n",
    "        generated_images = generated_images.repeat(1, 3, 1, 1)\n",
    "\n",
    "    fid = FrechetInceptionDistance(normalize=True).to(original_images.device)\n",
    "    fid.update(original_images, real=True)\n",
    "    fid.update(generated_images, real=False)\n",
    "    return fid.compute()\n",
    "\n",
    "imgs, lbls = next(iter(test_dataloader))\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "fid = get_fid_score(imgs, imgs)\n",
    "print(\"When the input is the exact same tensors, we expect FID score of 0, which is indeed the case\")\n",
    "print(\"Same images FID:\", fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6fb49",
   "metadata": {},
   "source": [
    "IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_is_metrics(original_images, generated_images):\n",
    "    # Convert grayscale (1-channel) images to 3-channel\n",
    "    if original_images.shape[1] == 1:\n",
    "        original_images = original_images.repeat(1, 3, 1, 1)\n",
    "        generated_images = generated_images.repeat(1, 3, 1, 1)\n",
    "    \n",
    "    is_metric = InceptionScore(normalize=True).to(device)\n",
    "    is_score_original, is_std_original = is_metric(original_images)\n",
    "    is_score_generated, is_std_generated = is_metric(generated_images)\n",
    "    return is_score_original, is_std_original, is_score_generated, is_std_generated\n",
    "\n",
    "imgs, lbls = next(iter(test_dataloader))\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "is_score_original, is_std_original, is_score_generated, is_std_generated = get_is_metrics(imgs, imgs)\n",
    "print(\"When the input is the exact same tensors, we expect exact same IS score, and indeed that is the case\")\n",
    "print(f\"IS Score:\\t\\t{is_score_original}, IS STD:\\t\\t{is_std_original}\")\n",
    "print(f\"IS Generated Score:\\t{is_score_generated}, IS Generated STD:\\t{is_std_generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_fid_is_metrics(model, epoch, imgs):\n",
    "    # Add noise to the images\n",
    "    t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "    x_noisy, noise = model.forward(imgs, t)\n",
    "\n",
    "    # Get noise prediction\n",
    "    noise_pred = model.backward(x_noisy, t)\n",
    "\n",
    "    # Remove the predicted noise from noisy image\n",
    "    samples = x_noisy - noise_pred\n",
    "\n",
    "    fid = get_fid_score(imgs, samples)\n",
    "    is_score_original, is_std_original, is_score_generated, is_std_generated = get_is_metrics(imgs, samples)\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        \"fid\": fid,\n",
    "        \"is_score_original\": is_score_original,\n",
    "        \"is_score_original_std\": is_std_original,\n",
    "        \"is_score_generated\": is_score_generated,\n",
    "        \"is_score_generated_std\": is_std_generated,\n",
    "    }, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c54680a",
   "metadata": {},
   "source": [
    "# <a id='toc12_'></a>[Loss functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bde0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, model, loss: str = \"l1\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "    \n",
    "    def forward(self, epoch: int, imgs: Tensor, t: Tensor) -> Tensor:\n",
    "        if self.loss == \"l1\":\n",
    "            x_noisy, noise = model.forward(imgs, t)\n",
    "            noise_pred = model.backward(x_noisy, t)\n",
    "\n",
    "            if last_epoch is None:\n",
    "                last_epoch = epoch\n",
    "\n",
    "            if epoch != last_epoch:\n",
    "                plot_noise_histogram_with_normal(noise, \n",
    "                                                noise_pred, \n",
    "                                                title=f\"Noise Histograms Comparison (epoch = {epoch})\", \n",
    "                                                first_tensor_label=\"Ground truth noise\", \n",
    "                                                second_tensor_label=\"Predicted noise\")\n",
    "                last_epoch = epoch\n",
    "\n",
    "            loss = F.l1_loss(noise_pred, noise)\n",
    "        elif self.loss == \"mse\":\n",
    "            raise \"Not yet implemented\"\n",
    "        else:\n",
    "            raise ValueError(f\"Loss function '{self.loss}' is not known.\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1477d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_checkpoint(model, epoch, log_every: int = 1):\n",
    "    \"\"\"\n",
    "    Save model checkpoint each \"log_every\" epochs\n",
    "\n",
    "    Parameters:\n",
    "        log_every: If set to 1, will save checkpoint every 1 epoch. If set to 2, will save checkpoint every 2 epochs and so on.\n",
    "    \"\"\"\n",
    "    if log_every == 1 or (epoch % log_every == 0):\n",
    "        checkpoint_path = f\"checkpoint_epoch_{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        mlflow.log_artifact(checkpoint_path, artifact_path=\"checkpoints\")\n",
    "        os.remove(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5754196e",
   "metadata": {},
   "source": [
    "# <a id='toc13_'></a>[Sampling](#toc0_)\n",
    "\n",
    "![](notebook_imgs/sampling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_and_plot_images():\n",
    "    with torch.no_grad():\n",
    "        # Line 1\n",
    "        img = torch.randn((1, 1, IMG_SIZE, IMG_SIZE)).to(device) #x_T - the model will denoise this\n",
    "\n",
    "        # Line 2\n",
    "        for i in reversed(range(T)):\n",
    "            t = torch.full((1,), i, dtype=torch.long, device=device) # Single tensor (i.e. the timestep in the for loop)\n",
    "\n",
    "\n",
    "            img = model.backward(img, t) # We denoise the image at current timestep t\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                plt.figure(figsize=(2, 2))\n",
    "                plt.imshow(to_plot(img[0]), cmap=\"gray\")\n",
    "                plt.show()\n",
    "\n",
    "gen_and_plot_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "mlflow_run = mlflow.start_run()\n",
    "\n",
    "mlflow.log_params({\n",
    "    \"lr\": LR,\n",
    "    \"device\": device,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"image_size\": IMG_SIZE,\n",
    "    \"T\": T\n",
    "})\n",
    "\n",
    "mlflow.set_tag(\"Training Info\", \"Basic MNIST Fashion dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc847e",
   "metadata": {},
   "source": [
    "# <a id='toc14_'></a>[Training loop](#toc0_)\n",
    "\n",
    "![](notebook_imgs/training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa792908",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_step = 0\n",
    "last_loss_epoch = -1\n",
    "running_loss = 0 # Epoch running loss\n",
    "\n",
    "loss_function = LossFunction(model, \"l1\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # Logs an image of different plots. Each subplot is different image (original, noisy, the noise, predicted noise and so on)\n",
    "    log_plot_inference_and_fid_is_metrics(model, epoch, imgs, t)\n",
    "\n",
    "    # Save checkpoint each epoch\n",
    "    log_checkpoint(model, epoch)\n",
    "\n",
    "    # Log FID, IS metrics\n",
    "    imgs, lbls = next(iter_test_dataloader)\n",
    "    imgs = imgs.to(device)\n",
    "    log_fid_is_metrics(model, epoch, imgs)\n",
    "\n",
    "    # Line 1\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Line 2\n",
    "        imgs, labels = batch\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        # Line 3\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).long() \n",
    "        \n",
    "        # Lines 4, 5 (calculate loss, take gradient step until converged)\n",
    "        loss = loss_function(epoch, imgs, t) # Calculate loss\n",
    "        loss.backward() # Take gradient step\n",
    "        optimizer.step() # Update model's parameters until converged\n",
    "\n",
    "        running_loss += loss\n",
    "        mlflow.log_metric(\"batch_loss\", loss.item(), step=batch_step)\n",
    "        batch_step += 1\n",
    "\n",
    "    running_loss = (running_loss / batch_step).item()\n",
    "    mlflow.log_metric(\"epoch_loss\", running_loss, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db489a",
   "metadata": {},
   "source": [
    "# <a id='toc15_'></a>[Generation test](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c52cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    img = torch.randn((1, 1, IMG_SIZE, IMG_SIZE)).to(device)\n",
    "    for i in reversed(range(T)):\n",
    "        t = torch.full((1,), i, dtype=torch.long, device=device)\n",
    "        img = model.backward(img, t)\n",
    "        if i % 50 == 0:\n",
    "            plt.figure(figsize=(2,2))\n",
    "            plt.imshow(to_plot(img[0]))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae6ba8",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model.pth\")\n",
    "mlflow.pytorch.log_model(model, artifact_path=\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda0b754",
   "metadata": {},
   "source": [
    "Generate images (the whole point of the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddaa62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Generate 16 random noise samples\n",
    "    num_images = 16\n",
    "    img_shape = (num_images, 1, IMG_SIZE, IMG_SIZE)\n",
    "    img = torch.randn(img_shape).to(device)\n",
    "    t = torch.zeros(num_images, dtype=torch.long).to(device)  # dummy timesteps\n",
    "\n",
    "    # Predict noise and denoise\n",
    "    noise_pred = model(img, t)\n",
    "    img = img - noise_pred\n",
    "\n",
    "    # Convert to grid for plotting\n",
    "    grid = torchvision.utils.make_grid(img, nrow=4, normalize=True)  # normalize for display\n",
    "    grid = grid.permute(1, 2, 0).cpu().numpy()  # CHW -> HWC\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(grid.squeeze(), cmap=\"gray\")  # squeeze if single channel\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Generated Samples\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6849a55b",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d566f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model\n",
    "# model = Model()\n",
    "# model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea21200",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69f9f0",
   "metadata": {},
   "source": [
    "Notify windows that we finished running notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    notifier = ToastNotifier()\n",
    "    notifier.show_toast(\"Notebook Run Complete\", \"Training finished.\", duration=10, threaded=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-conditioned-image-generation-using-st-35DVCAXA-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
