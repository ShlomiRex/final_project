# ============================================================================
# Accelerate Configuration for Multi-GPU Training
# ============================================================================
# This file configures Accelerate for distributed training on HPC cluster
#
# Usage:
#   accelerate launch --config_file accelerate_config.yaml scripts/train.py --config configs/base.yaml
#
# ============================================================================

compute_environment: LOCAL_MACHINE

distributed_type: MULTI_GPU  # or FSDP for very large models

# Multi-GPU settings
num_processes: 2              # Number of GPUs to use (update based on allocation)
gpu_ids: all                  # Use all available GPUs

# Mixed precision training
mixed_precision: bf16         # bf16 recommended for A100/H100, fp16 for V100

# Gradient accumulation
gradient_accumulation_steps: 1  # Set to >1 if batch size per GPU is too large

# Communication backend
backend: nccl                 # nccl is best for NVIDIA GPUs

# Other settings
use_cpu: false
num_machines: 1
machine_rank: 0
main_process_ip: null
main_process_port: null
rdzv_backend: static
same_network: true
main_training_function: main

# DeepSpeed (optional - uncomment if using DeepSpeed)
# deepspeed_config:
#   gradient_accumulation_steps: 1
#   gradient_clipping: 1.0
#   zero_stage: 2
#   offload_optimizer_device: none
#   offload_param_device: none

# FSDP (optional - uncomment if using FSDP for very large models)
# fsdp_config:
#   fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
#   fsdp_backward_prefetch_policy: BACKWARD_PRE
#   fsdp_sharding_strategy: 1
#   fsdp_state_dict_type: FULL_STATE_DICT
