{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a3cf66",
   "metadata": {},
   "source": [
    "# ðŸ”§ System Diagnostic - Run This First!\n",
    "\n",
    "Before training, we need to verify that the GPU is properly accessible. This cell will check:\n",
    "1. Slurm job allocation\n",
    "2. GPU hardware detection\n",
    "3. CUDA environment variables\n",
    "4. PyTorch CUDA compatibility\n",
    "5. Common issues and solutions\n",
    "\n",
    "**Run the diagnostic cell below FIRST before proceeding with training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea9b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive GPU Diagnostic for HPC Cluster\n",
    "This cell diagnoses why PyTorch might show \"device: cpu\" even on GPU nodes\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" ðŸ” COMPREHENSIVE GPU DIAGNOSTIC FOR HPC CLUSTER\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SLURM JOB INFORMATION\n",
    "# ============================================================================\n",
    "print(\"1ï¸âƒ£  SLURM JOB ALLOCATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "slurm_vars = {\n",
    "    'SLURM_JOB_ID': 'Job ID',\n",
    "    'SLURM_JOB_NODELIST': 'Assigned Node(s)',\n",
    "    'SLURM_NODEID': 'Node ID',\n",
    "    'SLURM_GPUS': 'Total GPUs Allocated',\n",
    "    'SLURM_GPUS_ON_NODE': 'GPUs on This Node',\n",
    "    'SLURM_JOB_GPUS': 'GPU IDs Allocated',\n",
    "    'SLURM_CPUS_ON_NODE': 'CPUs on Node',\n",
    "    'SLURM_MEM_PER_NODE': 'Memory per Node',\n",
    "}\n",
    "\n",
    "slurm_allocated = False\n",
    "for var, desc in slurm_vars.items():\n",
    "    value = os.environ.get(var, 'NOT SET')\n",
    "    print(f\"  {desc:25s}: {value}\")\n",
    "    if var in ['SLURM_GPUS', 'SLURM_GPUS_ON_NODE', 'SLURM_JOB_GPUS']:\n",
    "        if value != 'NOT SET' and value != '0' and value != '':\n",
    "            slurm_allocated = True\n",
    "\n",
    "print()\n",
    "if slurm_allocated:\n",
    "    print(\"  âœ… Slurm has allocated GPU(s) to this job\")\n",
    "else:\n",
    "    print(\"  âš ï¸  WARNING: No GPU allocation detected by Slurm!\")\n",
    "    print(\"     This job may not have requested GPU resources.\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CUDA ENVIRONMENT VARIABLES\n",
    "# ============================================================================\n",
    "print()\n",
    "print(\"2ï¸âƒ£  CUDA ENVIRONMENT VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "cuda_vars = {\n",
    "    'CUDA_VISIBLE_DEVICES': 'Which GPUs are visible to CUDA',\n",
    "    'CUDA_HOME': 'CUDA installation directory',\n",
    "    'CUDA_PATH': 'CUDA path',\n",
    "    'CUDA_ROOT': 'CUDA root directory',\n",
    "    'LD_LIBRARY_PATH': 'Library path (includes CUDA libs)',\n",
    "}\n",
    "\n",
    "cuda_env_ok = False\n",
    "for var, desc in cuda_vars.items():\n",
    "    value = os.environ.get(var, 'NOT SET')\n",
    "    if var == 'LD_LIBRARY_PATH' and value != 'NOT SET':\n",
    "        # Show only CUDA-related parts\n",
    "        cuda_parts = [p for p in value.split(':') if 'cuda' in p.lower() or 'CUDA' in p]\n",
    "        if cuda_parts:\n",
    "            print(f\"  {var:25s}: {cuda_parts[0]} (and {len(cuda_parts)-1} more)\")\n",
    "        else:\n",
    "            print(f\"  {var:25s}: (no CUDA paths found)\")\n",
    "    else:\n",
    "        print(f\"  {var:25s}: {value}\")\n",
    "    \n",
    "    if var == 'CUDA_VISIBLE_DEVICES' and value != 'NOT SET':\n",
    "        cuda_env_ok = True\n",
    "\n",
    "print()\n",
    "if cuda_env_ok:\n",
    "    print(\"  âœ… CUDA_VISIBLE_DEVICES is set\")\n",
    "else:\n",
    "    print(\"  âš ï¸  WARNING: CUDA_VISIBLE_DEVICES not set!\")\n",
    "    print(\"     GPUs may not be visible to applications.\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. GPU HARDWARE DETECTION (nvidia-smi)\n",
    "# ============================================================================\n",
    "print()\n",
    "print(\"3ï¸âƒ£  GPU HARDWARE DETECTION (nvidia-smi)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free',\n",
    "         '--format=csv'],\n",
    "        capture_output=True, text=True, timeout=5\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout)\n",
    "        print(\"  âœ… GPU hardware detected successfully\")\n",
    "        hardware_ok = True\n",
    "    else:\n",
    "        print(f\"  âŒ nvidia-smi failed with error:\\n{result.stderr}\")\n",
    "        hardware_ok = False\n",
    "except FileNotFoundError:\n",
    "    print(\"  âŒ nvidia-smi command not found!\")\n",
    "    print(\"     GPU drivers may not be installed.\")\n",
    "    hardware_ok = False\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Error running nvidia-smi: {e}\")\n",
    "    hardware_ok = False\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CUDA TOOLKIT VERSION\n",
    "# ============================================================================\n",
    "print(\"4ï¸âƒ£  CUDA TOOLKIT VERSION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'release' in line.lower():\n",
    "                print(f\"  System CUDA: {line.strip()}\")\n",
    "                # Extract version number\n",
    "                import re\n",
    "                match = re.search(r'release (\\d+\\.\\d+)', line)\n",
    "                if match:\n",
    "                    cuda_version = match.group(1)\n",
    "                    print(f\"  CUDA Version: {cuda_version}\")\n",
    "    else:\n",
    "        print(\"  âš ï¸  nvcc not found (CUDA toolkit may not be in PATH)\")\n",
    "except Exception as e:\n",
    "    print(f\"  âš ï¸  Could not determine CUDA version: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. PYTORCH CUDA DETECTION\n",
    "# ============================================================================\n",
    "print(\"5ï¸âƒ£  PYTORCH CUDA DETECTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"  PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"  PyTorch Built with CUDA: {torch.version.cuda}\")\n",
    "    print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"    GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"      Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print()\n",
    "        print(\"  âœ… PyTorch can access GPU(s)!\")\n",
    "        pytorch_ok = True\n",
    "    else:\n",
    "        print()\n",
    "        print(\"  âŒ PyTorch CANNOT access GPU!\")\n",
    "        pytorch_ok = False\n",
    "        \n",
    "        # Diagnose why\n",
    "        print()\n",
    "        print(\"  ðŸ” DIAGNOSIS:\")\n",
    "        \n",
    "        # Check CUDA version mismatch\n",
    "        if torch.version.cuda:\n",
    "            pytorch_cuda = torch.version.cuda\n",
    "            print(f\"     - PyTorch was built for CUDA {pytorch_cuda}\")\n",
    "            \n",
    "            # Try to get system CUDA version\n",
    "            try:\n",
    "                nvcc_result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=5)\n",
    "                if nvcc_result.returncode == 0:\n",
    "                    import re\n",
    "                    match = re.search(r'release (\\d+\\.\\d+)', nvcc_result.stdout)\n",
    "                    if match:\n",
    "                        system_cuda = match.group(1)\n",
    "                        print(f\"     - System has CUDA {system_cuda}\")\n",
    "                        \n",
    "                        # Compare major versions\n",
    "                        pytorch_major = pytorch_cuda.split('.')[0]\n",
    "                        system_major = system_cuda.split('.')[0]\n",
    "                        \n",
    "                        if pytorch_major != system_major:\n",
    "                            print()\n",
    "                            print(f\"     âš ï¸  CUDA VERSION MISMATCH!\")\n",
    "                            print(f\"         PyTorch needs CUDA {pytorch_major}.x\")\n",
    "                            print(f\"         System has CUDA {system_major}.x\")\n",
    "                            print()\n",
    "                            print(f\"     ðŸ’¡ SOLUTION: Reinstall PyTorch with CUDA {system_major}.x support\")\n",
    "                            if system_major == '11':\n",
    "                                print(f\"         pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "                            elif system_major == '12':\n",
    "                                print(f\"         pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Check if CUDA_VISIBLE_DEVICES is set\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES') == '':\n",
    "            print(\"     - CUDA_VISIBLE_DEVICES is empty (no GPUs visible)\")\n",
    "        elif os.environ.get('CUDA_VISIBLE_DEVICES') is None:\n",
    "            print(\"     - CUDA_VISIBLE_DEVICES is not set\")\n",
    "        \n",
    "        # Check if Slurm allocated GPU\n",
    "        if not slurm_allocated:\n",
    "            print(\"     - Slurm did not allocate GPU to this job\")\n",
    "            print(\"       Did you request GPU with --gres=gpu:1?\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"  âŒ PyTorch is not installed!\")\n",
    "    pytorch_ok = False\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. SUMMARY & RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\" ðŸ“‹ SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "all_checks = {\n",
    "    'Slurm GPU Allocation': slurm_allocated,\n",
    "    'CUDA Environment': cuda_env_ok,\n",
    "    'GPU Hardware (nvidia-smi)': hardware_ok,\n",
    "    'PyTorch CUDA Access': pytorch_ok,\n",
    "}\n",
    "\n",
    "for check, status in all_checks.items():\n",
    "    status_icon = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"  {status_icon} {check}\")\n",
    "\n",
    "print()\n",
    "\n",
    "if all(all_checks.values()):\n",
    "    print(\"ðŸŽ‰ ALL CHECKS PASSED! GPU is ready for training.\")\n",
    "else:\n",
    "    print(\"âš ï¸  ISSUES DETECTED. Review the diagnostic output above.\")\n",
    "    print()\n",
    "    print(\"Common solutions:\")\n",
    "    print()\n",
    "    print(\"1. If 'Slurm GPU Allocation' failed:\")\n",
    "    print(\"   - Make sure you started Jupyter with: bash slurm/start_jupyter.sh gpu7\")\n",
    "    print(\"   - Check job allocation: squeue -u $USER\")\n",
    "    print()\n",
    "    print(\"2. If 'PyTorch CUDA Access' failed but hardware is OK:\")\n",
    "    print(\"   - Most likely CUDA version mismatch\")\n",
    "    print(\"   - Run the PyTorch reinstall cell below\")\n",
    "    print()\n",
    "    print(\"3. If 'GPU Hardware' failed:\")\n",
    "    print(\"   - Job may be on a CPU-only node\")\n",
    "    print(\"   - Cancel and restart with: bash slurm/start_jupyter.sh gpu7\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2688543b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Train Conditional UNet2D on MNIST with VAE and CLIP\n",
    "\n",
    "This notebook trains a conditional diffusion model on MNIST using:\n",
    "- **Pretrained VAE** (AutoencoderKL from Stable Diffusion) to encode images to latents\n",
    "- **CLIP text encoder** to encode digit captions\n",
    "- **UNet2DConditionModel** with cross-attention for text conditioning\n",
    "- **DDPM scheduler** for training and inference\n",
    "- **Classifier-free guidance** for improved sampling\n",
    "\n",
    "## Key Design Choices\n",
    "- MNIST images (28x28, grayscale) are upscaled to 128x128 (reduced from 256), 3 channels, normalized to [-1,1] for VAE compatibility\n",
    "- VAE encodes to 16x16x4 latents\n",
    "- Only the UNet is trained; VAE and CLIP are frozen\n",
    "- Training includes loss tracking with per-batch and per-epoch visualization\n",
    "\n",
    "## Memory Optimizations for 8GB GPU\n",
    "- **Batch size**: 16 (reduced from 128)\n",
    "- **Image size**: 128x128 (reduced from 256x256)\n",
    "- **Mixed precision**: Enabled (FP16)\n",
    "- **UNet channels**: Reduced from (128, 256, 256) to (64, 128, 128)\n",
    "- **Gradient checkpointing**: Can be enabled if needed\n",
    "- **Cache clearing**: Periodic GPU cache clearing to prevent fragmentation\n",
    "- **DataLoader**: num_workers=0, pin_memory=False for memory efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688bf0a",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35cb9a",
   "metadata": {},
   "source": [
    "## Memory Usage Tips\n",
    "\n",
    "If you still encounter OOM errors, try these additional optimizations:\n",
    "\n",
    "1. **Further reduce batch size**: Change `batch_size` to 8 or even 4\n",
    "2. **Reduce image size**: Change `image_size` to 64 (will train faster but lower quality)\n",
    "3. **Enable gradient checkpointing** (uncomment in UNet creation if available)\n",
    "4. **Close other applications** that might be using GPU memory\n",
    "5. **Restart the kernel** to clear any lingering memory allocations\n",
    "\n",
    "Current settings should work on most 8GB GPUs with CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.models import AutoencoderKL\n",
    "from diffusers.models.unets.unet_2d_condition import UNet2DConditionModel\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fcbad",
   "metadata": {},
   "source": [
    "## IMPORTANT: CUDA Setup Check\n",
    "\n",
    "If you see \"Device: cpu\" in the output above, it means PyTorch cannot access the GPU. This is usually because:\n",
    "\n",
    "1. **CUDA version mismatch**: Your PyTorch is built for CUDA 12.x but the cluster has CUDA 11.x\n",
    "2. **PyTorch not installed with CUDA support**\n",
    "\n",
    "Run the diagnostic cell below to check and fix this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba431f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU Diagnostic and Fix\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU DIAGNOSTIC CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check PyTorch version and CUDA support\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        print(\"\\nâœ… GPU is working correctly!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  PyTorch cannot access GPU!\")\n",
    "        \n",
    "        # Check if nvidia-smi works\n",
    "        try:\n",
    "            result = subprocess.run(['nvidia-smi', '--query-gpu=name,driver_version,memory.total', \n",
    "                                   '--format=csv,noheader'], \n",
    "                                  capture_output=True, text=True, timeout=5)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"\\nGPU hardware detected:\")\n",
    "                print(f\"  {result.stdout.strip()}\")\n",
    "                \n",
    "                # Check CUDA version on system\n",
    "                cuda_result = subprocess.run(['nvcc', '--version'], \n",
    "                                           capture_output=True, text=True, timeout=5)\n",
    "                if cuda_result.returncode == 0:\n",
    "                    print(f\"\\nSystem CUDA:\")\n",
    "                    for line in cuda_result.stdout.split('\\n'):\n",
    "                        if 'release' in line.lower():\n",
    "                            print(f\"  {line.strip()}\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(\"SOLUTION: Reinstall PyTorch with correct CUDA version\")\n",
    "                print(\"=\"*70)\n",
    "                print(\"\\nYour PyTorch was built for a different CUDA version than\")\n",
    "                print(\"what's available on this system.\")\n",
    "                print(\"\\nTo fix this, run the following command:\")\n",
    "                print(\"\\n  pip uninstall torch torchvision torchaudio -y\")\n",
    "                print(\"  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "                print(\"\\nThis will install PyTorch with CUDA 11.8 support (matching your system).\")\n",
    "                print(\"\\nWould you like to automatically fix this? (This will take a few minutes)\")\n",
    "                print(\"If yes, run the cell below.\")\n",
    "            else:\n",
    "                print(\"\\nâŒ GPU hardware not detected by nvidia-smi\")\n",
    "                print(\"This job might not have GPU allocated!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError checking GPU: {e}\")\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"âŒ PyTorch is not installed!\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTO-FIX: Reinstall PyTorch with correct CUDA version\n",
    "# WARNING: This will uninstall and reinstall PyTorch (takes 3-5 minutes)\n",
    "# Only run this cell if the diagnostic above showed CUDA is not available\n",
    "\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# print(\"Uninstalling current PyTorch...\")\n",
    "# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \n",
    "#                       \"torch\", \"torchvision\", \"torchaudio\"])\n",
    "\n",
    "# print(\"\\nInstalling PyTorch with CUDA 11.8 support...\")\n",
    "# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \n",
    "#                       \"torch\", \"torchvision\", \"torchaudio\",\n",
    "#                       \"--index-url\", \"https://download.pytorch.org/whl/cu118\"])\n",
    "\n",
    "# print(\"\\nVerifying installation...\")\n",
    "# import torch\n",
    "# print(f\"PyTorch version: {torch.__version__}\")\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"CUDA version: {torch.version.cuda}\")\n",
    "#     print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "#     print(\"\\nâœ… GPU is now working!\")\n",
    "# else:\n",
    "#     print(\"\\nâš ï¸  Still having issues. Please check Slurm job GPU allocation.\")\n",
    "\n",
    "# print(\"\\nâš ï¸  IMPORTANT: Restart the kernel after installing PyTorch!\")\n",
    "# print(\"   Go to: Kernel -> Restart Kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf5ef29",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6d1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def build_transforms(image_size: int = 256) -> transforms.Compose:\n",
    "    \"\"\"Transform MNIST to 3x256x256 in [-1,1] for VAE.\"\"\"\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaec097",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    dataset_root: str\n",
    "    output_dir: str = \"./outputs/train8\"\n",
    "    batch_size: int = 64\n",
    "    num_epochs: int = 1\n",
    "    lr: float = 1e-4\n",
    "    num_train_timesteps: int = 1000\n",
    "    image_size: int = 256\n",
    "    tokenizer_max_length: int = 16\n",
    "    cfg_dropout_p: float = 0.1  # classifier-free guidance dropout during training\n",
    "    seed: int = 42\n",
    "    mixed_precision: bool = False\n",
    "    checkpoint_interval: int = 2000  # Save checkpoint every N steps\n",
    "    # UNet size: we keep it small for MNIST\n",
    "    unet_block_out_channels: tuple[int, ...] = (128, 256, 256)\n",
    "    layers_per_block: int = 2\n",
    "\n",
    "\n",
    "# Configure training parameters - OPTIMIZED FOR 8GB GPU\n",
    "config = TrainConfig(\n",
    "    dataset_root=\"../../datasets\",\n",
    "    output_dir=\"./outputs/train8\",\n",
    "    batch_size=16,  # Reduced from 128 to 16 for 8GB GPU\n",
    "    num_epochs=5,\n",
    "    lr=1e-4,\n",
    "    image_size=128,  # Reduced from 256 to 128 to save memory\n",
    "    mixed_precision=True,  # Enable mixed precision to save memory\n",
    "    checkpoint_interval=2000,  # Save checkpoint every 2000 steps\n",
    ")\n",
    "\n",
    "print(f\"Device: {get_device()}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Epochs: {config.num_epochs}\")\n",
    "print(f\"Learning rate: {config.lr}\")\n",
    "print(f\"Image size: {config.image_size}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"Checkpoint interval: {config.checkpoint_interval} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f908a84",
   "metadata": {},
   "source": [
    "## 4. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b35c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(device: torch.device, config: TrainConfig):\n",
    "    # Pretrained VAE and CLIP text encoder/tokenizer\n",
    "    print(\"Loading pretrained VAE...\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
    "    vae.requires_grad_(False)\n",
    "    vae.eval()\n",
    "    vae.to(device)\n",
    "\n",
    "    print(\"Loading CLIP text encoder...\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    text_encoder.requires_grad_(False)\n",
    "    text_encoder.eval()\n",
    "    text_encoder.to(device)\n",
    "\n",
    "    print(\"Creating UNet2DConditionModel...\")\n",
    "    # Conditional UNet operating in latent space (4 channels)\n",
    "    # Reduced size for memory efficiency on 8GB GPU\n",
    "    unet = UNet2DConditionModel(\n",
    "        sample_size=config.image_size // 8,  # 16 for 128x128\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        layers_per_block=2,  # Reduced\n",
    "        block_out_channels=(64, 128, 128),  # Reduced from (128, 256, 256)\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",\n",
    "            \"CrossAttnDownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\",\n",
    "            \"CrossAttnUpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "        ),\n",
    "        cross_attention_dim=512,  # CLIP ViT-B/32 hidden size\n",
    "    ).to(device)\n",
    "\n",
    "    num_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "    print(f\"UNet trainable parameters: {num_params:,}\")\n",
    "\n",
    "    return vae, tokenizer, text_encoder, unet\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "device = get_device()\n",
    "seed_everything(config.seed)\n",
    "vae, tokenizer, text_encoder, unet = create_models(device, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0e6ed",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(config: TrainConfig) -> DataLoader:\n",
    "    tfms = build_transforms(config.image_size)\n",
    "    ds = datasets.MNIST(root=config.dataset_root, train=True, download=True, transform=tfms)\n",
    "    # Reduced num_workers and disabled pin_memory for better memory management\n",
    "    return DataLoader(ds, batch_size=config.batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
    "\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = make_dataloader(config)\n",
    "print(f\"Dataset size: {len(dataloader.dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ae520",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch\n",
    "sample_images, sample_labels = next(iter(dataloader))\n",
    "print(f\"Batch shape: {sample_images.shape}\")\n",
    "print(f\"Labels: {sample_labels[:16].tolist()}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(sample_images):\n",
    "        img = sample_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img + 1) / 2  # Denormalize from [-1,1] to [0,1]\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Digit {sample_labels[i].item()}\")\n",
    "        ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a1895",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727e8541",
   "metadata": {},
   "source": [
    "## 7. Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf54e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(output_dir: str):\n",
    "    \"\"\"Find the latest checkpoint in the output directory.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        return None\n",
    "    \n",
    "    # Look for checkpoint files\n",
    "    checkpoint_files = []\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.endswith('.pt') and (filename.startswith('unet_step_') or filename.startswith('unet_epoch_')):\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            checkpoint_files.append(filepath)\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    # Get the most recently modified checkpoint\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)\n",
    "    return latest_checkpoint\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path: str, unet, optimizer=None):\n",
    "    \"\"\"Load checkpoint and return metadata.\"\"\"\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # Handle different checkpoint formats\n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'unet' in checkpoint:\n",
    "            unet.load_state_dict(checkpoint['unet'])\n",
    "        else:\n",
    "            unet.load_state_dict(checkpoint)\n",
    "        \n",
    "        # Load optimizer state if available\n",
    "        if optimizer is not None and 'optimizer' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = {\n",
    "            'global_step': checkpoint.get('global_step', checkpoint.get('step', 0)),\n",
    "            'epoch': checkpoint.get('epoch', 0),\n",
    "            'batch_losses': checkpoint.get('batch_losses', []),\n",
    "            'epoch_losses': checkpoint.get('epoch_losses', []),\n",
    "        }\n",
    "    else:\n",
    "        unet.load_state_dict(checkpoint)\n",
    "        metadata = {'global_step': 0, 'epoch': 0, 'batch_losses': [], 'epoch_losses': []}\n",
    "    \n",
    "    print(f\"Resumed from step {metadata['global_step']}, epoch {metadata['epoch']}\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str, unet, optimizer, global_step: int, epoch: int, \n",
    "                   batch_losses: List[float], epoch_losses: List[float]):\n",
    "    \"\"\"Save checkpoint with complete metadata.\"\"\"\n",
    "    checkpoint = {\n",
    "        'unet': unet.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'global_step': global_step,\n",
    "        'epoch': epoch,\n",
    "        'batch_losses': batch_losses,\n",
    "        'epoch_losses': epoch_losses,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "# Check for existing checkpoints\n",
    "latest_checkpoint = find_latest_checkpoint(config.output_dir)\n",
    "if latest_checkpoint:\n",
    "    print(f\"Found existing checkpoint: {latest_checkpoint}\")\n",
    "    print(\"Training will resume from this checkpoint.\")\n",
    "else:\n",
    "    print(\"No existing checkpoints found. Starting training from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd123c",
   "metadata": {},
   "source": [
    "### Checkpoint System Features\n",
    "\n",
    "The checkpoint system provides:\n",
    "\n",
    "1. **Automatic Resume**: Detects and loads the latest checkpoint automatically\n",
    "2. **Complete Metadata**: Each checkpoint stores:\n",
    "   - UNet model weights\n",
    "   - Optimizer state (for proper resume)\n",
    "   - Global step count\n",
    "   - Current epoch number\n",
    "   - Complete batch loss history\n",
    "   - Complete epoch loss history\n",
    "3. **Training Plots**: Loss plots saved with every checkpoint\n",
    "4. **Multiple Checkpoint Types**:\n",
    "   - Step checkpoints: `unet_step_2000.pt` (every 2000 steps)\n",
    "   - Epoch checkpoints: `unet_epoch_1.pt` (after each epoch)\n",
    "   - Final checkpoint: `unet_final.pt` (at completion)\n",
    "\n",
    "If training is interrupted, simply re-run the training cell and it will automatically resume from the latest checkpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14cff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: TrainConfig, vae, tokenizer, text_encoder, unet, dataloader, device, resume_from_checkpoint: str = None):\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "    # Noise scheduler for training\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=config.num_train_timesteps, beta_schedule=\"squaredcos_cap_v2\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(unet.parameters(), lr=config.lr)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.mixed_precision and device.type == \"cuda\")\n",
    "\n",
    "    # Loss tracking and training state\n",
    "    batch_losses = []\n",
    "    epoch_losses = []\n",
    "    global_step = 0\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Resume from checkpoint if available\n",
    "    if resume_from_checkpoint:\n",
    "        metadata = load_checkpoint(resume_from_checkpoint, unet, optimizer)\n",
    "        global_step = metadata['global_step']\n",
    "        start_epoch = metadata['epoch']\n",
    "        batch_losses = metadata['batch_losses']\n",
    "        epoch_losses = metadata['epoch_losses']\n",
    "        print(f\"Resuming training from step {global_step}, epoch {start_epoch}\")\n",
    "\n",
    "    unet.train()\n",
    "    \n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        epoch_loss_sum = 0.0\n",
    "        epoch_batch_count = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "\n",
    "            # Create text captions from labels\n",
    "            captions: List[str] = [f\"A handwritten digit {int(l)}\" for l in labels]\n",
    "\n",
    "            # Tokenize (with classifier-free guidance dropout during training)\n",
    "            if np.random.rand() < config.cfg_dropout_p:\n",
    "                captions_input = [\"\"] * len(captions)\n",
    "            else:\n",
    "                captions_input = captions\n",
    "\n",
    "            text_inputs = tokenizer(\n",
    "                captions_input,\n",
    "                padding=\"max_length\",\n",
    "                max_length=config.tokenizer_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Encode images to latents using frozen VAE\n",
    "                latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "                # Prepare text embeddings\n",
    "                text_embeddings = text_encoder(text_inputs[\"input_ids\"]).last_hidden_state\n",
    "\n",
    "            # Sample noise and timestep; add noise\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Predict noise\n",
    "            with torch.autocast(\n",
    "                device_type=device.type,\n",
    "                dtype=torch.float16 if (config.mixed_precision and device.type == \"cuda\") else torch.float32,\n",
    "                enabled=config.mixed_precision\n",
    "            ):\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=text_embeddings).sample\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Record loss\n",
    "            loss_value = loss.item()\n",
    "            batch_losses.append(loss_value)\n",
    "            epoch_loss_sum += loss_value\n",
    "            epoch_batch_count += 1\n",
    "\n",
    "            global_step += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{loss_value:.4f}\", \"step\": global_step})\n",
    "\n",
    "            # Clear cache every 50 batches to prevent fragmentation\n",
    "            if batch_idx % 50 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Save periodic checkpoints\n",
    "            if global_step % config.checkpoint_interval == 0:\n",
    "                ckpt_path = os.path.join(config.output_dir, f\"unet_step_{global_step}.pt\")\n",
    "                save_checkpoint(ckpt_path, unet, optimizer, global_step, epoch, batch_losses, epoch_losses)\n",
    "                print(f\"\\nCheckpoint saved: {ckpt_path}\")\n",
    "                \n",
    "                # Save training plots with the checkpoint\n",
    "                plot_path = os.path.join(config.output_dir, f\"training_loss_step_{global_step}.png\")\n",
    "                save_loss_plot(batch_losses, epoch_losses, plot_path)\n",
    "                print(f\"Training plot saved: {plot_path}\")\n",
    "\n",
    "        # Record epoch average loss\n",
    "        avg_epoch_loss = epoch_loss_sum / epoch_batch_count if epoch_batch_count > 0 else 0.0\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{config.num_epochs} - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # Save per-epoch checkpoint\n",
    "        ckpt_path = os.path.join(config.output_dir, f\"unet_epoch_{epoch+1}.pt\")\n",
    "        save_checkpoint(ckpt_path, unet, optimizer, global_step, epoch + 1, batch_losses, epoch_losses)\n",
    "        \n",
    "        # Save training plots with epoch checkpoint\n",
    "        plot_path = os.path.join(config.output_dir, f\"training_loss_epoch_{epoch+1}.png\")\n",
    "        save_loss_plot(batch_losses, epoch_losses, plot_path)\n",
    "        print(f\"Training plot saved: {plot_path}\")\n",
    "        \n",
    "        # Clear cache after each epoch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Save final\n",
    "    final_path = os.path.join(config.output_dir, \"unet_final.pt\")\n",
    "    save_checkpoint(final_path, unet, optimizer, global_step, config.num_epochs, batch_losses, epoch_losses)\n",
    "    print(f\"\\nFinal model saved: {final_path}\")\n",
    "    \n",
    "    # Save final training plots\n",
    "    final_plot_path = os.path.join(config.output_dir, \"training_loss_final.png\")\n",
    "    save_loss_plot(batch_losses, epoch_losses, final_plot_path)\n",
    "    print(f\"Final training plot saved: {final_plot_path}\")\n",
    "\n",
    "    return batch_losses, epoch_losses\n",
    "\n",
    "\n",
    "def save_loss_plot(batch_losses: List[float], epoch_losses: List[float], save_path: str):\n",
    "    \"\"\"Helper function to save loss plots without displaying them.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Per-batch loss\n",
    "    if len(batch_losses) > 0:\n",
    "        axes[0].plot(batch_losses, linewidth=0.8, alpha=0.7)\n",
    "        axes[0].set_xlabel(\"Batch\")\n",
    "        axes[0].set_ylabel(\"Loss\")\n",
    "        axes[0].set_title(\"Training Loss per Batch\")\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Per-epoch loss\n",
    "    if len(epoch_losses) > 0:\n",
    "        axes[1].plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', linewidth=2)\n",
    "        axes[1].set_xlabel(\"Epoch\")\n",
    "        axes[1].set_ylabel(\"Average Loss\")\n",
    "        axes[1].set_title(\"Training Loss per Epoch\")\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.close(fig)  # Close the figure to free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ed648",
   "metadata": {},
   "source": [
    "## 9. Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86d333",
   "metadata": {},
   "source": [
    "## 8.5. Memory Optimization (Clear GPU Cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157fa6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache before training to maximize available memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    # Set memory allocation configuration for better fragmentation handling\n",
    "    import os\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    # Print current memory status\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(\"Memory cache cleared and optimized for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4677265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoints and resume if available\n",
    "latest_checkpoint = find_latest_checkpoint(config.output_dir)\n",
    "\n",
    "# Train the model (will resume from checkpoint if found)\n",
    "batch_losses, epoch_losses = train(\n",
    "    config, vae, tokenizer, text_encoder, unet, dataloader, device, \n",
    "    resume_from_checkpoint=latest_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747ee8d6",
   "metadata": {},
   "source": [
    "## 10. Visualize Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfce6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(batch_losses: List[float], epoch_losses: List[float], output_dir: str = None):\n",
    "    \"\"\"Generate and save loss plots for per-batch and per-epoch losses.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Per-batch loss\n",
    "    axes[0].plot(batch_losses, linewidth=0.8, alpha=0.7)\n",
    "    axes[0].set_xlabel(\"Batch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Training Loss per Batch\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Per-epoch loss\n",
    "    if len(epoch_losses) > 0:\n",
    "        axes[1].plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', linewidth=2)\n",
    "        axes[1].set_xlabel(\"Epoch\")\n",
    "        axes[1].set_ylabel(\"Average Loss\")\n",
    "        axes[1].set_title(\"Training Loss per Epoch\")\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        loss_plot_path = os.path.join(output_dir, \"training_loss.png\")\n",
    "        plt.savefig(loss_plot_path, dpi=150)\n",
    "        print(f\"Loss plots saved to {loss_plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the losses\n",
    "plot_losses(batch_losses, epoch_losses, config.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce04e21",
   "metadata": {},
   "source": [
    "## 11. Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e813cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(\n",
    "    prompt: str,\n",
    "    guidance_scale: float = 7.5,\n",
    "    num_inference_steps: int = 50,\n",
    "    seed: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"Generate an image from a text prompt.\"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    # Get conditioned and unconditioned text embeddings for classifier-free guidance\n",
    "    text_inputs = tokenizer(\n",
    "        [prompt], padding=\"max_length\", max_length=16, truncation=True, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    text_embeddings = text_encoder(text_inputs.input_ids).last_hidden_state\n",
    "\n",
    "    uncond_inputs = tokenizer([\"\"], padding=\"max_length\", max_length=16, return_tensors=\"pt\").to(device)\n",
    "    uncond_embeddings = text_encoder(uncond_inputs.input_ids).last_hidden_state\n",
    "\n",
    "    encoder_hidden_states = torch.cat([uncond_embeddings, text_embeddings], dim=0)\n",
    "\n",
    "    # Init random latents in latent space\n",
    "    latents = torch.randn((1, 4, config.image_size // 8, config.image_size // 8), device=device)\n",
    "\n",
    "    unet.eval()\n",
    "    for t in tqdm(scheduler.timesteps, desc=\"Sampling\"):\n",
    "        # Expand for classifier-free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2, dim=0)\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict noise\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=encoder_hidden_states).sample\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # Step\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    # Decode latents to image\n",
    "    latents = latents / 0.18215\n",
    "    image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu()\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec8ea6",
   "metadata": {},
   "source": [
    "## 12. Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a62b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples for different digits\n",
    "prompts = [\n",
    "    \"A handwritten digit 0\",\n",
    "    \"A handwritten digit 1\",\n",
    "    \"A handwritten digit 2\",\n",
    "    \"A handwritten digit 3\",\n",
    "    \"A handwritten digit 4\",\n",
    "    \"A handwritten digit 5\",\n",
    "    \"A handwritten digit 6\",\n",
    "    \"A handwritten digit 7\",\n",
    "    \"A handwritten digit 8\",\n",
    "    \"A handwritten digit 9\",\n",
    "]\n",
    "\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 50\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, (ax, prompt) in enumerate(zip(axes.flat, prompts)):\n",
    "    print(f\"Generating: {prompt}\")\n",
    "    generated_image = sample(\n",
    "        prompt=prompt,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        seed=42 + i,\n",
    "    )\n",
    "    \n",
    "    # Display image\n",
    "    img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(prompt)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.output_dir, \"generated_samples.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636675ad",
   "metadata": {},
   "source": [
    "## 13. Test Different Guidance Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30661111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different guidance scales for a single prompt\n",
    "test_prompt = \"A handwritten digit 7\"\n",
    "guidance_scales = [0, 3, 5, 7.5, 10, 15]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(guidance_scales), figsize=(20, 4))\n",
    "for ax, gs in zip(axes, guidance_scales):\n",
    "    print(f\"Generating with guidance scale {gs}\")\n",
    "    generated_image = sample(\n",
    "        prompt=test_prompt,\n",
    "        guidance_scale=gs,\n",
    "        num_inference_steps=50,\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Guidance Scale: {gs}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.output_dir, \"guidance_scale_comparison.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621602c",
   "metadata": {},
   "source": [
    "## 14. Save Individual Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc55d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save a single high-quality sample\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "prompt = \"A handwritten digit 3\"\n",
    "print(f\"Generating: {prompt}\")\n",
    "\n",
    "generated_image = sample(\n",
    "    prompt=prompt,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=50,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "output_path = os.path.join(config.output_dir, \"sample_digit_3.png\")\n",
    "save_image(generated_image, output_path)\n",
    "print(f\"Sample saved to: {output_path}\")\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(6, 6))\n",
    "img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "plt.imshow(img)\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
