================================================================================
MNIST NOTEBOOK TO SCRIPT CONVERSION - FILES CREATED
================================================================================
Date: January 6, 2026
Original Notebook: notebooks-old/complete_new_model/diffusers/train3_working_with_prompts_mnist.ipynb
Target Project: stable_diffusion_research

================================================================================
PRIMARY FILES (Required for Training)
================================================================================

1. scripts/train_mnist.py
   - Main training script (601 lines)
   - Features: Multi-GPU, checkpointing, sample generation
   - Usage: python scripts/train_mnist.py [options]
   - Status: ✅ Production Ready

2. configs/mnist.yaml
   - Configuration file for MNIST training
   - Specifies model architecture, training params, data settings
   - Can be extended or modified for different settings

3. slurm/train_mnist.sh
   - SLURM job submission script for HPC cluster
   - Configured for 2 GPU training with Accelerate
   - Usage: sbatch slurm/train_mnist.sh

================================================================================
DOCUMENTATION FILES (Essential Reading)
================================================================================

1. scripts/README_MNIST.md (350+ lines)
   - Complete training guide
   - Model architecture details
   - Troubleshooting section
   - Future enhancements

2. QUICK_START_MNIST.md (120+ lines)
   - Quick reference guide
   - One-liner examples
   - Parameter combinations
   - Common issues and solutions

3. MNIST_SCRIPT_CONVERSION.md (250+ lines)
   - Detailed conversion notes
   - Architecture specifications
   - Performance characteristics
   - Feature comparisons (notebook vs script)

4. CONVERSION_COMPLETE.md
   - Summary of conversion
   - Project structure integration
   - Quick usage examples
   - Next steps

5. FILES_CREATED.txt (this file)
   - Index of all created files

================================================================================
QUICK START
================================================================================

Single GPU Training:
  cd /home/doshlom4/work/final_project/stable_diffusion_research
  python scripts/train_mnist.py

Multi-GPU Training (2x GPUs):
  cd /home/doshlom4/work/final_project/stable_diffusion_research
  accelerate launch --num_processes=2 scripts/train_mnist.py

HPC Cluster Training:
  cd /home/doshlom4/work/final_project/stable_diffusion_research
  sbatch slurm/train_mnist.sh

Custom Parameters:
  python scripts/train_mnist.py --num_epochs 20 --batch_size 256 --guidance_scale 10.0

================================================================================
OUTPUT LOCATIONS
================================================================================

Training outputs are saved to: outputs/mnist/

Structure:
  outputs/mnist/
  ├── checkpoints/
  │   ├── checkpoint_epoch_000.pt
  │   ├── checkpoint_epoch_001.pt
  │   └── ... (one per epoch)
  └── samples/
      ├── samples_epoch_000.png
      ├── samples_epoch_001.png
      └── ... (one per epoch)

================================================================================
KEY FEATURES
================================================================================

✅ Multi-GPU Training
   - Native Accelerate framework integration
   - Supports 1, 2, 4, 8+ GPUs with simple CLI argument

✅ Flexible Configuration
   - Command-line arguments for all major parameters
   - YAML config file for advanced customization
   - Easy to extend and modify

✅ Production Ready
   - Error handling and validation
   - Checkpoint saving and resumption
   - Reproducible results (seed management)
   - Comprehensive logging

✅ Comprehensive Documentation
   - 700+ lines of documentation
   - Multiple quick-start guides
   - Architecture specifications
   - Troubleshooting guide

✅ HPC Cluster Ready
   - SLURM job submission script
   - Environment validation
   - Multi-GPU launch with Accelerate

================================================================================
COMMAND REFERENCE
================================================================================

Display Help:
  python scripts/train_mnist.py --help

Training Examples:

  # Minimal setup (5 epochs, 512 batch)
  python scripts/train_mnist.py

  # Standard setup (10 epochs, 256 batch)
  python scripts/train_mnist.py --num_epochs 10 --batch_size 256

  # Multi-GPU (2 GPUs, mixed precision)
  accelerate launch --num_processes=2 --mixed_precision=fp16 scripts/train_mnist.py

  # Strong guidance, fast sampling
  python scripts/train_mnist.py --guidance_scale 10.0 --num_inference_steps 25

  # Custom output location
  python scripts/train_mnist.py --output_dir /tmp/my_mnist_experiment

  # Reproducible run with seed
  python scripts/train_mnist.py --seed 12345 --num_epochs 20

================================================================================
PARAMETER DESCRIPTIONS
================================================================================

--num_epochs              Number of training epochs (default: 5)
--batch_size              Batch size for training (default: 512)
--learning_rate           Learning rate for optimizer (default: 1e-3)
--tokenizer_max_length    Max tokens for text (default: 8)
--num_inference_steps     Denoising steps when sampling (default: 50)
--guidance_scale          Classifier-free guidance strength (default: 8.0)
--output_dir              Output directory for checkpoints/samples
--dataset_path            Path to MNIST dataset cache
--seed                    Random seed for reproducibility (default: 422)
--num_samples_per_epoch   Samples to generate per epoch (default: 6)

================================================================================
DEPENDENCIES
================================================================================

All dependencies are available in the HPC conda environment:
  /home/doshlom4/work/conda/envs/shlomid_conda_12_11_2025/

Required packages:
  ✅ torch >= 2.0
  ✅ transformers (CLIP models)
  ✅ diffusers (noise scheduler, pipeline)
  ✅ accelerate (distributed training)
  ✅ torchvision (MNIST dataset)
  ✅ pillow (image processing)
  ✅ matplotlib (visualization)

Verify with:
  python scripts/check_environment.py

================================================================================
INTEGRATION WITH PROJECT
================================================================================

Files follow the stable_diffusion_research structure:

✅ Uses src/utils/config.py for configuration management
✅ Compatible with src/models/ infrastructure
✅ Follows src/training/ conventions
✅ Integrates with existing SLURM setup
✅ Extensible to larger models/datasets

This allows easy evolution from MNIST to:
  - CIFAR-10 training
  - ImageNet training
  - Custom dataset training
  - Larger model architectures

================================================================================
NEXT STEPS
================================================================================

1. Verify environment:
   python scripts/check_environment.py

2. Start training (single GPU):
   python scripts/train_mnist.py --num_epochs 5

3. Monitor training:
   tail -f outputs/mnist/log.txt

4. Check generated samples:
   ls outputs/mnist/samples/

5. Review checkpoints:
   ls outputs/mnist/checkpoints/

For more details, see: QUICK_START_MNIST.md

================================================================================
SUPPORT & DOCUMENTATION
================================================================================

For issues, check these files in order:
  1. QUICK_START_MNIST.md (quick reference)
  2. scripts/README_MNIST.md (full guide)
  3. MNIST_SCRIPT_CONVERSION.md (conversion details)
  4. Inline comments in train_mnist.py

To verify environment:
  python scripts/check_environment.py

To check SLURM logs:
  tail slurm/logs/mnist_diffusion_*.out

================================================================================
