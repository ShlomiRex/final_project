{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76054edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from model import Model\n",
    "import mlflow\n",
    "import mlflow.sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca863e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 1 # Try more!\n",
    "\n",
    "IMG_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "T = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e3a70da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train = torchvision.datasets.FashionMNIST(root=\"./data\", download=True, transform=data_transform, train=True)\n",
    "\n",
    "test = torchvision.datasets.FashionMNIST(root=\"./data\", download=True, transform=data_transform, train=False)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa792908",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 3 (1683370079.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31moptimizer.zero_grad()\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'for' statement on line 3\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\", unit=\"epoch\"):\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            imgs, labels = batch\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "            imgs = torch.nn.functional.pad(imgs, (2, 2, 2, 2))  # Padding to make it compatible with the model input size\n",
    "            loss = model.get_loss(model, imgs, t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #   if epoch % 5 == 0 and step == 0:\n",
    "            #     print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "            #     sample_plot_image()\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-conditioned-image-generation-using-st-35DVCAXA-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
