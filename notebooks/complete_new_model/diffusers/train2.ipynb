{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "ba903ac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:57:34.850963Z",
     "start_time": "2025-07-03T08:57:34.845624Z"
    },
    "id": "ba903ac1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple, Optional, Any\n",
    "import math\n",
    "from torch import Tensor\n",
    "from einops import rearrange\n",
    "from diffusers import DDPMScheduler\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import einops\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union, Dict, Any, Optional, BinaryIO\n",
    "from PIL import Image, ImageColor, ImageDraw, ImageFont\n",
    "import tqdm\n",
    "import abc  # Abstract base class\n",
    "import numpy as np\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "b6d07ac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:57:35.325181Z",
     "start_time": "2025-07-03T08:57:35.322182Z"
    },
    "id": "b6d07ac8"
   },
   "outputs": [],
   "source": [
    "#@title Hyperparameters\n",
    "\n",
    "T = 1000 # Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "68a4ecb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:57:35.709935Z",
     "start_time": "2025-07-03T08:57:35.706259Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68a4ecb4",
    "outputId": "9a518c8e-1be8-4c51-93fe-f5bce4eb9b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#@title Device / CUDA\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "d0a30cce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:57:35.978987Z",
     "start_time": "2025-07-03T08:57:35.966448Z"
    },
    "id": "d0a30cce"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataPack:\n",
    "    train_dataset: Dataset\n",
    "    train_loader: DataLoader\n",
    "    val_dataset: Dataset\n",
    "    val_loader: DataLoader\n",
    "    transform_to_tensor: Any\n",
    "    transform_to_pil: Any\n",
    "    in_channels: int\n",
    "    out_channels: int\n",
    "    num_classes: int\n",
    "    recommended_steps: Tuple[int]\n",
    "    recommended_attn_step_indexes: List[int]\n",
    "\n",
    "class MNISTTransformation:\n",
    "    def __call__(self, tensor: torch.Tensor):\n",
    "        return (tensor * -1 + 1).permute(1,2,0).detach().cpu().numpy()\n",
    "\n",
    "def get_mnist_loader_and_transform(\n",
    "    path_to_dataset: str = \"../../../datasets\",\n",
    "    batch_size: int = 128,\n",
    "    num_workers: int = 2\n",
    ") -> DataPack:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),        # Ensure input size matches UNet2DModel\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.MNIST(root=path_to_dataset, download=True, transform=transform)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    val_dataset = torchvision.datasets.MNIST(root=path_to_dataset, download=True, transform=transform, train=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    return DataPack(\n",
    "        train_dataset=train_dataset,\n",
    "        train_loader=train_dataloader,\n",
    "        val_dataset=val_dataset,\n",
    "        val_loader=val_dataloader,\n",
    "        transform_to_tensor=transform,\n",
    "        transform_to_pil=MNISTTransformation(),\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        num_classes=10,\n",
    "        recommended_steps=(1,2,4),\n",
    "        recommended_attn_step_indexes=[1]\n",
    "    )\n",
    "\n",
    "class CifarTransformation:\n",
    "    def __call__(self, tensor: torch.Tensor):\n",
    "        return (tensor * 127.5 + 127.5).long().clip(0,255).permute(1,2,0).detach().cpu().numpy()\n",
    "\n",
    "def get_cifar10_loader_and_transform(\n",
    "    path_to_dataset: str = \"../../../datasets\",\n",
    "    batch_size: int = 128,\n",
    "    num_workers: int = 2\n",
    ") -> DataPack:\n",
    "    transform_to_tensor = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((.5,.5,.5), (.5,.5,.5))\n",
    "    ])\n",
    "    transform_to_pil = CifarTransformation()\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=path_to_dataset, download=True, transform=transform_to_tensor)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    val_dataset = torchvision.datasets.CIFAR10(root=path_to_dataset, download=True, transform=transform_to_tensor, train=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    return DataPack(\n",
    "        train_dataset=train_dataset,\n",
    "        train_loader=train_dataloader,\n",
    "        val_dataset=val_dataset,\n",
    "        val_loader=val_dataloader,\n",
    "        transform_to_tensor=transform_to_tensor,\n",
    "        transform_to_pil=transform_to_pil,\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        num_classes=10,\n",
    "        recommended_steps=(1,2,2,2),\n",
    "        recommended_attn_step_indexes=[1,2]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3bb5dfda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:57:38.246171Z",
     "start_time": "2025-07-03T08:57:38.240472Z"
    }
   },
   "outputs": [],
   "source": [
    "from diffusers.models.unets.unet_2d_blocks import (\n",
    "    AttnDownBlock2D as DiffAttnDownBlock2D,\n",
    "    DownBlock2D as DiffDownBlock2D,\n",
    "    CrossAttnDownBlock2D as DiffCrossAttnDownBlock2D,\n",
    "    AttnUpBlock2D as DiffAttnUpBlock2D,\n",
    "    UpBlock2D as DiffUpBlock2D,\n",
    "    ResnetBlock2D as DiffResnetBlock2D,\n",
    ")\n",
    "\n",
    "from diffusers.models.unets.unet_2d_condition import UNet2DConditionModel\n",
    "\n",
    "from diffusers import DDPMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "febfd77b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:57:52.361773Z",
     "start_time": "2025-07-03T08:57:52.353043Z"
    },
    "id": "febfd77b"
   },
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional DDPM with UNet backbone and support for training + inference.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        T: int,\n",
    "        unet: UNet2DConditionModel,\n",
    "        noise_scheduler: DDPMScheduler,\n",
    "        device: str\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.unet = unet.to(device)\n",
    "        self.noise_sched = noise_scheduler\n",
    "        self.device = device\n",
    "\n",
    "        # Pre-compute terms from beta schedule\n",
    "        bar_alpha_t = self.noise_sched.alphas_cumprod.to(device)\n",
    "        self.register_buffer(\"sqrt_bar_alpha_t\", torch.sqrt(bar_alpha_t))  # √ᾱ_t\n",
    "        self.register_buffer(\"sqrt_minus_bar_alpha_t_schedule\", torch.sqrt(1 - bar_alpha_t))  # √(1 - ᾱ_t)\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, batch: dict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward diffusion + loss. Used in training.\n",
    "        \"\"\"\n",
    "        imgs = batch[\"image\"].to(self.device)\n",
    "        lbls = batch[\"label\"]\n",
    "        text = batch[\"text\"]\n",
    "        text_embedding = batch[\"text_embedding\"]\n",
    "\n",
    "        b, c, h, w = imgs.shape\n",
    "\n",
    "        # Sample time steps\n",
    "        t = torch.randint(low=0, high=self.T, size=(b,), device=self.device, dtype=torch.long)\n",
    "\n",
    "        # Sample Gaussian noise\n",
    "        noise = torch.randn_like(imgs, device=self.device)\n",
    "\n",
    "        # q(x_t | x_0): noisy image generation\n",
    "        noisy_imgs = self.sqrt_bar_alpha_t[t].view(b, 1, 1, 1) * imgs + \\\n",
    "                     self.sqrt_minus_bar_alpha_t_schedule[t].view(b, 1, 1, 1) * noise\n",
    "\n",
    "        # Predict noise with UNet\n",
    "        pred_noise = self.unet(noisy_imgs, t, encoder_hidden_states=text_embedding).sample\n",
    "\n",
    "        # Compute MSE loss between predicted and true noise\n",
    "        return self.criterion(pred_noise, noise)\n",
    "\n",
    "    def predict_noise(self, noisy_imgs: torch.Tensor, t: torch.Tensor, cond: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict noise given noisy image x_t and timestep t (used during inference).\n",
    "        \"\"\"\n",
    "        return self.unet(noisy_imgs, t, cond).sample\n",
    "\n",
    "    def sample(self, shape: tuple, text_encoder, batch) -> torch.Tensor:\n",
    "        b, c, h, w = shape\n",
    "\n",
    "        sample = torch.randn((b, c, h, w), device=self.device)\n",
    "\n",
    "        imgs, txt, lbls = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for t in reversed(range(self.noise_sched.num_train_timesteps)):\n",
    "                t_tensor = torch.full((b,), t, device=self.device, dtype=torch.long)\n",
    "\n",
    "                # Get the text embedding for conditioning\n",
    "                encoder_hidden_states = text_encoder(txt, return_dict=False)[0]\n",
    "\n",
    "                # Predict noise\n",
    "                noise_pred = self.unet(sample, t_tensor, encoder_hidden_states).sample\n",
    "\n",
    "                # Run step per item in batch since diffusers expects scalar timestep\n",
    "                prev_samples = []\n",
    "                for i in range(b):\n",
    "                    out = self.noise_sched.step(\n",
    "                        model_output=noise_pred[i:i+1],  # single sample\n",
    "                        timestep=t_tensor[i].cpu(),     # scalar\n",
    "                        sample=sample[i:i+1]            # single sample\n",
    "                    ).prev_sample\n",
    "                    prev_samples.append(out)\n",
    "\n",
    "                sample = torch.cat(prev_samples, dim=0)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "6fc22e6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:57:53.111161Z",
     "start_time": "2025-07-03T08:57:52.644509Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fc22e6f",
    "outputId": "0434961c-fe31-4f6a-e32e-a5d126633a01"
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# data = get_mnist_loader_and_transform(batch_size=batch_size)\n",
    "\n",
    "cond_dim = 10\n",
    "\n",
    "unet = UNet2DConditionModel(\n",
    "    sample_size=32,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    layers_per_block=1,\n",
    "    block_out_channels=(112, 224, 336, 448),\n",
    "    norm_num_groups=16,\n",
    "    addition_embed_type=\"text\",  # `addition_embed_type`: simple_projection must be None, 'text', 'text_image', 'text_time', 'image', or 'image_hint'.\n",
    "    encoder_hid_dim=512\n",
    ").to(device)\n",
    "\n",
    "model = DDPM(\n",
    "    T=T,\n",
    "    unet = unet,\n",
    "    noise_scheduler = DDPMScheduler(T),\n",
    "    device = device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "32642fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in the model: 64,325,521\n"
     ]
    }
   ],
   "source": [
    "num_of_params = sum([p.numel() for p in model.parameters()])\n",
    "print(\"Number of trainable parameters in the model: \" + str(f\"{num_of_params:,}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9235f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTWithTextLabel(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        self.dataset = datasets.MNIST(root=root, train=train, download=True, transform=transform)\n",
    "        self.label_to_text = {\n",
    "            0: \"zero\", 1: \"one\", 2: \"two\", 3: \"three\", 4: \"four\",\n",
    "            5: \"five\", 6: \"six\", 7: \"seven\", 8: \"eight\", 9: \"nine\"\n",
    "        }\n",
    "\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\", use_safetensors=True)\n",
    "        self.text_encoder = self.text_encoder.eval().to(device)  # disable training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        text = self.label_to_text[label]\n",
    "\n",
    "        # Tokenize the text and move to the correct device\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(self.text_encoder.device)\n",
    "\n",
    "        # Get the text embedding (last_hidden_state or pooled output)\n",
    "        with torch.no_grad():\n",
    "            embedding = self.text_encoder(**inputs).last_hidden_state  # shape: [1, seq_len, hidden_dim]\n",
    "\n",
    "        embedding = embedding.mean(dim=1)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"label\": label,\n",
    "            \"text\": text,\n",
    "            \"text_embedding\": embedding  # shape: [768] for CLIP ViT-B/32\n",
    "        }\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = MNISTWithTextLabel(root=\"../../datasets\", train=True, transform=transform)\n",
    "val_dataset = MNISTWithTextLabel(root=\"../../datasets\", train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "6a0244cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:57:54.337202Z",
     "start_time": "2025-07-03T08:57:54.332083Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_and_plot(model: DDPM, device: str, num_samples: int = 8):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cond = torch.randn((num_samples, 10, 64), device=device)  # random condition\n",
    "        samples = model.sample((num_samples, 1, 32, 32), cond=cond).cpu()\n",
    "        grid = torchvision.utils.make_grid(samples, nrow=4, normalize=True, value_range=(0, 1))\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Generated Samples\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ea163",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e03c442a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T08:57:54.669360Z",
     "start_time": "2025-07-03T08:57:54.662835Z"
    },
    "id": "e03c442a"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: DDPM,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epochs: int,\n",
    "    device: str,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    sample_every: int = 1,\n",
    "):\n",
    "    training_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        training_loss = 0.0\n",
    "        pbar = tqdm.tqdm(train_dataloader, desc=f\"Epoch {epoch}\")\n",
    "\n",
    "        for index, batch in enumerate(pbar):\n",
    "            loss = model(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.item()\n",
    "            pbar.set_postfix(loss=training_loss / (index + 1))\n",
    "        \n",
    "        if epoch % sample_every == 0:\n",
    "            sample_and_plot(model, device)\n",
    "\n",
    "        training_loss /= len(train_dataloader)\n",
    "        training_losses.append(training_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_dataloader:\n",
    "                imgs = imgs.to(device)\n",
    "                cond = torch.nn.functional.one_hot(labels, num_classes=10).float().to(device)\n",
    "                loss = model(imgs, cond=cond)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {training_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    sample_and_plot(model, device)\n",
    "    return training_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc69501",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T09:01:26.710322Z",
     "start_time": "2025-07-03T09:01:01.641651Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8bc69501",
    "outputId": "1aa71866-0ff7-4eaf-9900-655d4cea7cfe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    optimizer=torch.optim.Adam(params=model.parameters(), lr=2e-4),\n",
    "    epochs=10,\n",
    "    device=device,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
