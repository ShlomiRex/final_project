{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0626ad",
   "metadata": {},
   "source": [
    "# ðŸ”§ System Diagnostic - Run This First!\n",
    "\n",
    "Before training, we need to verify that the GPU is properly accessible. This cell will check:\n",
    "1. Slurm job allocation\n",
    "2. GPU hardware detection\n",
    "3. CUDA environment variables\n",
    "4. PyTorch CUDA compatibility\n",
    "5. Common issues and solutions\n",
    "\n",
    "**Run the diagnostic cell below FIRST before proceeding with training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Comprehensive GPU Diagnostic for HPC Cluster\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" ðŸ” COMPREHENSIVE GPU DIAGNOSTIC FOR HPC CLUSTER\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SLURM JOB INFORMATION\n",
    "# ============================================================================\n",
    "print(\"1ï¸âƒ£  SLURM JOB ALLOCATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "slurm_vars = {\n",
    "    'SLURM_JOB_ID': 'Job ID',\n",
    "    'SLURM_JOB_NODELIST': 'Assigned Node(s)',\n",
    "    'SLURM_NODEID': 'Node ID',\n",
    "    'SLURM_GPUS': 'Total GPUs Allocated',\n",
    "    'SLURM_GPUS_ON_NODE': 'GPUs on This Node',\n",
    "    'SLURM_JOB_GPUS': 'GPU IDs Allocated',\n",
    "    'SLURM_CPUS_ON_NODE': 'CPUs on Node',\n",
    "    'SLURM_MEM_PER_NODE': 'Memory per Node',\n",
    "}\n",
    "\n",
    "slurm_allocated = False\n",
    "for var, desc in slurm_vars.items():\n",
    "    value = os.environ.get(var, 'NOT SET')\n",
    "    print(f\"  {desc:25s}: {value}\")\n",
    "    if var in ['SLURM_GPUS', 'SLURM_GPUS_ON_NODE', 'SLURM_JOB_GPUS']:\n",
    "        if value != 'NOT SET' and value != '0' and value != '':\n",
    "            slurm_allocated = True\n",
    "\n",
    "print()\n",
    "if slurm_allocated:\n",
    "    print(\"  âœ… Slurm has allocated GPU(s) to this job\")\n",
    "else:\n",
    "    print(\"  âš ï¸  WARNING: No GPU allocation detected by Slurm!\")\n",
    "    print(\"     This job may not have requested GPU resources.\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CUDA ENVIRONMENT VARIABLES\n",
    "# ============================================================================\n",
    "print()\n",
    "print(\"2ï¸âƒ£  CUDA ENVIRONMENT VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "cuda_vars = {\n",
    "    'CUDA_VISIBLE_DEVICES': 'Which GPUs are visible to CUDA',\n",
    "    'CUDA_HOME': 'CUDA installation directory',\n",
    "    'CUDA_PATH': 'CUDA path',\n",
    "    'CUDA_ROOT': 'CUDA root directory',\n",
    "    'LD_LIBRARY_PATH': 'Library path (includes CUDA libs)',\n",
    "}\n",
    "\n",
    "cuda_env_ok = False\n",
    "for var, desc in cuda_vars.items():\n",
    "    value = os.environ.get(var, 'NOT SET')\n",
    "    if var == 'LD_LIBRARY_PATH' and value != 'NOT SET':\n",
    "        cuda_parts = [p for p in value.split(':') if 'cuda' in p.lower() or 'CUDA' in p]\n",
    "        if cuda_parts:\n",
    "            print(f\"  {var:25s}: {cuda_parts[0]} (and {len(cuda_parts)-1} more)\")\n",
    "        else:\n",
    "            print(f\"  {var:25s}: (no CUDA paths found)\")\n",
    "    else:\n",
    "        print(f\"  {var:25s}: {value}\")\n",
    "    \n",
    "    if var == 'CUDA_VISIBLE_DEVICES' and value != 'NOT SET':\n",
    "        cuda_env_ok = True\n",
    "\n",
    "print()\n",
    "if cuda_env_ok:\n",
    "    print(\"  âœ… CUDA_VISIBLE_DEVICES is set\")\n",
    "else:\n",
    "    print(\"  âš ï¸  WARNING: CUDA_VISIBLE_DEVICES not set!\")\n",
    "    print(\"     GPUs may not be visible to applications.\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. GPU HARDWARE DETECTION\n",
    "# ============================================================================\n",
    "print()\n",
    "print(\"3ï¸âƒ£  GPU HARDWARE DETECTION (nvidia-smi)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free',\n",
    "         '--format=csv'],\n",
    "        capture_output=True, text=True, timeout=5\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout)\n",
    "        print(\"  âœ… GPU hardware detected successfully\")\n",
    "        hardware_ok = True\n",
    "    else:\n",
    "        print(f\"  âŒ nvidia-smi failed with error:\\n{result.stderr}\")\n",
    "        hardware_ok = False\n",
    "except FileNotFoundError:\n",
    "    print(\"  âŒ nvidia-smi command not found!\")\n",
    "    hardware_ok = False\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Error running nvidia-smi: {e}\")\n",
    "    hardware_ok = False\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. PYTORCH CUDA DETECTION\n",
    "# ============================================================================\n",
    "print(\"4ï¸âƒ£  PYTORCH CUDA DETECTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"  PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"  PyTorch Built with CUDA: {torch.version.cuda}\")\n",
    "    print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"    GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"      Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print()\n",
    "        print(\"  âœ… PyTorch can access GPU(s)!\")\n",
    "        pytorch_ok = True\n",
    "    else:\n",
    "        print()\n",
    "        print(\"  âŒ PyTorch CANNOT access GPU!\")\n",
    "        pytorch_ok = False\n",
    "except ImportError:\n",
    "    print(\"  âŒ PyTorch is not installed!\")\n",
    "    pytorch_ok = False\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SUMMARY\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\" ðŸ“‹ SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "all_checks = {\n",
    "    'Slurm GPU Allocation': slurm_allocated,\n",
    "    'CUDA Environment': cuda_env_ok,\n",
    "    'GPU Hardware (nvidia-smi)': hardware_ok,\n",
    "    'PyTorch CUDA Access': pytorch_ok,\n",
    "}\n",
    "\n",
    "for check, status in all_checks.items():\n",
    "    status_icon = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"  {status_icon} {check}\")\n",
    "\n",
    "print()\n",
    "\n",
    "if all(all_checks.values()):\n",
    "    print(\"ðŸŽ‰ ALL CHECKS PASSED! GPU is ready for training.\")\n",
    "else:\n",
    "    print(\"âš ï¸  ISSUES DETECTED. Review the diagnostic output above.\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea26da",
   "metadata": {},
   "source": [
    "# Train Unconditional UNet2D on CelebA with VAE\n",
    "\n",
    "This notebook trains an **unconditional** diffusion model on CelebA using:\n",
    "- **Pretrained VAE** (AutoencoderKL from Stable Diffusion) to encode images to latents\n",
    "- **UNet2DModel** (unconditional, no cross-attention)\n",
    "- **DDPM scheduler** for training and inference\n",
    "\n",
    "## Key Design Choices\n",
    "- CelebA images (178x218) are center-cropped to 178x178 and resized to 256x256\n",
    "- Images normalized to [-1,1] for VAE compatibility\n",
    "- VAE encodes to 32x32x4 latents\n",
    "- Only the UNet is trained; VAE is frozen\n",
    "- **Unconditional generation** - no text prompts or class labels\n",
    "\n",
    "## Memory Optimizations for GPU\n",
    "- **Batch size**: 16 (adjustable based on GPU memory)\n",
    "- **Image size**: 256x256\n",
    "- **Mixed precision**: Enabled (FP16)\n",
    "- **Cache clearing**: Periodic GPU cache clearing to prevent fragmentation\n",
    "- **DataLoader**: num_workers=2, pin_memory=True for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93405989",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ CelebA Unconditional Diffusion Model Training\n",
    "\n",
    "## What's Different from train9 (CIFAR10)?\n",
    "\n",
    "1. **Dataset**: CelebA (celebrity faces) instead of CIFAR10 (objects)\n",
    "2. **Model Type**: **Unconditional UNet2D** (no text conditioning)\n",
    "3. **Image Size**: 256x256 face images\n",
    "4. **No Text Encoder**: CLIP and text embeddings removed\n",
    "5. **No Classifier-Free Guidance**: Unconditional generation only\n",
    "6. **Simpler Architecture**: Pure UNet2D without cross-attention\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Run the diagnostic cell to verify GPU access\n",
    "2. Configure training parameters (adjust batch size for your GPU)\n",
    "3. Start training - checkpoints and sample images saved every 2000 steps\n",
    "4. Monitor progress through generated sample grids showing random faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d22c47d",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20229bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.models import AutoencoderKL, UNet2DModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3094d3",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31774c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def build_transforms(image_size: int = 256) -> transforms.Compose:\n",
    "    \"\"\"Transform CelebA images: center crop to square, resize, normalize to [-1,1] for VAE.\"\"\"\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.CenterCrop(178),  # CelebA images are 178x218, crop to 178x178\n",
    "            transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e549db",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6033348",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    dataset_root: str\n",
    "    output_dir: str = \"./outputs/train10_celeba\"\n",
    "    batch_size: int = 16\n",
    "    num_epochs: int = 20\n",
    "    lr: float = 1e-4\n",
    "    num_train_timesteps: int = 1000\n",
    "    image_size: int = 256\n",
    "    seed: int = 42\n",
    "    mixed_precision: bool = True\n",
    "    checkpoint_interval: int = 2000  # Save checkpoint every N steps\n",
    "    # UNet size for CelebA faces\n",
    "    unet_block_out_channels: tuple[int, ...] = (128, 256, 512, 512)\n",
    "    layers_per_block: int = 2\n",
    "\n",
    "\n",
    "# Configure training parameters - OPTIMIZED FOR CELEBA\n",
    "config = TrainConfig(\n",
    "    dataset_root=\"../../datasets\",\n",
    "    output_dir=\"./outputs/train10_celeba\",\n",
    "    batch_size=16,  # Adjust based on GPU memory\n",
    "    num_epochs=20,  # CelebA needs multiple epochs for good face generation\n",
    "    lr=1e-4,\n",
    "    image_size=256,  # Full resolution for high-quality faces\n",
    "    mixed_precision=True,  # Enable mixed precision to save memory\n",
    "    checkpoint_interval=2000,  # Save checkpoint every 2000 steps\n",
    ")\n",
    "\n",
    "print(f\"Device: {get_device()}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Epochs: {config.num_epochs}\")\n",
    "print(f\"Learning rate: {config.lr}\")\n",
    "print(f\"Image size: {config.image_size}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"Checkpoint interval: {config.checkpoint_interval} steps\")\n",
    "print(f\"\\nDataset: CelebA (Unconditional Face Generation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94c0aa3",
   "metadata": {},
   "source": [
    "## 4. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47cae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(device: torch.device, config: TrainConfig):\n",
    "    # Pretrained VAE\n",
    "    print(\"Loading pretrained VAE...\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
    "    vae.requires_grad_(False)\n",
    "    vae.eval()\n",
    "    vae.to(device)\n",
    "\n",
    "    print(\"Creating UNet2DModel (unconditional)...\")\n",
    "    # Unconditional UNet operating in latent space (4 channels)\n",
    "    unet = UNet2DModel(\n",
    "        sample_size=config.image_size // 8,  # 32 for 256x256\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        layers_per_block=config.layers_per_block,\n",
    "        block_out_channels=config.unet_block_out_channels,\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\",\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "        ),\n",
    "    ).to(device)\n",
    "\n",
    "    num_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "    print(f\"UNet trainable parameters: {num_params:,}\")\n",
    "\n",
    "    return vae, unet\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "device = get_device()\n",
    "seed_everything(config.seed)\n",
    "vae, unet = create_models(device, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34e57e1",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(config: TrainConfig) -> DataLoader:\n",
    "    tfms = build_transforms(config.image_size)\n",
    "    ds = datasets.CelebA(root=config.dataset_root, split='train', download=True, transform=tfms)\n",
    "    # Using num_workers=2 and pin_memory=True for better performance\n",
    "    return DataLoader(ds, batch_size=config.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "\n",
    "# Create dataloader\n",
    "print(\"Loading CelebA dataset...\")\n",
    "dataloader = make_dataloader(config)\n",
    "print(f\"Dataset size: {len(dataloader.dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ffe14",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711fdf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch\n",
    "sample_batch = next(iter(dataloader))\n",
    "# CelebA returns (image, attr) tuples, we only need images\n",
    "sample_images = sample_batch[0] if isinstance(sample_batch, (list, tuple)) else sample_batch\n",
    "print(f\"Batch shape: {sample_images.shape}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(sample_images):\n",
    "        img = sample_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img + 1) / 2  # Denormalize from [-1,1] to [0,1]\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f9262",
   "metadata": {},
   "source": [
    "## 7. Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80859e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(output_dir: str):\n",
    "    \"\"\"Find the latest checkpoint in the output directory.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        return None\n",
    "    \n",
    "    # Look for checkpoint files\n",
    "    checkpoint_files = []\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.endswith('.pt') and (filename.startswith('unet_step_') or filename.startswith('unet_epoch_')):\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            checkpoint_files.append(filepath)\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    # Get the most recently modified checkpoint\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)\n",
    "    return latest_checkpoint\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path: str, unet, optimizer=None):\n",
    "    \"\"\"Load checkpoint and return metadata.\"\"\"\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # Handle different checkpoint formats\n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'unet' in checkpoint:\n",
    "            unet.load_state_dict(checkpoint['unet'])\n",
    "        else:\n",
    "            unet.load_state_dict(checkpoint)\n",
    "        \n",
    "        # Load optimizer state if available\n",
    "        if optimizer is not None and 'optimizer' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = {\n",
    "            'global_step': checkpoint.get('global_step', checkpoint.get('step', 0)),\n",
    "            'epoch': checkpoint.get('epoch', 0),\n",
    "            'batch_losses': checkpoint.get('batch_losses', []),\n",
    "            'epoch_losses': checkpoint.get('epoch_losses', []),\n",
    "        }\n",
    "    else:\n",
    "        unet.load_state_dict(checkpoint)\n",
    "        metadata = {'global_step': 0, 'epoch': 0, 'batch_losses': [], 'epoch_losses': []}\n",
    "    \n",
    "    print(f\"Resumed from step {metadata['global_step']}, epoch {metadata['epoch']}\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str, unet, optimizer, global_step: int, epoch: int, \n",
    "                   batch_losses: List[float], epoch_losses: List[float]):\n",
    "    \"\"\"Save checkpoint with complete metadata.\"\"\"\n",
    "    checkpoint = {\n",
    "        'unet': unet.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'global_step': global_step,\n",
    "        'epoch': epoch,\n",
    "        'batch_losses': batch_losses,\n",
    "        'epoch_losses': epoch_losses,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "# Check for existing checkpoints\n",
    "latest_checkpoint = find_latest_checkpoint(config.output_dir)\n",
    "if latest_checkpoint:\n",
    "    print(f\"Found existing checkpoint: {latest_checkpoint}\")\n",
    "    print(\"Training will resume from this checkpoint.\")\n",
    "else:\n",
    "    print(\"No existing checkpoints found. Starting training from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846cc19d",
   "metadata": {},
   "source": [
    "### Checkpoint System Features\n",
    "\n",
    "The checkpoint system provides:\n",
    "\n",
    "1. **Automatic Resume**: Detects and loads the latest checkpoint automatically\n",
    "2. **Complete Metadata**: Each checkpoint stores:\n",
    "   - UNet model weights\n",
    "   - Optimizer state (for proper resume)\n",
    "   - Global step count\n",
    "   - Current epoch number\n",
    "   - Complete batch loss history\n",
    "   - Complete epoch loss history\n",
    "3. **Training Plots**: Loss plots saved with every checkpoint\n",
    "4. **Sample Images**: Generated face samples saved at each checkpoint to visualize training progress\n",
    "5. **Multiple Checkpoint Types**:\n",
    "   - Step checkpoints: `unet_step_2000.pt` (every 2000 steps)\n",
    "   - Epoch checkpoints: `unet_epoch_1.pt` (after each epoch)\n",
    "   - Final checkpoint: `unet_final.pt` (at completion)\n",
    "   - Sample images: `samples_step_2000.png`, `samples_epoch_1.png`, `samples_final.png`\n",
    "\n",
    "If training is interrupted, simply re-run the training cell and it will automatically resume from the latest checkpoint!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e887e",
   "metadata": {},
   "source": [
    "## 8. Sample Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67917fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_checkpoint_samples(\n",
    "    unet, vae, device, config, \n",
    "    save_path: str, global_step: int, num_samples: int = 16, num_inference_steps: int = 50\n",
    "):\n",
    "    \"\"\"Generate sample images during training checkpoints (unconditional).\"\"\"\n",
    "    print(f\"\\nGenerating {num_samples} samples at step {global_step}...\")\n",
    "    \n",
    "    # Create scheduler for sampling\n",
    "    scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    # Set UNet to eval mode for sampling\n",
    "    unet.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i >= num_samples:\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "                \n",
    "            # Set seed for reproducibility\n",
    "            torch.manual_seed(42 + i)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(42 + i)\n",
    "            \n",
    "            # Init random latents\n",
    "            latents = torch.randn((1, 4, config.image_size // 8, config.image_size // 8), device=device)\n",
    "            \n",
    "            # Denoising loop (unconditional)\n",
    "            for t in scheduler.timesteps:\n",
    "                latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "                \n",
    "                # Predict noise (unconditional - no text embeddings)\n",
    "                noise_pred = unet(latent_model_input, t).sample\n",
    "                \n",
    "                # Step\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "            \n",
    "            # Decode latents to image\n",
    "            latents = latents / 0.18215\n",
    "            image = vae.decode(latents).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.detach().cpu()\n",
    "            \n",
    "            # Display image\n",
    "            img = image[0].permute(1, 2, 0).numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Generated Samples at Step {global_step}\", fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Sample images saved: {save_path}\")\n",
    "    \n",
    "    # Set UNet back to train mode\n",
    "    unet.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934d6b3",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c2bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: TrainConfig, vae, unet, dataloader, device, resume_from_checkpoint: str = None):\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "    # Noise scheduler for training\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=config.num_train_timesteps, beta_schedule=\"squaredcos_cap_v2\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(unet.parameters(), lr=config.lr)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.mixed_precision and device.type == \"cuda\")\n",
    "\n",
    "    # Loss tracking and training state\n",
    "    batch_losses = []\n",
    "    epoch_losses = []\n",
    "    global_step = 0\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Resume from checkpoint if available\n",
    "    if resume_from_checkpoint:\n",
    "        metadata = load_checkpoint(resume_from_checkpoint, unet, optimizer)\n",
    "        global_step = metadata['global_step']\n",
    "        start_epoch = metadata['epoch']\n",
    "        batch_losses = metadata['batch_losses']\n",
    "        epoch_losses = metadata['epoch_losses']\n",
    "        print(f\"Resuming training from step {global_step}, epoch {start_epoch}\")\n",
    "\n",
    "    unet.train()\n",
    "    \n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        epoch_loss_sum = 0.0\n",
    "        epoch_batch_count = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # CelebA returns (image, attr) tuples, we only need images\n",
    "            images = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "            images = images.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Encode images to latents using frozen VAE\n",
    "                latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "\n",
    "            # Sample noise and timestep; add noise\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Predict noise (unconditional - no text embeddings needed)\n",
    "            with torch.autocast(\n",
    "                device_type=device.type,\n",
    "                dtype=torch.float16 if (config.mixed_precision and device.type == \"cuda\") else torch.float32,\n",
    "                enabled=config.mixed_precision\n",
    "            ):\n",
    "                noise_pred = unet(noisy_latents, timesteps).sample\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Record loss\n",
    "            loss_value = loss.item()\n",
    "            batch_losses.append(loss_value)\n",
    "            epoch_loss_sum += loss_value\n",
    "            epoch_batch_count += 1\n",
    "\n",
    "            global_step += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{loss_value:.4f}\", \"step\": global_step})\n",
    "\n",
    "            # Clear cache every 50 batches to prevent fragmentation\n",
    "            if batch_idx % 50 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Save periodic checkpoints\n",
    "            if global_step % config.checkpoint_interval == 0:\n",
    "                ckpt_path = os.path.join(config.output_dir, f\"unet_step_{global_step}.pt\")\n",
    "                save_checkpoint(ckpt_path, unet, optimizer, global_step, epoch, batch_losses, epoch_losses)\n",
    "                print(f\"\\nCheckpoint saved: {ckpt_path}\")\n",
    "                \n",
    "                # Save training plots with the checkpoint\n",
    "                plot_path = os.path.join(config.output_dir, f\"training_loss_step_{global_step}.png\")\n",
    "                save_loss_plot(batch_losses, epoch_losses, plot_path)\n",
    "                print(f\"Training plot saved: {plot_path}\")\n",
    "                \n",
    "                # Generate and save sample images\n",
    "                samples_path = os.path.join(config.output_dir, f\"samples_step_{global_step}.png\")\n",
    "                generate_checkpoint_samples(\n",
    "                    unet, vae, device, config,\n",
    "                    save_path=samples_path, global_step=global_step\n",
    "                )\n",
    "                \n",
    "                # Clear cache after sampling\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        # Record epoch average loss\n",
    "        avg_epoch_loss = epoch_loss_sum / epoch_batch_count if epoch_batch_count > 0 else 0.0\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{config.num_epochs} - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # Save per-epoch checkpoint\n",
    "        ckpt_path = os.path.join(config.output_dir, f\"unet_epoch_{epoch+1}.pt\")\n",
    "        save_checkpoint(ckpt_path, unet, optimizer, global_step, epoch + 1, batch_losses, epoch_losses)\n",
    "        \n",
    "        # Save training plots with epoch checkpoint\n",
    "        plot_path = os.path.join(config.output_dir, f\"training_loss_epoch_{epoch+1}.png\")\n",
    "        save_loss_plot(batch_losses, epoch_losses, plot_path)\n",
    "        print(f\"Training plot saved: {plot_path}\")\n",
    "        \n",
    "        # Generate and save sample images at end of epoch\n",
    "        samples_path = os.path.join(config.output_dir, f\"samples_epoch_{epoch+1}.png\")\n",
    "        generate_checkpoint_samples(\n",
    "            unet, vae, device, config,\n",
    "            save_path=samples_path, global_step=global_step\n",
    "        )\n",
    "        \n",
    "        # Clear cache after each epoch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Save final\n",
    "    final_path = os.path.join(config.output_dir, \"unet_final.pt\")\n",
    "    save_checkpoint(final_path, unet, optimizer, global_step, config.num_epochs, batch_losses, epoch_losses)\n",
    "    print(f\"\\nFinal model saved: {final_path}\")\n",
    "    \n",
    "    # Save final training plots\n",
    "    final_plot_path = os.path.join(config.output_dir, \"training_loss_final.png\")\n",
    "    save_loss_plot(batch_losses, epoch_losses, final_plot_path)\n",
    "    print(f\"Final training plot saved: {final_plot_path}\")\n",
    "    \n",
    "    # Generate final sample images\n",
    "    final_samples_path = os.path.join(config.output_dir, \"samples_final.png\")\n",
    "    generate_checkpoint_samples(\n",
    "        unet, vae, device, config,\n",
    "        save_path=final_samples_path, global_step=global_step\n",
    "    )\n",
    "\n",
    "    return batch_losses, epoch_losses\n",
    "\n",
    "\n",
    "def save_loss_plot(batch_losses: List[float], epoch_losses: List[float], save_path: str):\n",
    "    \"\"\"Helper function to save loss plots without displaying them.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Per-batch loss\n",
    "    if len(batch_losses) > 0:\n",
    "        axes[0].plot(batch_losses, linewidth=0.8, alpha=0.7)\n",
    "        axes[0].set_xlabel(\"Batch\")\n",
    "        axes[0].set_ylabel(\"Loss\")\n",
    "        axes[0].set_title(\"Training Loss per Batch\")\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Per-epoch loss\n",
    "    if len(epoch_losses) > 0:\n",
    "        axes[1].plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', linewidth=2)\n",
    "        axes[1].set_xlabel(\"Epoch\")\n",
    "        axes[1].set_ylabel(\"Average Loss\")\n",
    "        axes[1].set_title(\"Training Loss per Epoch\")\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.close(fig)  # Close the figure to free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c578bc2",
   "metadata": {},
   "source": [
    "## 10. Memory Optimization (Clear GPU Cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa161b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache before training to maximize available memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    # Set memory allocation configuration for better fragmentation handling\n",
    "    import os\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    # Print current memory status\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(\"Memory cache cleared and optimized for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bb4be",
   "metadata": {},
   "source": [
    "## 11. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e51ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoints and resume if available\n",
    "latest_checkpoint = find_latest_checkpoint(config.output_dir)\n",
    "\n",
    "# Train the model (will resume from checkpoint if found)\n",
    "batch_losses, epoch_losses = train(\n",
    "    config, vae, unet, dataloader, device, \n",
    "    resume_from_checkpoint=latest_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb15659",
   "metadata": {},
   "source": [
    "## 12. Visualize Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed08b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(batch_losses: List[float], epoch_losses: List[float], output_dir: str = None):\n",
    "    \"\"\"Generate and save loss plots for per-batch and per-epoch losses.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Per-batch loss\n",
    "    axes[0].plot(batch_losses, linewidth=0.8, alpha=0.7)\n",
    "    axes[0].set_xlabel(\"Batch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Training Loss per Batch\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Per-epoch loss\n",
    "    if len(epoch_losses) > 0:\n",
    "        axes[1].plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', linewidth=2)\n",
    "        axes[1].set_xlabel(\"Epoch\")\n",
    "        axes[1].set_ylabel(\"Average Loss\")\n",
    "        axes[1].set_title(\"Training Loss per Epoch\")\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        loss_plot_path = os.path.join(output_dir, \"training_loss.png\")\n",
    "        plt.savefig(loss_plot_path, dpi=150)\n",
    "        print(f\"Loss plots saved to {loss_plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the losses\n",
    "plot_losses(batch_losses, epoch_losses, config.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849446f1",
   "metadata": {},
   "source": [
    "## 13. Unconditional Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3668a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(\n",
    "    num_inference_steps: int = 50,\n",
    "    seed: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"Generate an image unconditionally.\"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    # Init random latents in latent space\n",
    "    latents = torch.randn((1, 4, config.image_size // 8, config.image_size // 8), device=device)\n",
    "\n",
    "    unet.eval()\n",
    "    for t in tqdm(scheduler.timesteps, desc=\"Sampling\"):\n",
    "        latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "\n",
    "        # Predict noise (unconditional)\n",
    "        noise_pred = unet(latent_model_input, t).sample\n",
    "\n",
    "        # Step\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    # Decode latents to image\n",
    "    latents = latents / 0.18215\n",
    "    image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu()\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef0673",
   "metadata": {},
   "source": [
    "## 14. Generate Sample Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb955f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple random face samples\n",
    "num_samples = 16\n",
    "num_inference_steps = 50\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    print(f\"Generating sample {i+1}/{num_samples}\")\n",
    "    generated_image = sample(\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        seed=42 + i,\n",
    "    )\n",
    "    \n",
    "    # Display image\n",
    "    img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.output_dir, \"generated_samples.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6f1a4",
   "metadata": {},
   "source": [
    "## 15. Save Individual Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save a single high-quality sample\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "print(\"Generating single face sample...\")\n",
    "\n",
    "generated_image = sample(\n",
    "    num_inference_steps=50,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "output_path = os.path.join(config.output_dir, \"sample_face.png\")\n",
    "save_image(generated_image, output_path)\n",
    "print(f\"Sample saved to: {output_path}\")\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(6, 6))\n",
    "img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "plt.imshow(img)\n",
    "plt.title(\"Generated CelebA Face\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
