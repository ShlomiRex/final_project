{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432716d8",
   "metadata": {},
   "source": [
    "# ðŸ›ï¸ LSUN Churches Unconditional Diffusion Model Training\n",
    "\n",
    "## About LSUN Churches\n",
    "\n",
    "- **Dataset**: Large-Scale Scene Understanding (LSUN) - Churches category\n",
    "- **Size**: ~126,000 high-quality images of churches\n",
    "- **Resolution**: Variable, resized to 256x256 for training\n",
    "- **Download**: Automatically downloaded via torchvision (~2.3GB compressed)\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "- **VAE**: Pretrained AutoencoderKL from Stable Diffusion v1.5 (frozen)\n",
    "- **UNet**: UNet2DModel for unconditional generation\n",
    "- **Scheduler**: DDPM with squaredcos_cap_v2 beta schedule\n",
    "- **Training**: Mixed precision FP16, batch size 16, 1 epoch for checkpoint testing\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Run the diagnostic cell to verify GPU access\n",
    "2. Configure training parameters\n",
    "3. Start training - checkpoints and samples saved every 2000 steps\n",
    "4. Monitor progress through generated sample grids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9b826",
   "metadata": {},
   "source": [
    "## ðŸ”§ System Diagnostic - Run This First!\n",
    "\n",
    "Before training, verify that the GPU is properly accessible. This cell checks:\n",
    "1. Slurm job allocation\n",
    "2. GPU hardware detection\n",
    "3. CUDA environment variables\n",
    "4. PyTorch CUDA compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f84b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Comprehensive GPU Diagnostic for HPC Cluster\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" ðŸ” COMPREHENSIVE GPU DIAGNOSTIC FOR HPC CLUSTER\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# 1. SLURM JOB INFORMATION\n",
    "print(\"1ï¸âƒ£  SLURM JOB ALLOCATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "slurm_vars = {\n",
    "    'SLURM_JOB_ID': 'Job ID',\n",
    "    'SLURM_JOB_NODELIST': 'Assigned Node(s)',\n",
    "    'SLURM_GPUS_ON_NODE': 'GPUs on This Node',\n",
    "}\n",
    "\n",
    "for var, desc in slurm_vars.items():\n",
    "    value = os.environ.get(var, 'NOT SET')\n",
    "    print(f\"  {desc:25s}: {value}\")\n",
    "\n",
    "# 2. GPU HARDWARE\n",
    "print(\"\\n2ï¸âƒ£  GPU HARDWARE\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], \n",
    "                          capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"  {result.stdout.strip()}\")\n",
    "except:\n",
    "    print(\"  GPU info unavailable\")\n",
    "\n",
    "# 3. PYTORCH CUDA\n",
    "print(\"\\n3ï¸âƒ£  PYTORCH CUDA\")\n",
    "print(\"-\" * 80)\n",
    "import torch\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  âœ… GPU Ready!\")\n",
    "else:\n",
    "    print(f\"  âŒ No GPU detected!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db73e9f",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0eee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.models import AutoencoderKL, UNet2DModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286bbee",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def build_transforms(image_size: int = 256) -> transforms.Compose:\n",
    "    \"\"\"Transform LSUN Churches images: resize to square, normalize to [-1,1] for VAE.\"\"\"\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb671d8d",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    dataset_root: str\n",
    "    output_dir: str = \"./outputs/train10_lsun_churches\"\n",
    "    batch_size: int = 16\n",
    "    num_epochs: int = 1\n",
    "    lr: float = 1e-4\n",
    "    num_train_timesteps: int = 1000\n",
    "    image_size: int = 256\n",
    "    seed: int = 42\n",
    "    mixed_precision: bool = True\n",
    "    checkpoint_interval: int = 2000\n",
    "    unet_block_out_channels: tuple[int, ...] = (128, 256, 512, 512)\n",
    "    layers_per_block: int = 2\n",
    "\n",
    "\n",
    "# Configure training parameters\n",
    "config = TrainConfig(\n",
    "    dataset_root=\"../../datasets\",\n",
    "    output_dir=\"./outputs/train10_lsun_churches\",\n",
    "    batch_size=16,\n",
    "    num_epochs=1,\n",
    "    lr=1e-4,\n",
    "    image_size=256,\n",
    "    mixed_precision=True,\n",
    "    checkpoint_interval=2000,\n",
    ")\n",
    "\n",
    "print(f\"Device: {get_device()}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Epochs: {config.num_epochs}\")\n",
    "print(f\"Learning rate: {config.lr}\")\n",
    "print(f\"Image size: {config.image_size}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"Checkpoint interval: {config.checkpoint_interval} steps\")\n",
    "print(f\"\\nDataset: LSUN Churches (Unconditional Church Generation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efbfdf8",
   "metadata": {},
   "source": [
    "## 4. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f318c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(device: torch.device, config: TrainConfig):\n",
    "    print(\"Loading pretrained VAE...\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
    "    vae.requires_grad_(False)\n",
    "    vae.eval()\n",
    "    vae.to(device)\n",
    "\n",
    "    print(\"Creating UNet2DModel (unconditional)...\")\n",
    "    unet = UNet2DModel(\n",
    "        sample_size=config.image_size // 8,\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        layers_per_block=config.layers_per_block,\n",
    "        block_out_channels=config.unet_block_out_channels,\n",
    "        down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
    "        up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\"),\n",
    "    ).to(device)\n",
    "\n",
    "    num_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "    print(f\"UNet trainable parameters: {num_params:,}\")\n",
    "\n",
    "    return vae, unet\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "seed_everything(config.seed)\n",
    "vae, unet = create_models(device, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12918e7a",
   "metadata": {},
   "source": [
    "## 5. Load LSUN Churches Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(config: TrainConfig) -> DataLoader:\n",
    "    tfms = build_transforms(config.image_size)\n",
    "    \n",
    "    print(\"Downloading LSUN Churches dataset...\")\n",
    "    print(\"Note: First download may take a while (~2.3GB compressed)\")\n",
    "    \n",
    "    ds = datasets.LSUN(\n",
    "        root=config.dataset_root, \n",
    "        classes=['church_outdoor_train'],\n",
    "        transform=tfms\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds, \n",
    "        batch_size=config.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2, \n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Loading LSUN Churches dataset...\")\n",
    "dataloader = make_dataloader(config)\n",
    "print(f\"Dataset size: {len(dataloader.dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1494a672",
   "metadata": {},
   "source": [
    "## 6. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd2b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\" LSUN CHURCHES DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total number of images: {len(dataloader.dataset):,}\")\n",
    "print(f\"Number of batches: {len(dataloader):,}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Total training steps per epoch: {len(dataloader):,}\")\n",
    "print(f\"Total training steps ({config.num_epochs} epochs): {len(dataloader) * config.num_epochs:,}\")\n",
    "print(f\"Checkpoint interval: every {config.checkpoint_interval:,} steps\")\n",
    "print(f\"Expected checkpoints per epoch: ~{len(dataloader) // config.checkpoint_interval}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac73e3",
   "metadata": {},
   "source": [
    "## 7. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e81617",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(dataloader))\n",
    "sample_images = sample_batch[0] if isinstance(sample_batch, (list, tuple)) else sample_batch\n",
    "print(f\"Batch shape: {sample_images.shape}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(sample_images):\n",
    "        img = sample_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img + 1) / 2\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "plt.suptitle(\"Sample LSUN Churches Images\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd465a7",
   "metadata": {},
   "source": [
    "## 8. Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c78585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(output_dir: str):\n",
    "    if not os.path.exists(output_dir):\n",
    "        return None\n",
    "    \n",
    "    checkpoint_files = []\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.endswith('.pt') and (filename.startswith('unet_step_') or filename.startswith('unet_epoch_')):\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            checkpoint_files.append(filepath)\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    return max(checkpoint_files, key=os.path.getmtime)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path: str, unet, optimizer=None):\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'unet' in checkpoint:\n",
    "            unet.load_state_dict(checkpoint['unet'])\n",
    "        else:\n",
    "            unet.load_state_dict(checkpoint)\n",
    "        \n",
    "        if optimizer is not None and 'optimizer' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "        metadata = {\n",
    "            'global_step': checkpoint.get('global_step', checkpoint.get('step', 0)),\n",
    "            'epoch': checkpoint.get('epoch', 0),\n",
    "            'batch_losses': checkpoint.get('batch_losses', []),\n",
    "            'epoch_losses': checkpoint.get('epoch_losses', []),\n",
    "        }\n",
    "    else:\n",
    "        unet.load_state_dict(checkpoint)\n",
    "        metadata = {'global_step': 0, 'epoch': 0, 'batch_losses': [], 'epoch_losses': []}\n",
    "    \n",
    "    print(f\"Resumed from step {metadata['global_step']}, epoch {metadata['epoch']}\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str, unet, optimizer, global_step: int, epoch: int, \n",
    "                   batch_losses: List[float], epoch_losses: List[float]):\n",
    "    checkpoint = {\n",
    "        'unet': unet.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'global_step': global_step,\n",
    "        'epoch': epoch,\n",
    "        'batch_losses': batch_losses,\n",
    "        'epoch_losses': epoch_losses,\n",
    "    }\n",
    "    torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d831e9",
   "metadata": {},
   "source": [
    "## 9. Sample Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6be514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_checkpoint_samples(\n",
    "    unet, vae, device, config, \n",
    "    save_path: str, global_step: int, num_samples: int = 16, num_inference_steps: int = 50\n",
    "):\n",
    "    print(f\"\\nGenerating {num_samples} samples at step {global_step}...\")\n",
    "    \n",
    "    scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    unet.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i >= num_samples:\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "                \n",
    "            torch.manual_seed(42 + i)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(42 + i)\n",
    "            \n",
    "            latents = torch.randn((1, 4, config.image_size // 8, config.image_size // 8), device=device)\n",
    "            \n",
    "            for t in scheduler.timesteps:\n",
    "                latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "                noise_pred = unet(latent_model_input, t).sample\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "            \n",
    "            latents = latents / 0.18215\n",
    "            image = vae.decode(latents).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.detach().cpu()\n",
    "            \n",
    "            img = image[0].permute(1, 2, 0).numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Generated Samples at Step {global_step}\", fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Sample images saved: {save_path}\")\n",
    "    \n",
    "    unet.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c922bc6",
   "metadata": {},
   "source": [
    "## 10. Checkpoint Validation\n",
    "\n",
    "If a checkpoint exists, validate it by generating test samples before resuming training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12bfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_checkpoint = find_latest_checkpoint(config.output_dir)\n",
    "if latest_checkpoint:\n",
    "    print(f\"âœ… Found existing checkpoint: {latest_checkpoint}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ðŸ” VALIDATING CHECKPOINT BEFORE RESUMING TRAINING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nLoading checkpoint for validation...\")\n",
    "    temp_metadata = load_checkpoint(latest_checkpoint, unet, None)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Checkpoint Information:\")\n",
    "    print(f\"   Global Step: {temp_metadata['global_step']:,}\")\n",
    "    print(f\"   Epoch: {temp_metadata['epoch']}\")\n",
    "    print(f\"   Batch Losses Recorded: {len(temp_metadata['batch_losses']):,}\")\n",
    "    print(f\"   Epoch Losses Recorded: {len(temp_metadata['epoch_losses'])}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¨ Generating test samples from checkpoint...\")\n",
    "    validation_samples_path = os.path.join(config.output_dir, f\"checkpoint_validation_step_{temp_metadata['global_step']}.png\")\n",
    "    \n",
    "    generate_checkpoint_samples(\n",
    "        unet, vae, device, config,\n",
    "        save_path=validation_samples_path,\n",
    "        global_step=temp_metadata['global_step'],\n",
    "        num_samples=16,\n",
    "        num_inference_steps=50\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Checkpoint validation complete!\")\n",
    "    print(f\"   Validation samples saved: {validation_samples_path}\")\n",
    "    print(f\"\\nâ–¶ï¸  Training will resume from step {temp_metadata['global_step']}, epoch {temp_metadata['epoch']}\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"No existing checkpoints found. Starting training from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81958f",
   "metadata": {},
   "source": [
    "## 11. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7acdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: TrainConfig, vae, unet, dataloader, device, resume_from_checkpoint: str = None):\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=config.num_train_timesteps, beta_schedule=\"squaredcos_cap_v2\")\n",
    "    optimizer = torch.optim.AdamW(unet.parameters(), lr=config.lr)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.mixed_precision and device.type == \"cuda\")\n",
    "\n",
    "    batch_losses = []\n",
    "    epoch_losses = []\n",
    "    global_step = 0\n",
    "    start_epoch = 0\n",
    "    \n",
    "    if resume_from_checkpoint:\n",
    "        metadata = load_checkpoint(resume_from_checkpoint, unet, optimizer)\n",
    "        global_step = metadata['global_step']\n",
    "        start_epoch = metadata['epoch']\n",
    "        batch_losses = metadata['batch_losses']\n",
    "        epoch_losses = metadata['epoch_losses']\n",
    "        print(f\"Resuming training from step {global_step}, epoch {start_epoch}\")\n",
    "\n",
    "    unet.train()\n",
    "    \n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        epoch_loss_sum = 0.0\n",
    "        epoch_batch_count = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            images = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "            images = images.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            with torch.autocast(\n",
    "                device_type=device.type,\n",
    "                dtype=torch.float16 if (config.mixed_precision and device.type == \"cuda\") else torch.float32,\n",
    "                enabled=config.mixed_precision\n",
    "            ):\n",
    "                noise_pred = unet(noisy_latents, timesteps).sample\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            loss_value = loss.item()\n",
    "            batch_losses.append(loss_value)\n",
    "            epoch_loss_sum += loss_value\n",
    "            epoch_batch_count += 1\n",
    "\n",
    "            global_step += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{loss_value:.4f}\", \"step\": global_step})\n",
    "\n",
    "            if batch_idx % 50 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            if global_step % config.checkpoint_interval == 0:\n",
    "                ckpt_path = os.path.join(config.output_dir, f\"unet_step_{global_step}.pt\")\n",
    "                save_checkpoint(ckpt_path, unet, optimizer, global_step, epoch, batch_losses, epoch_losses)\n",
    "                print(f\"\\nCheckpoint saved: {ckpt_path}\")\n",
    "                \n",
    "                samples_path = os.path.join(config.output_dir, f\"samples_step_{global_step}.png\")\n",
    "                generate_checkpoint_samples(unet, vae, device, config, save_path=samples_path, global_step=global_step)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss_sum / epoch_batch_count if epoch_batch_count > 0 else 0.0\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{config.num_epochs} - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        ckpt_path = os.path.join(config.output_dir, f\"unet_epoch_{epoch+1}.pt\")\n",
    "        save_checkpoint(ckpt_path, unet, optimizer, global_step, epoch + 1, batch_losses, epoch_losses)\n",
    "        \n",
    "        samples_path = os.path.join(config.output_dir, f\"samples_epoch_{epoch+1}.png\")\n",
    "        generate_checkpoint_samples(unet, vae, device, config, save_path=samples_path, global_step=global_step)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    final_path = os.path.join(config.output_dir, \"unet_final.pt\")\n",
    "    save_checkpoint(final_path, unet, optimizer, global_step, config.num_epochs, batch_losses, epoch_losses)\n",
    "    print(f\"\\nFinal model saved: {final_path}\")\n",
    "    \n",
    "    final_samples_path = os.path.join(config.output_dir, \"samples_final.png\")\n",
    "    generate_checkpoint_samples(unet, vae, device, config, save_path=final_samples_path, global_step=global_step)\n",
    "\n",
    "    return batch_losses, epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbbaf55",
   "metadata": {},
   "source": [
    "## 12. Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(\"Memory cache cleared and optimized for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5512bbd",
   "metadata": {},
   "source": [
    "## 13. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea4151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_checkpoint = find_latest_checkpoint(config.output_dir)\n",
    "\n",
    "batch_losses, epoch_losses = train(\n",
    "    config, vae, unet, dataloader, device, \n",
    "    resume_from_checkpoint=latest_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208c5e4c",
   "metadata": {},
   "source": [
    "## 14. Unconditional Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da254837",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(num_inference_steps: int = 50, seed: Optional[int] = None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    latents = torch.randn((1, 4, config.image_size // 8, config.image_size // 8), device=device)\n",
    "\n",
    "    unet.eval()\n",
    "    for t in tqdm(scheduler.timesteps, desc=\"Sampling\"):\n",
    "        latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "        noise_pred = unet(latent_model_input, t).sample\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    latents = latents / 0.18215\n",
    "    image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu()\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a8849",
   "metadata": {},
   "source": [
    "## 15. Generate Sample Churches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 16\n",
    "num_inference_steps = 50\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    print(f\"Generating sample {i+1}/{num_samples}\")\n",
    "    generated_image = sample(num_inference_steps=num_inference_steps, seed=42 + i)\n",
    "    \n",
    "    img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Generated LSUN Churches\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.output_dir, \"generated_samples.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f9b32",
   "metadata": {},
   "source": [
    "## 16. Save Individual Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478d4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "print(\"Generating single church sample...\")\n",
    "\n",
    "generated_image = sample(num_inference_steps=50, seed=42)\n",
    "\n",
    "output_path = os.path.join(config.output_dir, \"sample_church.png\")\n",
    "save_image(generated_image, output_path)\n",
    "print(f\"Sample saved to: {output_path}\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "plt.imshow(img)\n",
    "plt.title(\"Generated LSUN Church\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
