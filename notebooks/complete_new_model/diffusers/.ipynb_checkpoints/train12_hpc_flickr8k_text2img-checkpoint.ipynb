{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a70fcd9",
   "metadata": {},
   "source": [
    "# Train12: Flickr8k Text-to-Image Diffusion (Conditional UNet)\n",
    "\n",
    "This notebook trains a text-conditional diffusion model on the Flickr8k dataset using:\n",
    "- Pretrained VAE (AutoencoderKL) to work in latent space (32x32x4 for 256x256)\n",
    "- CLIP tokenizer + text encoder for caption embeddings\n",
    "- UNet2DConditionModel with cross-attention\n",
    "- DDPM scheduler and EMA for stable training\n",
    "\n",
    "It follows the style of prior notebooks (train9, train11) and saves checkpoints and samples periodically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c6021",
   "metadata": {},
   "source": [
    "## 1. Set Up Environment and Install Dependencies\n",
    "\n",
    "If needed, install/update required libraries. (Skip if already installed on HPC environment.)\n",
    "\n",
    "```bash\n",
    "# OPTIONAL: Uncomment to install dependencies\n",
    "# pip install --upgrade diffusers transformers datasets accelerate huggingface_hub safetensors\n",
    "# pip install xformers==0.0.27.post2  # if CUDA compatible\n",
    "# pip install clean-fid torchmetrics   # for later FID computation\n",
    "```\n",
    "\n",
    "We rely on: `diffusers`, `transformers`, `datasets`, `accelerate`, and optionally `xformers` for memory-efficient attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f8c4b",
   "metadata": {},
   "source": [
    "## 2. Runtime and Accelerator Configuration\n",
    "We use `accelerate` to support multi-GPU / distributed training transparently and mixed precision for speed & memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3011a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from diffusers import DDPMScheduler, DDIMScheduler\n",
    "from diffusers.models import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers.training_utils import EMAModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "device = accelerator.device\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    print(f\"Accelerator initialized | device={device} | num_processes={accelerator.num_processes} | mixed_precision={accelerator.mixed_precision}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA devices: {torch.cuda.device_count()} | Current: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be2917",
   "metadata": {},
   "source": [
    "## 3. Experiment Config: Paths and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    run_name: str = \"train12_flickr8k_text2img\"\n",
    "    output_dir: str = \"./outputs/train12_flickr8k_text2img\"\n",
    "    cache_dir: str = os.path.abspath(\"../../dataset_cache\")\n",
    "    dataset_name_candidates: List[str] = None\n",
    "    image_size: int = 256\n",
    "    train_batch_size: int = 16\n",
    "    eval_batch_size: int = 16\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    num_epochs: int = 50\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    num_workers: int = 2\n",
    "    max_caption_length: int = 77\n",
    "    save_every: int = 2000\n",
    "    sample_every: int = 2000\n",
    "    mixed_precision: str = \"fp16\"\n",
    "    ema_decay: float = 0.9999\n",
    "    classifier_free_guidance_prob: float = 0.1\n",
    "    seed: int = 42\n",
    "    prompts: List[str] = None\n",
    "\n",
    "\n",
    "config = TrainConfig(\n",
    "    dataset_name_candidates=[\n",
    "        \"nlphuji/flickr8k\",\n",
    "        \"flickr8k\",\n",
    "        \"yashkant/Flickr8k\",\n",
    "        \"conceptofmind/flickr8k\",\n",
    "    ],\n",
    "    prompts=[\n",
    "        \"A child playing with a dog\",\n",
    "        \"A man riding a bicycle\",\n",
    "        \"Two people sitting on a bench\",\n",
    "        \"A group of hikers on a mountain\",\n",
    "        \"A dog catching a frisbee in a park\",\n",
    "        \"A girl holding a red balloon\",\n",
    "        \"A person surfing a big wave\",\n",
    "        \"A cat sleeping on a couch\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "set_seed(config.seed)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    print(f\"Output dir: {os.path.abspath(config.output_dir)}\")\n",
    "    print(f\"Dataset cache dir: {config.cache_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6de0c5",
   "metadata": {},
   "source": [
    "## 4–6. Flickr8k Dataset: Load, Preprocess, Build DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b434ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_image_transforms(image_size: int):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # to [-1,1]\n",
    "    ])\n",
    "\n",
    "\n",
    "def try_load_flickr8k(config: TrainConfig) -> Dict[str, HFDataset]:\n",
    "    \"\"\"Attempt multiple dataset names until success. Returns split dict with train/validation/test.\"\"\"\n",
    "    last_err = None\n",
    "    for name in config.dataset_name_candidates:\n",
    "        try:\n",
    "            ds = load_dataset(name, cache_dir=config.cache_dir)\n",
    "            if accelerator.is_main_process:\n",
    "                print(f\"Loaded dataset '{name}' with splits: {list(ds.keys())}\")\n",
    "            return ds\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if accelerator.is_main_process:\n",
    "                print(f\"Failed loading '{name}': {e}\")\n",
    "    raise RuntimeError(f\"Could not load Flickr8k from candidates {config.dataset_name_candidates}. Last error: {last_err}\n",
    "    You may need to manually place images + captions and implement a local loader.\")\n",
    "\n",
    "\n",
    "# Load dataset (train/validation/test splits vary by dataset variant)\n",
    "raw_ds = try_load_flickr8k(config)\n",
    "\n",
    "# Heuristic: Use 'train' split; fall back to largest available\n",
    "train_split_name = 'train' if 'train' in raw_ds else list(raw_ds.keys())[0]\n",
    "val_split_name = 'validation' if 'validation' in raw_ds else ('val' if 'val' in raw_ds else train_split_name)\n",
    "\n",
    "train_ds = raw_ds[train_split_name]\n",
    "val_ds = raw_ds[val_split_name]\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    print(f\"Train split size: {len(train_ds)} | Val split size: {len(val_ds)}\")\n",
    "\n",
    "image_tfms = build_image_transforms(config.image_size)\n",
    "\n",
    "\n",
    "class Flickr8kCaptionDataset(Dataset):\n",
    "    def __init__(self, hf_dataset: HFDataset, transform, max_caption_len: int, cfg_prob: float = 0.1):\n",
    "        self.ds = hf_dataset\n",
    "        self.transform = transform\n",
    "        self.max_caption_len = max_caption_len\n",
    "        self.cfg_prob = cfg_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.ds[idx]\n",
    "        image = ex.get('image') or ex.get('img')\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Captions field variants: 'caption', 'sentences', 'captions'\n",
    "        caption = ''\n",
    "        if 'caption' in ex:\n",
    "            # may already be a string or list\n",
    "            c = ex['caption']\n",
    "            if isinstance(c, list):\n",
    "                caption = random.choice(c)\n",
    "            else:\n",
    "                caption = str(c)\n",
    "        elif 'captions' in ex:\n",
    "            c = ex['captions']\n",
    "            caption = random.choice(c) if isinstance(c, list) and c else str(c)\n",
    "        elif 'sentences' in ex:\n",
    "            c = ex['sentences']\n",
    "            # some variants list of dicts with 'raw'\n",
    "            if isinstance(c, list) and len(c) > 0:\n",
    "                choice = random.choice(c)\n",
    "                if isinstance(choice, dict):\n",
    "                    caption = choice.get('raw', '')\n",
    "                else:\n",
    "                    caption = str(choice)\n",
    "\n",
    "        # Classifier-free guidance dropout\n",
    "        if random.random() < self.cfg_prob:\n",
    "            caption = ''\n",
    "\n",
    "        return {\"pixel_values\": image, \"text\": caption}\n",
    "\n",
    "\n",
    "def collate_fn(batch: List[Dict[str, Any]], tokenizer: CLIPTokenizer, max_len: int):\n",
    "    pixel_values = torch.stack([b['pixel_values'] for b in batch])\n",
    "    captions = [b['text'] for b in batch]\n",
    "    tokenized = tokenizer(\n",
    "        captions,\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'input_ids': tokenized.input_ids,\n",
    "        'attention_mask': tokenized.attention_mask,\n",
    "        'captions': captions,\n",
    "    }\n",
    "\n",
    "\n",
    "# Tokenizer placeholder (will be loaded later). We'll create loaders after tokenizer init.\n",
    "train_dataset = Flickr8kCaptionDataset(train_ds, image_tfms, config.max_caption_length, config.classifier_free_guidance_prob)\n",
    "val_dataset = Flickr8kCaptionDataset(val_ds, image_tfms, config.max_caption_length, 0.0)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    print(f\"Wrapped train dataset length: {len(train_dataset)} | val dataset length: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c423e",
   "metadata": {},
   "source": [
    "## 7–8. Initialize Tokenizer, Text Encoder, VAE, UNet & Noise Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c88205",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading CLIP tokenizer & text encoder...\")\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "TokenizerClass = CLIPTokenizer\n",
    "TextEncoderClass = CLIPTextModel\n",
    "\n",
    "# Load tokenizer first for DataLoader collate\n",
    "tokenizer = TokenizerClass.from_pretrained(clip_model_name)\n",
    "\n",
    "# Build DataLoaders now that we have tokenizer\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=lambda b: collate_fn(b, tokenizer, config.max_caption_length),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=lambda b: collate_fn(b, tokenizer, config.max_caption_length),\n",
    ")\n",
    "\n",
    "print(\"Loading CLIP text encoder...\")\n",
    "text_encoder = TextEncoderClass.from_pretrained(clip_model_name)\n",
    "text_encoder.eval()\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Loading pretrained VAE (AutoencoderKL)...\")\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\")\n",
    "vae.eval()\n",
    "for p in vae.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Creating conditional UNet...\")\n",
    "unet = UNet2DConditionModel(\n",
    "    sample_size=config.image_size // 8,  # latent resolution\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\"\n",
    "    ),\n",
    "    cross_attention_dim=text_encoder.config.hidden_size,\n",
    ")\n",
    "\n",
    "# Noise scheduler for training\n",
    "noise_scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", num_train_timesteps=1000)\n",
    "\n",
    "print(\"Models initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97795acb",
   "metadata": {},
   "source": [
    "## 9–10. Optimizer, LR Scheduler, EMA, Accelerator Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02578541",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "# Set total training steps for scheduler (approximation)\n",
    "steps_per_epoch = max(1, len(train_loader) // accelerator.num_processes)\n",
    "t_total = steps_per_epoch * config.num_epochs // config.gradient_accumulation_steps\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=max(10, t_total // 100), num_training_steps=t_total)\n",
    "\n",
    "ema_unet = EMAModel(unet.parameters(), decay=config.ema_decay)\n",
    "\n",
    "# Enable gradient checkpointing and optional xFormers\n",
    "try:\n",
    "    unet.enable_gradient_checkpointing()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    unet.enable_xformers_memory_efficient_attention()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Prepare with accelerator\n",
    "(unet, optimizer, train_loader, val_loader, lr_scheduler) = accelerator.prepare(\n",
    "    unet, optimizer, train_loader, val_loader, lr_scheduler\n",
    ")\n",
    "\n",
    "def encode_text(input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.FloatTensor:\n",
    "    with torch.no_grad():\n",
    "        return text_encoder(input_ids.to(device), attention_mask=attention_mask.to(device))[0]\n",
    "\n",
    "print(\"Optimization components ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20da27b",
   "metadata": {},
   "source": [
    "## 11. Training Step Definition (Noise Prediction Objective)\n",
    "The loss is $\\mathcal{L} = \\mathbb{E}_{t,\\epsilon}[\\lVert \\epsilon - \\epsilon_\\theta(x_t, t, c) \\rVert^2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcec375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch) -> torch.Tensor:\n",
    "    pixel_values = batch['pixel_values'].to(device)\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latents = vae.encode(pixel_values).latent_dist.sample() * 0.18215\n",
    "    # Sample noise\n",
    "    noise = torch.randn_like(latents)\n",
    "    bsz = latents.shape[0]\n",
    "    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device).long()\n",
    "    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "    encoder_hidden_states = encode_text(input_ids, attention_mask)\n",
    "\n",
    "    with accelerator.autocast():\n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "        loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c237f511",
   "metadata": {},
   "source": [
    "## 12–15. Distributed Training Loop, Sampling, and Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(step: int, is_epoch: bool = False):\n",
    "    tag = f\"epoch_{step}\" if is_epoch else f\"step_{step}\"\n",
    "    ckpt = {\n",
    "        'unet': accelerator.get_state_dict(unet),\n",
    "        'ema': ema_unet.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'step': step,\n",
    "        'config': vars(config),\n",
    "    }\n",
    "    path = os.path.join(config.output_dir, f\"unet_{tag}.pt\")\n",
    "    accelerator.save(ckpt, path)\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Saved checkpoint: {path}\")\n",
    "\n",
    "\n",
    "def sample_prompts(unet_for_eval: UNet2DConditionModel, prompts: List[str], num_inference_steps: int = 50, guidance_scale: float = 7.5, seed: Optional[int] = 42, save_path: Optional[str] = None):\n",
    "    # Use EMA weights for sampling\n",
    "    unet_for_eval.eval()\n",
    "\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    images = []\n",
    "    with torch.no_grad():\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            tokens = tokenizer(prompt, padding='max_length', max_length=config.max_caption_length, truncation=True, return_tensors='pt')\n",
    "            text_emb = text_encoder(tokens.input_ids.to(device), attention_mask=tokens.attention_mask.to(device))[0]\n",
    "\n",
    "            if guidance_scale > 1.0:\n",
    "                uncond = tokenizer([\"\"], padding='max_length', max_length=config.max_caption_length, return_tensors='pt')\n",
    "                uncond_emb = text_encoder(uncond.input_ids.to(device), attention_mask=uncond.attention_mask.to(device))[0]\n",
    "                text_emb = torch.cat([uncond_emb, text_emb], dim=0)\n",
    "\n",
    "            latents = torch.randn((1, 4, config.image_size // 8, config.image_size // 8), device=device)\n",
    "\n",
    "            for t in scheduler.timesteps:\n",
    "                latent_model_input = latents if guidance_scale <= 1.0 else torch.cat([latents] * 2)\n",
    "                latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "                noise_pred = unet_for_eval(latent_model_input, t, encoder_hidden_states=text_emb).sample\n",
    "                if guidance_scale > 1.0:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "            latents = latents / 0.18215\n",
    "            image = vae.decode(latents).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            images.append(image.cpu())\n",
    "\n",
    "    grid = make_grid(torch.cat(images, dim=0), nrow=min(4, len(images)))\n",
    "    if save_path and accelerator.is_main_process:\n",
    "        save_image(grid, save_path)\n",
    "        print(f\"Saved samples to {save_path}\")\n",
    "    return grid\n",
    "\n",
    "\n",
    "def train_loop():\n",
    "    global_step = 0\n",
    "    ema_unet.to(device)\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        unet.train()\n",
    "        pbar = enumerate(train_loader)\n",
    "        total_loss = 0.0\n",
    "        for step, batch in pbar:\n",
    "            with accelerator.accumulate(unet):\n",
    "                loss = training_step(batch)\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                ema_unet.step(unet.parameters())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            if accelerator.is_main_process and (global_step % 50 == 0):\n",
    "                avg = total_loss / (step + 1)\n",
    "                print(f\"Epoch {epoch+1}/{config.num_epochs} | step {global_step} | loss {avg:.4f}\")\n",
    "\n",
    "            if accelerator.is_main_process and (global_step % config.sample_every == 0):\n",
    "                # Copy EMA params to a temp UNet for eval\n",
    "                ema_unet.store(unet.parameters())\n",
    "                ema_unet.copy_to(unet.parameters())\n",
    "                samples_path = os.path.join(config.output_dir, f\"samples_step_{global_step}.png\")\n",
    "                _ = sample_prompts(unet, config.prompts[:8], num_inference_steps=50, guidance_scale=7.5, seed=42, save_path=samples_path)\n",
    "                ema_unet.restore(unet.parameters())\n",
    "\n",
    "            if accelerator.is_main_process and (global_step % config.save_every == 0):\n",
    "                save_checkpoint(global_step, is_epoch=False)\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            # End of epoch: save checkpoint and do val sampling\n",
    "            save_checkpoint(epoch + 1, is_epoch=True)\n",
    "            ema_unet.store(unet.parameters())\n",
    "            ema_unet.copy_to(unet.parameters())\n",
    "            val_path = os.path.join(config.output_dir, f\"val_samples_epoch_{epoch+1}.png\")\n",
    "            _ = sample_prompts(unet, config.prompts[:8], num_inference_steps=50, guidance_scale=7.5, seed=123, save_path=val_path)\n",
    "            ema_unet.restore(unet.parameters())\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Training complete.\")\n",
    "\n",
    "\n",
    "train_loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b863d0",
   "metadata": {},
   "source": [
    "## 17. Inference: Text-to-Image with Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, num_inference_steps: int = 50, guidance_scale: float = 7.5, seed: int = 42):\n",
    "    unet.eval()\n",
    "    ema_unet.store(unet.parameters())\n",
    "    ema_unet.copy_to(unet.parameters())\n",
    "\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    scheduler = DDIMScheduler(beta_schedule=\"squaredcos_cap_v2\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    tokens = tokenizer([prompt], padding='max_length', max_length=config.max_caption_length, truncation=True, return_tensors='pt')\n",
    "    text_emb = text_encoder(tokens.input_ids.to(device), attention_mask=tokens.attention_mask.to(device))[0]\n",
    "\n",
    "    if guidance_scale > 1.0:\n",
    "        uncond = tokenizer([\"\"], padding='max_length', max_length=config.max_caption_length, return_tensors='pt')\n",
    "        uncond_emb = text_encoder(uncond.input_ids.to(device), attention_mask=uncond.attention_mask.to(device))[0]\n",
    "        text_emb = torch.cat([uncond_emb, text_emb], dim=0)\n",
    "\n",
    "    latents = torch.randn((1, 4, config.image_size // 8, config.image_size // 8), device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in scheduler.timesteps:\n",
    "            latent_model_input = latents if guidance_scale <= 1.0 else torch.cat([latents] * 2)\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_emb).sample\n",
    "            if guidance_scale > 1.0:\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "        latents = latents / 0.18215\n",
    "        image = vae.decode(latents).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "    ema_unet.restore(unet.parameters())\n",
    "    return image.cpu()\n",
    "\n",
    "# Example prompt generation (after training) - uncomment when ready\n",
    "# out = generate(\"A dog running through a field\", num_inference_steps=50, guidance_scale=7.5, seed=123)\n",
    "# save_image(out, os.path.join(config.output_dir, \"example_generation.png\"))\n",
    "# print(\"Saved example_generation.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0584ae",
   "metadata": {},
   "source": [
    "## 6b. Quick Visual Check of a Batch (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c9d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample batch (run before training if desired)\n",
    "if accelerator.is_main_process:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    imgs = sample_batch['pixel_values'][:8]\n",
    "    caps = sample_batch['captions'][:8]\n",
    "    grid = make_grid(imgs, nrow=4)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(grid.permute(1,2,0).numpy() * 0.5 + 0.5)\n",
    "    plt.axis('off')\n",
    "    plt.title('Sample Flickr8k Images (first 8)')\n",
    "    plt.show()\n",
    "    print(\"Captions:\")\n",
    "    for i,c in enumerate(caps):\n",
    "        print(f\"{i+1}. {c}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
