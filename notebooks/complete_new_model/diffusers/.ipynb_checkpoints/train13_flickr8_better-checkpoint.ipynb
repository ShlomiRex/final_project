{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42fc9d31",
   "metadata": {},
   "source": [
    "# ðŸ”§ System Diagnostic - Run This First!\n",
    "\n",
    "Before training, we need to verify that the GPU is properly accessible. This cell will check:\n",
    "1. Slurm job allocation\n",
    "2. GPU hardware detection\n",
    "3. CUDA environment variables\n",
    "4. PyTorch CUDA compatibility\n",
    "5. Common issues and solutions\n",
    "\n",
    "**Run the diagnostic cell below FIRST before proceeding with training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0488f15a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " ðŸ” COMPREHENSIVE GPU DIAGNOSTIC FOR HPC CLUSTER\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£  SLURM JOB ALLOCATION\n",
      "--------------------------------------------------------------------------------\n",
      "  Job ID                   : 3032684\n",
      "  Assigned Node(s)         : gpu8\n",
      "  Node ID                  : 0\n",
      "  Total GPUs Allocated     : NOT SET\n",
      "  GPUs on This Node        : 2\n",
      "  GPU IDs Allocated        : 0,7\n",
      "  CPUs on Node             : 2\n",
      "  Memory per Node          : 16384\n",
      "\n",
      "  âœ… Slurm has allocated GPU(s) to this job\n",
      "\n",
      "2ï¸âƒ£  CUDA ENVIRONMENT VARIABLES\n",
      "--------------------------------------------------------------------------------\n",
      "  CUDA_VISIBLE_DEVICES     : 0,1\n",
      "  CUDA_HOME                : /prefix/software/CUDA/11.8.0\n",
      "  CUDA_PATH                : /prefix/software/CUDA/11.8.0\n",
      "  CUDA_ROOT                : /prefix/software/CUDA/11.8.0\n",
      "  LD_LIBRARY_PATH          : /prefix/software/CUDA/11.8.0/nvvm/lib64 (and 2 more)\n",
      "\n",
      "  âœ… CUDA_VISIBLE_DEVICES is set\n",
      "\n",
      "3ï¸âƒ£  GPU HARDWARE DETECTION (nvidia-smi)\n",
      "--------------------------------------------------------------------------------\n",
      "index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "0, NVIDIA A100 80GB PCIe, 470.161.03, 80994 MiB, 0 MiB, 80994 MiB\n",
      "1, NVIDIA A100 80GB PCIe, 470.161.03, 80994 MiB, 0 MiB, 80994 MiB\n",
      "\n",
      "  âœ… GPU hardware detected successfully\n",
      "\n",
      "4ï¸âƒ£  CUDA TOOLKIT VERSION\n",
      "--------------------------------------------------------------------------------\n",
      "  System CUDA: Cuda compilation tools, release 11.8, V11.8.89\n",
      "  CUDA Version: 11.8\n",
      "\n",
      "5ï¸âƒ£  PYTORCH CUDA DETECTION\n",
      "--------------------------------------------------------------------------------\n",
      "  PyTorch Version: 2.6.0\n",
      "  PyTorch Built with CUDA: None\n",
      "  CUDA Available: False\n",
      "\n",
      "  âŒ PyTorch CANNOT access GPU!\n",
      "\n",
      "  ðŸ” DIAGNOSIS:\n",
      "\n",
      "================================================================================\n",
      " ðŸ“‹ SUMMARY & RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "  âœ… Slurm GPU Allocation\n",
      "  âœ… CUDA Environment\n",
      "  âœ… GPU Hardware (nvidia-smi)\n",
      "  âŒ PyTorch CUDA Access\n",
      "\n",
      "âš ï¸  ISSUES DETECTED. Review the diagnostic output above.\n",
      "\n",
      "Common solutions:\n",
      "\n",
      "1. If 'Slurm GPU Allocation' failed:\n",
      "   - Make sure you started Jupyter with: bash slurm/start_jupyter.sh gpu7\n",
      "   - Check job allocation: squeue -u $USER\n",
      "\n",
      "2. If 'PyTorch CUDA Access' failed but hardware is OK:\n",
      "   - Most likely CUDA version mismatch\n",
      "   - Run the PyTorch reinstall cell below\n",
      "\n",
      "3. If 'GPU Hardware' failed:\n",
      "   - Job may be on a CPU-only node\n",
      "   - Cancel and restart with: bash slurm/start_jupyter.sh gpu7\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive GPU Diagnostic for HPC Cluster\n",
    "This cell diagnoses why PyTorch might show \"device: cpu\" even on GPU nodes\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" ðŸ” COMPREHENSIVE GPU DIAGNOSTIC FOR HPC CLUSTER\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# ===========================================================================\n",
    "# 1. SLURM JOB INFORMATION\n",
    "# ===========================================================================\n",
    "print(\"1ï¸âƒ£  SLURM JOB ALLOCATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "slurm_vars = {\n",
    "    'SLURM_JOB_ID': 'Job ID',\n",
    "    'SLURM_JOB_NODELIST': 'Assigned Node(s)',\n",
    "    'SLURM_NODEID': 'Node ID',\n",
    "    'SLURM_GPUS': 'Total GPUs Allocated',\n",
    "    'SLURM_GPUS_ON_NODE': 'GPUs on This Node',\n",
    "    'SLURM_JOB_GPUS': 'GPU IDs Allocated',\n",
    "    'SLURM_CPUS_ON_NODE': 'CPUs on Node',\n",
    "    'SLURM_MEM_PER_NODE': 'Memory per Node',\n",
    "}\n",
    "\n",
    "slurm_allocated = False\n",
    "for var, desc in slurm_vars.items():\n",
    "    value = os.environ.get(var, 'NOT SET')\n",
    "    print(f\"  {desc:25s}: {value}\")\n",
    "    if var in ['SLURM_GPUS', 'SLURM_GPUS_ON_NODE', 'SLURM_JOB_GPUS']:\n",
    "        if value != 'NOT SET' and value != '0' and value != '':\n",
    "            slurm_allocated = True\n",
    "\n",
    "print()\n",
    "if slurm_allocated:\n",
    "    print(\"  âœ… Slurm has allocated GPU(s) to this job\")\n",
    "else:\n",
    "    print(\"  âš ï¸  WARNING: No GPU allocation detected by Slurm!\")\n",
    "    print(\"     This job may not have requested GPU resources.\")\n",
    "    print()\n",
    "\n",
    "# ===========================================================================\n",
    "# 2. CUDA ENVIRONMENT VARIABLES\n",
    "# ===========================================================================\n",
    "print()\n",
    "print(\"2ï¸âƒ£  CUDA ENVIRONMENT VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "cuda_vars = {\n",
    "    'CUDA_VISIBLE_DEVICES': 'Which GPUs are visible to CUDA',\n",
    "    'CUDA_HOME': 'CUDA installation directory',\n",
    "    'CUDA_PATH': 'CUDA path',\n",
    "    'CUDA_ROOT': 'CUDA root directory',\n",
    "    'LD_LIBRARY_PATH': 'Library path (includes CUDA libs)',\n",
    "}\n",
    "\n",
    "cuda_env_ok = False\n",
    "for var, desc in cuda_vars.items():\n",
    "    value = os.environ.get(var, 'NOT SET')\n",
    "    if var == 'LD_LIBRARY_PATH' and value != 'NOT SET':\n",
    "        # Show only CUDA-related parts\n",
    "        cuda_parts = [p for p in value.split(':') if 'cuda' in p.lower() or 'CUDA' in p]\n",
    "        if cuda_parts:\n",
    "            print(f\"  {var:25s}: {cuda_parts[0]} (and {len(cuda_parts)-1} more)\")\n",
    "        else:\n",
    "            print(f\"  {var:25s}: (no CUDA paths found)\")\n",
    "    else:\n",
    "        print(f\"  {var:25s}: {value}\")\n",
    "    \n",
    "    if var == 'CUDA_VISIBLE_DEVICES' and value != 'NOT SET':\n",
    "        cuda_env_ok = True\n",
    "\n",
    "print()\n",
    "if cuda_env_ok:\n",
    "    print(\"  âœ… CUDA_VISIBLE_DEVICES is set\")\n",
    "else:\n",
    "    print(\"  âš ï¸  WARNING: CUDA_VISIBLE_DEVICES not set!\")\n",
    "    print(\"     GPUs may not be visible to applications.\")\n",
    "    print()\n",
    "\n",
    "# ===========================================================================\n",
    "# 3. GPU HARDWARE DETECTION (nvidia-smi)\n",
    "# ===========================================================================\n",
    "print()\n",
    "print(\"3ï¸âƒ£  GPU HARDWARE DETECTION (nvidia-smi)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free',\n",
    "         '--format=csv'],\n",
    "        capture_output=True, text=True, timeout=5\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout)\n",
    "        print(\"  âœ… GPU hardware detected successfully\")\n",
    "        hardware_ok = True\n",
    "    else:\n",
    "        print(f\"  âŒ nvidia-smi failed with error:\\n{result.stderr}\")\n",
    "        hardware_ok = False\n",
    "except FileNotFoundError:\n",
    "    print(\"  âŒ nvidia-smi command not found!\")\n",
    "    print(\"     GPU drivers may not be installed.\")\n",
    "    hardware_ok = False\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Error running nvidia-smi: {e}\")\n",
    "    hardware_ok = False\n",
    "\n",
    "print()\n",
    "\n",
    "# ===========================================================================\n",
    "# 4. CUDA TOOLKIT VERSION\n",
    "# ===========================================================================\n",
    "print(\"4ï¸âƒ£  CUDA TOOLKIT VERSION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'release' in line.lower():\n",
    "                print(f\"  System CUDA: {line.strip()}\")\n",
    "                # Extract version number\n",
    "                import re\n",
    "                match = re.search(r'release (\\d+\\.\\d+)', line)\n",
    "                if match:\n",
    "                    cuda_version = match.group(1)\n",
    "                    print(f\"  CUDA Version: {cuda_version}\")\n",
    "    else:\n",
    "        print(\"  âš ï¸  nvcc not found (CUDA toolkit may not be in PATH)\")\n",
    "except Exception as e:\n",
    "    print(f\"  âš ï¸  Could not determine CUDA version: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ===========================================================================\n",
    "# 5. PYTORCH CUDA DETECTION\n",
    "# ===========================================================================\n",
    "print(\"5ï¸âƒ£  PYTORCH CUDA DETECTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"  PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"  PyTorch Built with CUDA: {torch.version.cuda}\")\n",
    "    print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"    GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"      Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print()\n",
    "        print(\"  âœ… PyTorch can access GPU(s)!\")\n",
    "        pytorch_ok = True\n",
    "    else:\n",
    "        print()\n",
    "        print(\"  âŒ PyTorch CANNOT access GPU!\")\n",
    "        pytorch_ok = False\n",
    "        \n",
    "        # Diagnose why\n",
    "        print()\n",
    "        print(\"  ðŸ” DIAGNOSIS:\")\n",
    "        \n",
    "        # Check CUDA version mismatch\n",
    "        if torch.version.cuda:\n",
    "            pytorch_cuda = torch.version.cuda\n",
    "            print(f\"     - PyTorch was built for CUDA {pytorch_cuda}\")\n",
    "            \n",
    "            # Try to get system CUDA version\n",
    "            try:\n",
    "                nvcc_result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=5)\n",
    "                if nvcc_result.returncode == 0:\n",
    "                    import re\n",
    "                    match = re.search(r'release (\\d+\\.\\d+)', nvcc_result.stdout)\n",
    "                    if match:\n",
    "                        system_cuda = match.group(1)\n",
    "                        print(f\"     - System has CUDA {system_cuda}\")\n",
    "                        \n",
    "                        # Compare major versions\n",
    "                        pytorch_major = pytorch_cuda.split('.')[0]\n",
    "                        system_major = system_cuda.split('.')[0]\n",
    "                        \n",
    "                        if pytorch_major != system_major:\n",
    "                            print()\n",
    "                            print(f\"     âš ï¸  CUDA VERSION MISMATCH!\")\n",
    "                            print(f\"         PyTorch needs CUDA {pytorch_major}.x\")\n",
    "                            print(f\"         System has CUDA {system_major}.x\")\n",
    "                            print()\n",
    "                            print(f\"     ðŸ’¡ SOLUTION: Reinstall PyTorch with CUDA {system_major}.x support\")\n",
    "                            if system_major == '11':\n",
    "                                print(f\"         pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "                            elif system_major == '12':\n",
    "                                print(f\"         pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Check if CUDA_VISIBLE_DEVICES is set\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES') == '':\n",
    "            print(\"     - CUDA_VISIBLE_DEVICES is empty (no GPUs visible)\")\n",
    "        elif os.environ.get('CUDA_VISIBLE_DEVICES') is None:\n",
    "            print(\"     - CUDA_VISIBLE_DEVICES is not set\")\n",
    "        \n",
    "        # Check if Slurm allocated GPU\n",
    "        if not slurm_allocated:\n",
    "            print(\"     - Slurm did not allocate GPU to this job\")\n",
    "            print(\"       Did you request GPU with --gres=gpu:1?\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"  âŒ PyTorch is not installed!\")\n",
    "    pytorch_ok = False\n",
    "\n",
    "print()\n",
    "\n",
    "# ===========================================================================\n",
    "# 6. SUMMARY & RECOMMENDATIONS\n",
    "# ===========================================================================\n",
    "print(\"=\"*80)\n",
    "print(\" ðŸ“‹ SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "all_checks = {\n",
    "    'Slurm GPU Allocation': slurm_allocated,\n",
    "    'CUDA Environment': cuda_env_ok,\n",
    "    'GPU Hardware (nvidia-smi)': hardware_ok,\n",
    "    'PyTorch CUDA Access': pytorch_ok,\n",
    "}\n",
    "\n",
    "for check, status in all_checks.items():\n",
    "    status_icon = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"  {status_icon} {check}\")\n",
    "\n",
    "print()\n",
    "\n",
    "if all(all_checks.values()):\n",
    "    print(\"ðŸŽ‰ ALL CHECKS PASSED! GPU is ready for training.\")\n",
    "else:\n",
    "    print(\"âš ï¸  ISSUES DETECTED. Review the diagnostic output above.\")\n",
    "    print()\n",
    "    print(\"Common solutions:\")\n",
    "    print()\n",
    "    print(\"1. If 'Slurm GPU Allocation' failed:\")\n",
    "    print(\"   - Make sure you started Jupyter with: bash slurm/start_jupyter.sh gpu7\")\n",
    "    print(\"   - Check job allocation: squeue -u $USER\")\n",
    "    print()\n",
    "    print(\"2. If 'PyTorch CUDA Access' failed but hardware is OK:\")\n",
    "    print(\"   - Most likely CUDA version mismatch\")\n",
    "    print(\"   - Run the PyTorch reinstall cell below\")\n",
    "    print()\n",
    "    print(\"3. If 'GPU Hardware' failed:\")\n",
    "    print(\"   - Job may be on a CPU-only node\")\n",
    "    print(\"   - Cancel and restart with: bash slurm/start_jupyter.sh gpu7\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484027a",
   "metadata": {},
   "source": [
    "# Train Conditional UNet2D on Flickr8k with Text Captions + VAE\n",
    "\n",
    "This notebook trains a **text-conditional** diffusion model on Flickr8k using:\n",
    "- **Pretrained VAE** (AutoencoderKL from Stable Diffusion) to encode images to latents\n",
    "- **Pretrained CLIP** text encoder for text embeddings\n",
    "- **UNet2DConditionModel** (with cross-attention for text conditioning)\n",
    "- **DDPM scheduler** for training and inference\n",
    "\n",
    "## Key Design Choices\n",
    "- Flickr8k images resized to 256x256\n",
    "- Images normalized to [-1,1] for VAE compatibility\n",
    "- VAE encodes to 32x32x4 latents\n",
    "- CLIP encodes text captions to 77x768 embeddings\n",
    "- Only the UNet is trained; VAE and CLIP are frozen\n",
    "- **Text-conditional generation** - generate images from text prompts\n",
    "\n",
    "## Memory Optimizations for GPU\n",
    "- **Batch size**: 16 (adjustable based on GPU memory)\n",
    "- **Image size**: 256x256\n",
    "- **Mixed precision**: Enabled (FP16)\n",
    "- **Cache clearing**: Periodic GPU cache clearing to prevent fragmentation\n",
    "- **DataLoader**: num_workers=2, pin_memory=True for efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434c71c",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ Flickr8k Text-to-Image Conditional Diffusion Training\n",
    "\n",
    "## About Flickr8k Dataset\n",
    "\n",
    "1. **Dataset**: Flickr8k - 8,000 images from Flickr\n",
    "2. **Captions**: 5 human-written captions per image (40,000 total captions)\n",
    "3. **Content**: Diverse everyday scenes with people, animals, objects, and activities\n",
    "4. **Image Complexity**: Natural photographs with varied subjects and backgrounds\n",
    "5. **Text Descriptions**: Detailed natural language descriptions of image content\n",
    "6. **Download**: Automatically loaded via HuggingFace datasets\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Run the diagnostic cell to verify GPU access\n",
    "2. Configure training parameters (adjust batch size for your GPU)\n",
    "3. Start training - checkpoints and sample images saved every 500 steps\n",
    "4. Monitor progress through text-to-image samples\n",
    "5. Generate images from custom text prompts!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef2508",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e9d64",
   "metadata": {},
   "source": [
    "## Memory Usage Tips\n",
    "\n",
    "If you still encounter OOM errors, try these additional optimizations:\n",
    "\n",
    "1. **Further reduce batch size**: Change `batch_size` to 8 or even 4\n",
    "2. **Reduce image size**: Change `image_size` to 64 (will train faster but lower quality)\n",
    "3. **Enable gradient checkpointing** (uncomment in UNet creation if available)\n",
    "4. **Close other applications** that might be using GPU memory\n",
    "5. **Restart the kernel** to clear any lingering memory allocations\n",
    "\n",
    "Current settings should work on most 8GB GPUs with CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c68263",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "operator torchvision::nms does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/work/conda/envs/torch114/lib/python3.9/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/work/conda/envs/torch114/lib/python3.9/site-packages/torchvision/_meta_registrations.py:164\u001b[0m\n\u001b[1;32m    153\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m    154\u001b[0m         grad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         ),\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchvision::nms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mmeta_nms\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should be a 2d tensor, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes should have 4 elements in dimension 1, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/conda/envs/torch114/lib/python3.9/site-packages/torch/library.py:828\u001b[0m, in \u001b[0;36mregister_fake.<locals>.register\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     use_lib \u001b[38;5;241m=\u001b[39m lib\n\u001b[0;32m--> 828\u001b[0m \u001b[43muse_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "File \u001b[0;32m~/work/conda/envs/torch114/lib/python3.9/site-packages/torch/library.py:198\u001b[0m, in \u001b[0;36mLibrary._register_fake\u001b[0;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     func_to_register \u001b[38;5;241m=\u001b[39m fn\n\u001b[0;32m--> 198\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfake_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_to_register\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registration_handles\u001b[38;5;241m.\u001b[39mappend(handle)\n",
      "File \u001b[0;32m~/work/conda/envs/torch114/lib/python3.9/site-packages/torch/_library/fake_impl.py:31\u001b[0m, in \u001b[0;36mFakeImplHolder.register\u001b[0;34m(self, func, source)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an fake impl registered at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_has_kernel_for_dispatch_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqualname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready has an DispatchKey::Meta implementation via a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_fake.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_dispatch_has_kernel_for_dispatch_key(\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqualname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompositeImplicitAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m ):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.models import AutoencoderKL\n",
    "from diffusers.models.unets.unet_2d_condition import UNet2DConditionModel\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# TensorBoard SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Accelerate for multi-GPU training\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Metrics: FID and CLIP Score\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.multimodal.clip_score import CLIPScore\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac9a5d",
   "metadata": {},
   "source": [
    "## ðŸš€ Multi-GPU Training with Accelerate\n",
    "\n",
    "This notebook now supports **multi-GPU training** using the Accelerate library!\n",
    "\n",
    "### How to use Multi-GPU Training:\n",
    "\n",
    "**Option 1: Launch from Terminal (Recommended for HPC)**\n",
    "```bash\n",
    "# For 2 GPUs\n",
    "accelerate launch --multi_gpu --num_processes=2 train13_flickr8_better.ipynb\n",
    "\n",
    "# For 4 GPUs\n",
    "accelerate launch --multi_gpu --num_processes=4 train13_flickr8_better.ipynb\n",
    "```\n",
    "\n",
    "**Option 2: Configure Accelerate (One-time setup)**\n",
    "```bash\n",
    "accelerate config\n",
    "```\n",
    "Follow the prompts to configure:\n",
    "- Multi-GPU training\n",
    "- Number of processes (GPUs)\n",
    "- Mixed precision (fp16 recommended)\n",
    "\n",
    "**Option 3: Run in Notebook (Current Method)**\n",
    "- The notebook will automatically use all available GPUs\n",
    "- Accelerator detects GPUs and distributes the workload\n",
    "- Only the main process (rank 0) saves checkpoints and logs to TensorBoard\n",
    "\n",
    "### Multi-GPU Benefits:\n",
    "- **Faster Training**: Linear speedup with number of GPUs (2x faster with 2 GPUs, 4x with 4 GPUs)\n",
    "- **Larger Batch Sizes**: Effective batch size = batch_size Ã— num_gpus\n",
    "- **Automatic Distribution**: Accelerate handles data distribution and gradient synchronization\n",
    "- **Mixed Precision**: Automatic FP16 support for faster training and less memory\n",
    "\n",
    "### Notes:\n",
    "- Each GPU will process `batch_size` samples, so effective batch size = `batch_size Ã— num_gpus`\n",
    "- With 2 GPUs and batch_size=16, you're effectively training with batch_size=32\n",
    "- Only rank 0 (main process) saves checkpoints and generates samples to avoid conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0149ad",
   "metadata": {},
   "source": [
    "## IMPORTANT: CUDA Setup Check\n",
    "\n",
    "If you see \"Device: cpu\" in the output above, it means PyTorch cannot access the GPU. This is usually because:\n",
    "\n",
    "1. **CUDA version mismatch**: Your PyTorch is built for CUDA 12.x but the cluster has CUDA 11.x\n",
    "2. **PyTorch not installed with CUDA support**\n",
    "\n",
    "Run the diagnostic cell below to check and fix this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3055dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Diagnostic and Fix\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU DIAGNOSTIC CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check PyTorch version and CUDA support\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        print(\"\\nâœ… GPU is working correctly!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  PyTorch cannot access GPU!\")\n",
    "        \n",
    "        # Check if nvidia-smi works\n",
    "        try:\n",
    "            result = subprocess.run(['nvidia-smi', '--query-gpu=name,driver_version,memory.total', \n",
    "                                   '--format=csv,noheader'], \n",
    "                                  capture_output=True, text=True, timeout=5)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"\\nGPU hardware detected:\")\n",
    "                print(f\"  {result.stdout.strip()}\")\n",
    "                \n",
    "                # Check CUDA version on system\n",
    "                cuda_result = subprocess.run(['nvcc', '--version'], \n",
    "                                           capture_output=True, text=True, timeout=5)\n",
    "                if cuda_result.returncode == 0:\n",
    "                    print(f\"\\nSystem CUDA:\")\n",
    "                    for line in cuda_result.stdout.split('\\n'):\n",
    "                        if 'release' in line.lower():\n",
    "                            print(f\"  {line.strip()}\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(\"SOLUTION: Reinstall PyTorch with correct CUDA version\")\n",
    "                print(\"=\"*70)\n",
    "                print(\"\\nYour PyTorch was built for a different CUDA version than\")\n",
    "                print(\"what's available on this system.\")\n",
    "                print(\"\\nTo fix this, run the following command:\")\n",
    "                print(\"\\n  pip uninstall torch torchvision torchaudio -y\")\n",
    "                print(\"  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "                print(\"\\nThis will install PyTorch with CUDA 11.8 support (matching your system).\")\n",
    "                print(\"\\nWould you like to automatically fix this? (This will take a few minutes)\")\n",
    "                print(\"If yes, run the cell below.\")\n",
    "            else:\n",
    "                print(\"\\nâŒ GPU hardware not detected by nvidia-smi\")\n",
    "                print(\"This job might not have GPU allocated!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError checking GPU: {e}\")\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"âŒ PyTorch is not installed!\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6954628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTO-FIX: Reinstall PyTorch with correct CUDA version\n",
    "# WARNING: This will uninstall and reinstall PyTorch (takes 3-5 minutes)\n",
    "# Only run this cell if the diagnostic above showed CUDA is not available\n",
    "\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# print(\"Uninstalling current PyTorch...\")\n",
    "# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \n",
    "#                       \"torch\", \"torchvision\", \"torchaudio\"])\n",
    "\n",
    "# print(\"\\nInstalling PyTorch with CUDA 11.8 support...\")\n",
    "# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \n",
    "#                       \"torch\", \"torchvision\", \"torchaudio\",\n",
    "#                       \"--index-url\", \"https://download.pytorch.org/whl/cu118\"])\n",
    "\n",
    "# print(\"\\nVerifying installation...\")\n",
    "# import torch\n",
    "# print(f\"PyTorch version: {torch.__version__}\")\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"CUDA version: {torch.version.cuda}\")\n",
    "#     print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "#     print(\"\\nâœ… GPU is now working!\")\n",
    "# else:\n",
    "#     print(\"\\nâš ï¸  Still having issues. Please check Slurm job GPU allocation.\")\n",
    "\n",
    "# print(\"\\nâš ï¸  IMPORTANT: Restart the kernel after installing PyTorch!\")\n",
    "# print(\"   Go to: Kernel -> Restart Kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88700b47",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e388bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def build_transforms(image_size: int = 256) -> transforms.Compose:\n",
    "    \"\"\"Transform Flickr8k images: resize to square, normalize to [-1,1] for VAE.\"\"\"\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def encode_text(text: str, tokenizer, text_encoder, device):\n",
    "    \"\"\"Encode text prompt into embeddings using CLIP.\"\"\"\n",
    "    text_inputs = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_input_ids)[0]\n",
    "    \n",
    "    return text_embeddings\n",
    "\n",
    "\n",
    "def compute_fid_score(real_images, generated_images, device):\n",
    "    \"\"\"\n",
    "    Compute FID score between real and generated images.\n",
    "    \n",
    "    Args:\n",
    "        real_images: Tensor of real images (B, C, H, W) in range [0, 1]\n",
    "        generated_images: Tensor of generated images (B, C, H, W) in range [0, 1]\n",
    "        device: Device to run computation on\n",
    "    \n",
    "    Returns:\n",
    "        FID score as float\n",
    "    \"\"\"\n",
    "    fid = FrechetInceptionDistance(normalize=True).to(device)\n",
    "    \n",
    "    # Convert to uint8 [0, 255] format expected by FID\n",
    "    real_images_uint8 = (real_images * 255).clamp(0, 255).to(torch.uint8)\n",
    "    generated_images_uint8 = (generated_images * 255).clamp(0, 255).to(torch.uint8)\n",
    "    \n",
    "    # Update FID with real and fake images\n",
    "    fid.update(real_images_uint8, real=True)\n",
    "    fid.update(generated_images_uint8, real=False)\n",
    "    \n",
    "    # Compute FID score\n",
    "    fid_score = fid.compute()\n",
    "    \n",
    "    return fid_score.item()\n",
    "\n",
    "\n",
    "def compute_clip_score(images, prompts, device):\n",
    "    \"\"\"\n",
    "    Compute CLIP score between images and their text prompts.\n",
    "    \n",
    "    Args:\n",
    "        images: Tensor of images (B, C, H, W) in range [0, 1]\n",
    "        prompts: List of text prompts\n",
    "        device: Device to run computation on\n",
    "    \n",
    "    Returns:\n",
    "        CLIP score as float\n",
    "    \"\"\"\n",
    "    clip_score_fn = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch32\").to(device)\n",
    "    \n",
    "    # Convert to uint8 [0, 255] format\n",
    "    images_uint8 = (images * 255).clamp(0, 255).to(torch.uint8)\n",
    "    \n",
    "    # Compute CLIP score\n",
    "    score = clip_score_fn(images_uint8, prompts)\n",
    "    \n",
    "    return score.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378122a1",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    dataset_root: str\n",
    "    output_dir: str = \"./outputs/train12_flickr8k_text2img\"\n",
    "    batch_size: int = 16\n",
    "    num_epochs: int = 5000\n",
    "    lr: float = 1e-4\n",
    "    num_train_timesteps: int = 1000\n",
    "    image_size: int = 256\n",
    "    seed: int = 42\n",
    "    mixed_precision: bool = True\n",
    "    checkpoint_interval: int = 500  # Save checkpoint every N steps\n",
    "    # UNet size for Flickr8k (text-conditional)\n",
    "    unet_block_out_channels: tuple[int, ...] = (128, 256, 512, 512)\n",
    "    layers_per_block: int = 2\n",
    "    # Text conditioning\n",
    "    classifier_free_guidance_prob: float = 0.1  # 10% unconditional training for CFG\n",
    "\n",
    "\n",
    "# Configure training parameters - OPTIMIZED FOR FLICKR8K TEXT-TO-IMAGE\n",
    "config = TrainConfig(\n",
    "    dataset_root=\"../../datasets\",\n",
    "    output_dir=\"./outputs/train12_flickr8k_text2img\",\n",
    "    batch_size=16,  # Adjust based on GPU memory\n",
    "    num_epochs=5000,  # Train for 5000 epochs\n",
    "    lr=1e-4,\n",
    "    image_size=256,  # Full resolution for high-quality images\n",
    "    mixed_precision=True,  # Enable mixed precision to save memory\n",
    "    checkpoint_interval=500,  # Save checkpoint every 500 steps\n",
    "    classifier_free_guidance_prob=0.1,  # Enable classifier-free guidance\n",
    ")\n",
    "\n",
    "print(f\"Device: {get_device()}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Epochs: {config.num_epochs}\")\n",
    "print(f\"Learning rate: {config.lr}\")\n",
    "print(f\"Image size: {config.image_size}\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"Checkpoint interval: {config.checkpoint_interval} steps\")\n",
    "print(f\"Classifier-free guidance: {config.classifier_free_guidance_prob * 100}% unconditional\")\n",
    "print(f\"\\nDataset: Flickr8k (Text-Conditional Image Generation)\")\n",
    "print(f\"Total training steps: ~{8000 // config.batch_size * config.num_epochs:,} (approx)\")\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter (will create log directory)\n",
    "tb_log_dir = os.path.join(config.output_dir, \"tensorboard\")\n",
    "writer = SummaryWriter(log_dir=tb_log_dir)\n",
    "print(f\"TensorBoard logs will be written to: {tb_log_dir}\")\n",
    "\n",
    "# Initialize Accelerator for multi-GPU training\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=\"fp16\" if config.mixed_precision else \"no\",\n",
    "    log_with=\"tensorboard\",\n",
    "    project_dir=config.output_dir,\n",
    ")\n",
    "print(f\"\\nðŸš€ Accelerator initialized:\")\n",
    "print(f\"  Device: {accelerator.device}\")\n",
    "print(f\"  Number of processes: {accelerator.num_processes}\")\n",
    "print(f\"  Mixed precision: {accelerator.mixed_precision}\")\n",
    "print(f\"  Is main process: {accelerator.is_main_process}\")\n",
    "print(f\"  Process index: {accelerator.process_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db2cba",
   "metadata": {},
   "source": [
    "## 4. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(device: torch.device, config: TrainConfig):\n",
    "    # Pretrained VAE and CLIP text encoder/tokenizer\n",
    "    print(\"Loading pretrained VAE...\")\n",
    "    vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
    "    vae.requires_grad_(False)\n",
    "    vae.eval()\n",
    "    vae.to(device)\n",
    "\n",
    "    print(\"Loading CLIP text encoder...\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    text_encoder.requires_grad_(False)\n",
    "    text_encoder.eval()\n",
    "    text_encoder.to(device)\n",
    "\n",
    "    print(\"Creating UNet2DConditionModel...\")\n",
    "    # Conditional UNet operating in latent space (4 channels)\n",
    "    # Larger model for CIFAR10's more complex images\n",
    "    unet = UNet2DConditionModel(\n",
    "        sample_size=config.image_size // 8,  # 32 for 256x256\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        layers_per_block=config.layers_per_block,\n",
    "        block_out_channels=config.unet_block_out_channels,\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",\n",
    "            \"CrossAttnDownBlock2D\",\n",
    "            \"CrossAttnDownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\",\n",
    "            \"CrossAttnUpBlock2D\",\n",
    "            \"CrossAttnUpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "        ),\n",
    "        cross_attention_dim=512,  # CLIP ViT-B/32 hidden size\n",
    "    ).to(device)\n",
    "\n",
    "    num_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "    print(f\"UNet trainable parameters: {num_params:,}\")\n",
    "\n",
    "    return vae, tokenizer, text_encoder, unet\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "device = accelerator.device\n",
    "seed_everything(config.seed)\n",
    "vae, tokenizer, text_encoder, unet = create_models(device, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292e237",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23366d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(config: TrainConfig) -> DataLoader:\n",
    "    \"\"\"Load Flickr8k dataset with captions using HuggingFace datasets with LOCAL DOWNLOAD.\"\"\"\n",
    "    import random\n",
    "    \n",
    "    # IMPORTANT: Set HuggingFace cache directory BEFORE importing datasets\n",
    "    # Use /home/doshlom4/work for disk quota\n",
    "    cache_dir = \"/home/doshlom4/work/huggingface_cache\"\n",
    "    os.environ['HF_HOME'] = cache_dir\n",
    "    os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "    os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "    \n",
    "    # Now import datasets after setting environment variables\n",
    "    from datasets import load_dataset\n",
    "    from torch.utils.data import Dataset\n",
    "    \n",
    "    print(\"Loading Flickr8k dataset with LOCAL DOWNLOAD...\")\n",
    "    print(f\"ðŸ“ Cache directory: {cache_dir}\")\n",
    "    \n",
    "    # Create cache directory if it doesn't exist\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Try multiple Flickr8k dataset sources with LOCAL DOWNLOAD\n",
    "    dataset_candidates = [\n",
    "        (\"nlphuji/flickr30k\", \"test\"),\n",
    "        (\"nlphuji/flickr30k\", \"train\"),\n",
    "    ]\n",
    "    \n",
    "    ds = None\n",
    "    for dataset_name, split_name in dataset_candidates:\n",
    "        try:\n",
    "            print(f\"Trying to load: {dataset_name} (split={split_name})...\")\n",
    "            print(f\"This will download the dataset to {cache_dir}\")\n",
    "            \n",
    "            # Use streaming=False to download locally\n",
    "            ds = load_dataset(\n",
    "                dataset_name, \n",
    "                split=split_name, \n",
    "                cache_dir=cache_dir,\n",
    "                streaming=False  # Download dataset locally\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… Successfully loaded {dataset_name} locally\")\n",
    "            print(f\"âœ… Dataset has {len(ds)} examples\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Failed to load {dataset_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if ds is None:\n",
    "        raise RuntimeError(\n",
    "            \"Could not load any Flickr dataset. Please check your internet connection and disk space.\"\n",
    "        )\n",
    "    \n",
    "    tfms = build_transforms(config.image_size)\n",
    "    \n",
    "    # Dataset wrapper for Flickr8k with text captions\n",
    "    class Flickr8kTextImageDataset(Dataset):\n",
    "        def __init__(self, hf_dataset, transform, cfg_prob=0.1):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                hf_dataset: HuggingFace dataset (downloaded locally)\n",
    "                transform: Image transformations\n",
    "                cfg_prob: Probability of dropping text for classifier-free guidance\n",
    "            \"\"\"\n",
    "            self.dataset = hf_dataset\n",
    "            self.transform = transform\n",
    "            self.cfg_prob = cfg_prob\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            max_retries = 5\n",
    "            for retry in range(max_retries):\n",
    "                try:\n",
    "                    example = self.dataset[idx % len(self.dataset)]\n",
    "                    \n",
    "                    # Get image - handle different field names\n",
    "                    image = example.get('image', None) or example.get('img', None) or example.get('jpg', None)\n",
    "                    if image is None:\n",
    "                        idx = (idx + 1) % len(self.dataset)\n",
    "                        continue\n",
    "                        \n",
    "                    if image.mode != 'RGB':\n",
    "                        image = image.convert('RGB')\n",
    "                    image = self.transform(image)\n",
    "                    \n",
    "                    # Get caption - handle different dataset formats\n",
    "                    caption = \"\"\n",
    "                    \n",
    "                    # Try different caption field names\n",
    "                    if 'caption' in example:\n",
    "                        captions = example['caption']\n",
    "                        if isinstance(captions, list) and len(captions) > 0:\n",
    "                            caption = random.choice(captions)\n",
    "                        elif isinstance(captions, str):\n",
    "                            caption = captions\n",
    "                    elif 'captions' in example:\n",
    "                        captions = example['captions']\n",
    "                        if isinstance(captions, list) and len(captions) > 0:\n",
    "                            caption = random.choice(captions)\n",
    "                        elif isinstance(captions, str):\n",
    "                            caption = captions\n",
    "                    elif 'sentences' in example:\n",
    "                        sentences = example['sentences']\n",
    "                        if isinstance(sentences, list) and len(sentences) > 0:\n",
    "                            sent = random.choice(sentences)\n",
    "                            if isinstance(sent, dict):\n",
    "                                caption = sent.get('raw', '') or sent.get('text', '')\n",
    "                            else:\n",
    "                                caption = str(sent)\n",
    "                    elif 'text' in example:\n",
    "                        caption = example['text']\n",
    "                    \n",
    "                    # Fallback: use any string field\n",
    "                    if not caption:\n",
    "                        for key, value in example.items():\n",
    "                            if isinstance(value, str) and len(value) > 10:\n",
    "                                caption = value\n",
    "                                break\n",
    "                    \n",
    "                    # For classifier-free guidance: randomly drop text\n",
    "                    if random.random() < self.cfg_prob:\n",
    "                        caption = \"\"\n",
    "                    \n",
    "                    return {'image': image, 'caption': caption}\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading example {idx}: {e}, retrying...\")\n",
    "                    idx = (idx + 1) % len(self.dataset)\n",
    "                    if retry == max_retries - 1:\n",
    "                        raise\n",
    "    \n",
    "    dataset = Flickr8kTextImageDataset(ds, tfms, config.classifier_free_guidance_prob)\n",
    "    \n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    \n",
    "    # Use multiple workers for faster data loading\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,  # Drop incomplete batches\n",
    "    )\n",
    "\n",
    "\n",
    "# Create dataloader\n",
    "print(\"=\"*80)\n",
    "dataloader = make_dataloader(config)\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… Dataset loaded successfully\")\n",
    "print(f\"Dataset size: {len(dataloader.dataset)}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6ca12",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61af30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch with captions\n",
    "print(\"Loading sample batch...\")\n",
    "sample_batch = next(iter(dataloader))\n",
    "sample_images = sample_batch['image']\n",
    "sample_captions = sample_batch['caption']\n",
    "\n",
    "print(f\"Batch shape: {sample_images.shape}\")\n",
    "print(f\"\\nSample captions:\")\n",
    "for i, caption in enumerate(sample_captions[:4]):\n",
    "    print(f\"  {i+1}. {caption}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(18, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(sample_images):\n",
    "        img = sample_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img + 1) / 2  # Denormalize from [-1,1] to [0,1]\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        # Add caption as title (truncated)\n",
    "        if i < len(sample_captions):\n",
    "            caption = sample_captions[i][:50] + \"...\" if len(sample_captions[i]) > 50 else sample_captions[i]\n",
    "            ax.set_title(caption, fontsize=7, wrap=True)\n",
    "            \n",
    "plt.suptitle(\"Sample Flickr8k Images with Captions\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del sample_batch, sample_images, sample_captions\n",
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc55e4",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909cd5a",
   "metadata": {},
   "source": [
    "## 7. Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10bb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(output_dir: str):\n",
    "    \"\"\"Find the latest checkpoint in the output directory.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        return None\n",
    "    \n",
    "    # Look for checkpoint files\n",
    "    checkpoint_files = []\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.endswith('.pt') and (filename.startswith('unet_step_') or filename.startswith('unet_epoch_')):\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            checkpoint_files.append(filepath)\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    # Get the most recently modified checkpoint\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)\n",
    "    return latest_checkpoint\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path: str, unet, optimizer=None):\n",
    "    \"\"\"Load checkpoint and return metadata.\"\"\"\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # Handle different checkpoint formats\n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'unet' in checkpoint:\n",
    "            unet.load_state_dict(checkpoint['unet'])\n",
    "        else:\n",
    "            unet.load_state_dict(checkpoint)\n",
    "        \n",
    "        # Load optimizer state if available\n",
    "        if optimizer is not None and 'optimizer' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = {\n",
    "            'global_step': checkpoint.get('global_step', checkpoint.get('step', 0)),\n",
    "            'epoch': checkpoint.get('epoch', 0),\n",
    "            'batch_losses': checkpoint.get('batch_losses', []),\n",
    "            'epoch_losses': checkpoint.get('epoch_losses', []),\n",
    "        }\n",
    "    else:\n",
    "        unet.load_state_dict(checkpoint)\n",
    "        metadata = {'global_step': 0, 'epoch': 0, 'batch_losses': [], 'epoch_losses': []}\n",
    "    \n",
    "    print(f\"Resumed from step {metadata['global_step']}, epoch {metadata['epoch']}\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str, unet, optimizer, global_step: int, epoch: int, \n",
    "                   batch_losses: List[float], epoch_losses: List[float]):\n",
    "    \"\"\"Save checkpoint with complete metadata.\"\"\"\n",
    "    checkpoint = {\n",
    "        'unet': unet.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'global_step': global_step,\n",
    "        'epoch': epoch,\n",
    "        'batch_losses': batch_losses,\n",
    "        'epoch_losses': epoch_losses,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "\n",
    "# Check for existing checkpoints\n",
    "latest_checkpoint = find_latest_checkpoint(config.output_dir)\n",
    "if latest_checkpoint:\n",
    "    print(f\"Found existing checkpoint: {latest_checkpoint}\")\n",
    "    print(\"Training will resume from this checkpoint.\")\n",
    "else:\n",
    "    print(\"No existing checkpoints found. Starting training from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ae37e",
   "metadata": {},
   "source": [
    "### Checkpoint System Features\n",
    "\n",
    "The checkpoint system provides:\n",
    "\n",
    "1. **Automatic Resume**: Detects and loads the latest checkpoint automatically\n",
    "2. **Complete Metadata**: Each checkpoint stores:\n",
    "   - UNet model weights\n",
    "   - Optimizer state (for proper resume)\n",
    "   - Global step count\n",
    "   - Current epoch number\n",
    "   - Complete batch loss history\n",
    "   - Complete epoch loss history\n",
    "3. **Training Plots**: Loss plots saved with every checkpoint\n",
    "4. **Sample Images**: Generated samples (all 10 CIFAR10 classes) saved at each checkpoint to visualize training progress\n",
    "5. **Multiple Checkpoint Types**:\n",
    "   - Step checkpoints: `unet_step_2000.pt` (every 2000 steps)\n",
    "   - Epoch checkpoints: `unet_epoch_1.pt` (after each epoch)\n",
    "   - Final checkpoint: `unet_final.pt` (at completion)\n",
    "   - Sample images: `samples_step_2000.png`, `samples_epoch_1.png`, `samples_final.png`\n",
    "\n",
    "If training is interrupted, simply re-run the training cell and it will automatically resume from the latest checkpoint!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876740c9",
   "metadata": {},
   "source": [
    "### ðŸ“Š Metrics Tracking\n",
    "\n",
    "At every checkpoint, the following metrics are computed and logged to TensorBoard:\n",
    "\n",
    "1. **FID (FrÃ©chet Inception Distance)**:\n",
    "   - Measures the quality and diversity of generated images\n",
    "   - Lower is better (0 = perfect match to real distribution)\n",
    "   - Compares generated images to a batch of real images from the dataset\n",
    "   - Standard metric for evaluating generative models\n",
    "\n",
    "2. **CLIP Score**:\n",
    "   - Measures how well generated images match their text prompts\n",
    "   - Higher is better (max ~100)\n",
    "   - Uses pretrained CLIP model to compare image-text alignment\n",
    "   - Ensures text-conditioning is working properly\n",
    "\n",
    "Both metrics are automatically computed at:\n",
    "- Every checkpoint interval (e.g., every 500 steps)\n",
    "- End of each epoch\n",
    "- Final checkpoint\n",
    "\n",
    "View metrics in TensorBoard under the \"Metrics\" tab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_checkpoint_samples(\n",
    "    unet, vae, text_encoder, tokenizer, device, config, \n",
    "    writer=None, global_step=None, num_samples: int = 16, num_inference_steps: int = 50,\n",
    "    real_images_batch=None\n",
    "):\n",
    "    \"\"\"Generate sample images from text prompts during training checkpoints and log to TensorBoard.\n",
    "    Also computes FID and CLIP score metrics.\n",
    "    \n",
    "    Args:\n",
    "        real_images_batch: Optional batch of real images for FID computation\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'fid_score' and 'clip_score' if computed, else None\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating {num_samples} text-to-image samples at step {global_step}...\")\n",
    "    \n",
    "    # Predefined interesting prompts for visualization\n",
    "    prompts = [\n",
    "        \"A dog running on the beach\",\n",
    "        \"A child playing with a ball\",\n",
    "        \"A person riding a bike\",\n",
    "        \"A cat sitting on a chair\",\n",
    "        \"People walking in a park\",\n",
    "        \"A bird in a tree\",\n",
    "        \"A man with a hat\",\n",
    "        \"A woman smiling\",\n",
    "        \"Children playing outside\",\n",
    "        \"A dog jumping in the air\",\n",
    "        \"A person holding an umbrella\",\n",
    "        \"A group of people standing\",\n",
    "        \"A child with a toy\",\n",
    "        \"A dog with a stick\",\n",
    "        \"A person near water\",\n",
    "        \"Two people talking\",\n",
    "    ]\n",
    "    \n",
    "    # Create scheduler for sampling\n",
    "    scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    # Set models to eval mode\n",
    "    unet.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 8, figsize=(20, 6))\n",
    "    \n",
    "    # Collect generated images and prompts for metrics\n",
    "    generated_images_list = []\n",
    "    used_prompts = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i >= num_samples:\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            # Get prompt and encode\n",
    "            prompt = prompts[i % len(prompts)]\n",
    "            used_prompts.append(prompt)\n",
    "            text_embeddings = encode_text(prompt, tokenizer, text_encoder, device)\n",
    "            \n",
    "            # Set seed for reproducibility\n",
    "            torch.manual_seed(42 + i)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(42 + i)\n",
    "            \n",
    "            # Init random latents\n",
    "            latents = torch.randn((1, 4, config.image_size // 8, config.image_size // 8), device=device)\n",
    "            \n",
    "            # Denoising loop with text conditioning\n",
    "            for t in scheduler.timesteps:\n",
    "                latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "                \n",
    "                # Predict noise with text conditioning\n",
    "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "                \n",
    "                # Step\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "            \n",
    "            # Decode latents to image\n",
    "            latents = latents / 0.18215\n",
    "            image = vae.decode(latents).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.detach().cpu()\n",
    "            \n",
    "            # Store for metrics\n",
    "            generated_images_list.append(image)\n",
    "            \n",
    "            # Display image with prompt as title\n",
    "            img = image[0].permute(1, 2, 0).numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(prompt[:40], fontsize=7)\n",
    "    \n",
    "    plt.suptitle(f\"Text-to-Image Samples at Step {global_step}\", fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Log figure to TensorBoard instead of saving to disk\n",
    "    if writer is not None and global_step is not None:\n",
    "        writer.add_figure(f\"samples/step_{global_step}\", fig, global_step)\n",
    "        writer.flush()\n",
    "    else:\n",
    "        # If writer not provided, fallback to saving to disk (rare)\n",
    "        pass\n",
    "    plt.close(fig)\n",
    "    print(f\"Sample images logged to TensorBoard at step {global_step}\")\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {}\n",
    "    try:\n",
    "        # Stack generated images\n",
    "        generated_images = torch.cat(generated_images_list, dim=0)  # (num_samples, C, H, W)\n",
    "        \n",
    "        # Compute CLIP Score\n",
    "        print(f\"Computing CLIP Score...\")\n",
    "        clip_score = compute_clip_score(generated_images, used_prompts, device)\n",
    "        metrics['clip_score'] = clip_score\n",
    "        print(f\"CLIP Score: {clip_score:.4f}\")\n",
    "        \n",
    "        # Compute FID if real images are provided\n",
    "        if real_images_batch is not None:\n",
    "            print(f\"Computing FID Score...\")\n",
    "            # Ensure real images are in [0, 1] range\n",
    "            real_images = real_images_batch\n",
    "            if real_images.min() < 0:\n",
    "                real_images = (real_images + 1) / 2  # Convert from [-1, 1] to [0, 1]\n",
    "            \n",
    "            # Resize real images to match generated images if needed\n",
    "            if real_images.shape[-1] != config.image_size:\n",
    "                real_images = torch.nn.functional.interpolate(\n",
    "                    real_images, size=(config.image_size, config.image_size), mode='bilinear'\n",
    "                )\n",
    "            \n",
    "            # Take only as many real images as generated\n",
    "            real_images = real_images[:num_samples].cpu()\n",
    "            \n",
    "            fid_score = compute_fid_score(real_images, generated_images, device)\n",
    "            metrics['fid_score'] = fid_score\n",
    "            print(f\"FID Score: {fid_score:.4f}\")\n",
    "        \n",
    "        # Log metrics to TensorBoard\n",
    "        if writer is not None and global_step is not None:\n",
    "            if 'clip_score' in metrics:\n",
    "                writer.add_scalar(\"metrics/clip_score\", metrics['clip_score'], global_step)\n",
    "            if 'fid_score' in metrics:\n",
    "                writer.add_scalar(\"metrics/fid_score\", metrics['fid_score'], global_step)\n",
    "            writer.flush()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compute metrics: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Set UNet back to train mode\n",
    "    unet.train()\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ff223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: TrainConfig, vae, tokenizer, text_encoder, unet, dataloader, device, accelerator, resume_from_checkpoint: str = None):\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "    # Noise scheduler for training\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=config.num_train_timesteps, beta_schedule=\"squaredcos_cap_v2\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(unet.parameters(), lr=config.lr)\n",
    "    \n",
    "    # Prepare models, optimizer, and dataloader with Accelerator\n",
    "    # Note: VAE and text_encoder are frozen, so we don't prepare them\n",
    "    unet, optimizer, dataloader = accelerator.prepare(unet, optimizer, dataloader)\n",
    "\n",
    "    # Loss tracking and training state\n",
    "    batch_losses = []\n",
    "    epoch_losses = []\n",
    "    global_step = 0\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Resume from checkpoint if available\n",
    "    if resume_from_checkpoint:\n",
    "        metadata = load_checkpoint(resume_from_checkpoint, accelerator.unwrap_model(unet), optimizer)\n",
    "        global_step = metadata['global_step']\n",
    "        start_epoch = metadata['epoch']\n",
    "        batch_losses = metadata['batch_losses']\n",
    "        epoch_losses = metadata['epoch_losses']\n",
    "        if accelerator.is_main_process:\n",
    "            print(f\"Resuming training from step {global_step}, epoch {start_epoch}\")\n",
    "\n",
    "    unet.train()\n",
    "    \n",
    "    # Store a batch of real images for FID computation during checkpoints\n",
    "    real_images_for_metrics = None\n",
    "    \n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        epoch_loss_sum = 0.0\n",
    "        epoch_batch_count = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\", disable=not accelerator.is_local_main_process)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Get images and captions from batch\n",
    "            images = batch['image']\n",
    "            captions = batch['caption']\n",
    "            \n",
    "            # Store first batch of real images for FID metrics (only on main process, only once)\n",
    "            if real_images_for_metrics is None and accelerator.is_main_process:\n",
    "                real_images_for_metrics = images.clone().cpu()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Encode images to latents using frozen VAE\n",
    "                latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "                \n",
    "                # Encode text prompts using frozen CLIP\n",
    "                text_embeddings = []\n",
    "                for caption in captions:\n",
    "                    emb = encode_text(caption, tokenizer, text_encoder, accelerator.device)\n",
    "                    text_embeddings.append(emb)\n",
    "                text_embeddings = torch.cat(text_embeddings, dim=0)\n",
    "\n",
    "            # Sample noise and timestep; add noise\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=accelerator.device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Predict noise with text conditioning\n",
    "            # Accelerator handles mixed precision automatically\n",
    "            with accelerator.autocast():\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=text_embeddings).sample\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            # Backward pass with Accelerator\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Record loss\n",
    "            loss_value = loss.item()\n",
    "            batch_losses.append(loss_value)\n",
    "            epoch_loss_sum += loss_value\n",
    "            epoch_batch_count += 1\n",
    "\n",
    "            global_step += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{loss_value:.4f}\", \"step\": global_step})\n",
    "\n",
    "            # Clear cache every 50 batches to prevent fragmentation\n",
    "            if batch_idx % 50 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Save periodic checkpoints (only on main process)\n",
    "            if global_step % config.checkpoint_interval == 0 and accelerator.is_main_process:\n",
    "                ckpt_path = os.path.join(config.output_dir, f\"unet_step_{global_step}.pt\")\n",
    "                save_checkpoint(ckpt_path, accelerator.unwrap_model(unet), optimizer, global_step, epoch, batch_losses, epoch_losses)\n",
    "                print(f\"\\nCheckpoint saved: {ckpt_path}\")\n",
    "                \n",
    "                # Log training plots to TensorBoard\n",
    "                try:\n",
    "                    save_loss_plot(batch_losses, epoch_losses, writer=writer, global_step=global_step)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to log loss plot to TensorBoard: {e}\")\n",
    "                \n",
    "                # Generate and log sample images to TensorBoard with metrics\n",
    "                try:\n",
    "                    # Use stored real images for FID computation\n",
    "                    generate_checkpoint_samples(\n",
    "                        accelerator.unwrap_model(unet), vae, text_encoder, tokenizer, accelerator.device, config,\n",
    "                        writer=writer, global_step=global_step, real_images_batch=real_images_for_metrics\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to generate/log sample images: {e}\")\n",
    "                \n",
    "                # Clear cache after sampling\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        # Record epoch average loss\n",
    "        avg_epoch_loss = epoch_loss_sum / epoch_batch_count if epoch_batch_count > 0 else 0.0\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        if accelerator.is_main_process:\n",
    "            print(f\"Epoch {epoch+1}/{config.num_epochs} - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # Save per-epoch checkpoint (only on main process)\n",
    "        if accelerator.is_main_process:\n",
    "            ckpt_path = os.path.join(config.output_dir, f\"unet_epoch_{epoch+1}.pt\")\n",
    "            save_checkpoint(ckpt_path, accelerator.unwrap_model(unet), optimizer, global_step, epoch + 1, batch_losses, epoch_losses)\n",
    "            \n",
    "            # Log training plots to TensorBoard at epoch end\n",
    "            try:\n",
    "                save_loss_plot(batch_losses, epoch_losses, writer=writer, global_step=global_step)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to log loss plot at epoch end: {e}\")\n",
    "            \n",
    "            # Generate and log sample images at end of epoch with metrics\n",
    "            try:\n",
    "                generate_checkpoint_samples(\n",
    "                    accelerator.unwrap_model(unet), vae, text_encoder, tokenizer, accelerator.device, config,\n",
    "                    writer=writer, global_step=global_step, real_images_batch=real_images_for_metrics\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to generate/log sample images at epoch end: {e}\")\n",
    "        \n",
    "        # Clear cache after each epoch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Save final (only on main process)\n",
    "    if accelerator.is_main_process:\n",
    "        final_path = os.path.join(config.output_dir, \"unet_final.pt\")\n",
    "        save_checkpoint(final_path, accelerator.unwrap_model(unet), optimizer, global_step, config.num_epochs, batch_losses, epoch_losses)\n",
    "        print(f\"\\nFinal model saved: {final_path}\")\n",
    "        \n",
    "        # Log final training plots to TensorBoard\n",
    "        try:\n",
    "            save_loss_plot(batch_losses, epoch_losses, writer=writer, global_step=global_step)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to log final loss plot: {e}\")\n",
    "        \n",
    "        # Generate final sample images and log to TensorBoard with metrics\n",
    "        try:\n",
    "            generate_checkpoint_samples(\n",
    "                accelerator.unwrap_model(unet), vae, text_encoder, tokenizer, accelerator.device, config,\n",
    "                writer=writer, global_step=global_step, real_images_batch=real_images_for_metrics\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate/log final sample images: {e}\")\n",
    "\n",
    "    return batch_losses, epoch_losses\n",
    "\n",
    "\n",
    "def save_loss_plot(batch_losses: List[float], epoch_losses: List[float], writer=None, global_step: int = None):\n",
    "    \"\"\"Helper function to create loss plot figure and log to TensorBoard if writer is provided.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Per-batch loss\n",
    "    if len(batch_losses) > 0:\n",
    "        axes[0].plot(batch_losses, linewidth=0.8, alpha=0.7)\n",
    "        axes[0].set_xlabel(\"Batch\")\n",
    "        axes[0].set_ylabel(\"Loss\")\n",
    "        axes[0].set_title(\"Training Loss per Batch\")\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Per-epoch loss\n",
    "    if len(epoch_losses) > 0:\n",
    "        axes[1].plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', linewidth=2)\n",
    "        axes[1].set_xlabel(\"Epoch\")\n",
    "        axes[1].set_ylabel(\"Average Loss\")\n",
    "        axes[1].set_title(\"Training Loss per Epoch\")\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Log to TensorBoard if writer provided\n",
    "    if writer is not None and global_step is not None:\n",
    "        try:\n",
    "            writer.add_figure(\"training/loss\", fig, global_step)\n",
    "            # Also log scalar for epoch loss\n",
    "            if len(epoch_losses) > 0:\n",
    "                writer.add_scalar(\"training/epoch_loss\", epoch_losses[-1], global_step)\n",
    "            writer.flush()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to write loss figure to TensorBoard: {e}\")\n",
    "    else:\n",
    "        # If no writer is provided, show the plot inline\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)  # Close the figure to free memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43dd0c",
   "metadata": {},
   "source": [
    "## 9. Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1713a",
   "metadata": {},
   "source": [
    "## 8.5. Memory Optimization (Clear GPU Cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74842b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache before training to maximize available memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    # Set memory allocation configuration for better fragmentation handling\n",
    "    import os\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    # Print current memory status\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(\"Memory cache cleared and optimized for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoints and resume if available\n",
    "latest_checkpoint = find_latest_checkpoint(config.output_dir)\n",
    "\n",
    "# Train the model (will resume from checkpoint if found)\n",
    "batch_losses, epoch_losses = train(\n",
    "    config, vae, tokenizer, text_encoder, unet, dataloader, device, accelerator,\n",
    "    resume_from_checkpoint=latest_checkpoint\n",
    ")\n",
    "\n",
    "# Close TensorBoard writer to flush remaining logs\n",
    "try:\n",
    "    writer.close()\n",
    "    print(\"TensorBoard writer closed. Logs available at:\", tb_log_dir)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to close TensorBoard writer cleanly: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7bd06f",
   "metadata": {},
   "source": [
    "## 10. Visualize Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d54c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(batch_losses: List[float], epoch_losses: List[float], writer=None, global_step: int = None):\n",
    "    \"\"\"Generate and log loss plots for per-batch and per-epoch losses (TensorBoard if writer provided).\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Per-batch loss\n",
    "    axes[0].plot(batch_losses, linewidth=0.8, alpha=0.7)\n",
    "    axes[0].set_xlabel(\"Batch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Training Loss per Batch\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Per-epoch loss\n",
    "    if len(epoch_losses) > 0:\n",
    "        axes[1].plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', linewidth=2)\n",
    "        axes[1].set_xlabel(\"Epoch\")\n",
    "        axes[1].set_ylabel(\"Average Loss\")\n",
    "        axes[1].set_title(\"Training Loss per Epoch\")\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if writer is not None and global_step is not None:\n",
    "        writer.add_figure(\"training/loss_overview\", fig, global_step)\n",
    "        writer.flush()\n",
    "        print(\"Loss plots logged to TensorBoard.\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# Plot the losses to TensorBoard\n",
    "plot_losses(batch_losses, epoch_losses, writer=writer, global_step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db4313",
   "metadata": {},
   "source": [
    "## 11. Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d34a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(\n",
    "    prompt: str,\n",
    "    num_inference_steps: int = 50,\n",
    "    guidance_scale: float = 7.5,\n",
    "    seed: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"Generate an image from a text prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text description of the image to generate\n",
    "        num_inference_steps: Number of denoising steps\n",
    "        guidance_scale: Classifier-free guidance scale (higher = more prompt adherence)\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    scheduler = DDPMScheduler(beta_schedule=\"squaredcos_cap_v2\", num_train_timesteps=1000)\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    # Encode text prompt\n",
    "    text_embeddings = encode_text(prompt, tokenizer, text_encoder, accelerator.device)\n",
    "    \n",
    "    # For classifier-free guidance, also encode empty prompt\n",
    "    if guidance_scale > 1.0:\n",
    "        uncond_embeddings = encode_text(\"\", tokenizer, text_encoder, accelerator.device)\n",
    "        # Concatenate for batch processing\n",
    "        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "    # Init random latents in latent space\n",
    "    latents = torch.randn((1, 4, config.image_size // 8, config.image_size // 8), device=accelerator.device)\n",
    "\n",
    "    unet.eval()\n",
    "    for t in tqdm(scheduler.timesteps, desc=f\"Generating '{prompt}'\"):\n",
    "        # Prepare latent input\n",
    "        latent_model_input = latents\n",
    "        if guidance_scale > 1.0:\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict noise with text conditioning\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # Perform classifier-free guidance\n",
    "        if guidance_scale > 1.0:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # Step\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    # Decode latents to image\n",
    "    latents = latents / 0.18215\n",
    "    image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu()\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f26d1",
   "metadata": {},
   "source": [
    "## 12. Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images from various text prompts\n",
    "test_prompts = [\n",
    "    \"A dog running on the beach\",\n",
    "    \"A child playing with a ball\",\n",
    "    \"A person riding a bike\",\n",
    "    \"A cat sitting on a chair\",\n",
    "    \"People walking in a park\",\n",
    "    \"A bird in a tree\",\n",
    "    \"A man with a hat\",\n",
    "    \"A woman smiling\",\n",
    "    \"Children playing outside\",\n",
    "    \"A dog jumping in the air\",\n",
    "    \"A person holding an umbrella\",\n",
    "    \"A group of people standing\",\n",
    "    \"A child with a toy\",\n",
    "    \"A dog with a stick\",\n",
    "    \"A person near water\",\n",
    "    \"Two people talking\",\n",
    "]\n",
    "\n",
    "num_inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(18, 18))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(test_prompts):\n",
    "        prompt = test_prompts[i]\n",
    "        print(f\"Generating {i+1}/{len(test_prompts)}: '{prompt}'\")\n",
    "        generated_image = sample(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            seed=42 + i,\n",
    "        )\n",
    "        \n",
    "        # Display image\n",
    "        img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(prompt, fontsize=9, wrap=True)\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Text-to-Image Samples from Flickr8k Model\", fontsize=16, y=0.995)\n",
    "plt.tight_layout()\n",
    "# Log figure to TensorBoard instead of saving to disk\n",
    "try:\n",
    "    writer.add_figure(\"samples/text_to_image_samples\", fig, 0)\n",
    "    writer.flush()\n",
    "    print(\"Text-to-image samples logged to TensorBoard.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to log samples to TensorBoard: {e}\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7e582",
   "metadata": {},
   "source": [
    "## 13. Test Different Guidance Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different guidance scales for a single prompt\n",
    "test_prompt = \"A happy dog playing with a ball in a park\"\n",
    "guidance_scales = [0, 3, 5, 7.5, 10, 15]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(guidance_scales), figsize=(20, 4))\n",
    "for ax, gs in zip(axes, guidance_scales):\n",
    "    print(f\"Generating with guidance scale {gs}\")\n",
    "    generated_image = sample(\n",
    "        prompt=test_prompt,\n",
    "        guidance_scale=gs,\n",
    "        num_inference_steps=50,\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Guidance Scale: {gs}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Log guidance scale comparison to TensorBoard\n",
    "try:\n",
    "    writer.add_figure(\"samples/guidance_scale_comparison\", fig, 0)\n",
    "    writer.flush()\n",
    "    print(\"Guidance scale comparison logged to TensorBoard.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to log guidance comparison: {e}\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7cd20",
   "metadata": {},
   "source": [
    "## 14. Save Individual Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and log a single high-quality sample from a custom prompt\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# CUSTOMIZE YOUR PROMPT HERE\n",
    "custom_prompt = \"A happy dog playing with a ball in a sunny park\"\n",
    "\n",
    "print(f\"Generating image for prompt: '{custom_prompt}'\")\n",
    "\n",
    "generated_image = sample(\n",
    "    prompt=custom_prompt,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Log to TensorBoard instead of saving to file\n",
    "try:\n",
    "    # writer.add_image expects CHW and values 0..1; generated_image[0] fits that\n",
    "    writer.add_image(\"samples/custom\", generated_image[0], 0)\n",
    "    writer.flush()\n",
    "    print(\"Custom sample logged to TensorBoard.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to log custom sample: {e}\")\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(8, 8))\n",
    "img = generated_image[0].permute(1, 2, 0).numpy()\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Generated: '{custom_prompt}'\", fontsize=12, wrap=True)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2b765",
   "metadata": {},
   "source": [
    "## ðŸ“š How to Use This Notebook\n",
    "\n",
    "### Single GPU Training (Default)\n",
    "Just run all cells in order. The notebook will automatically use the available GPU.\n",
    "\n",
    "### Multi-GPU Training (Recommended for faster training)\n",
    "\n",
    "**Method 1: Using accelerate launch (Best for HPC/Slurm)**\n",
    "\n",
    "Create a Python script from this notebook or use `jupyter nbconvert`:\n",
    "```bash\n",
    "# Convert notebook to script\n",
    "jupyter nbconvert --to script train13_flickr8_better.ipynb\n",
    "\n",
    "# Launch with 2 GPUs\n",
    "accelerate launch --multi_gpu --num_processes=2 train13_flickr8_better.py\n",
    "\n",
    "# Launch with 4 GPUs  \n",
    "accelerate launch --multi_gpu --num_processes=4 train13_flickr8_better.py\n",
    "```\n",
    "\n",
    "**Method 2: Configure accelerate once**\n",
    "```bash\n",
    "accelerate config\n",
    "```\n",
    "Answer the prompts:\n",
    "- Compute environment: This machine\n",
    "- Distributed training: multi-GPU\n",
    "- Number of processes: [number of GPUs you want to use]\n",
    "- Use FP16: Yes\n",
    "\n",
    "Then run:\n",
    "```bash\n",
    "accelerate launch train13_flickr8_better.py\n",
    "```\n",
    "\n",
    "### View TensorBoard Logs\n",
    "\n",
    "**Local machine:**\n",
    "```bash\n",
    "tensorboard --logdir ./outputs/train12_flickr8k_text2img/tensorboard --port 6006\n",
    "```\n",
    "Then open http://localhost:6006\n",
    "\n",
    "**HPC with port forwarding:**\n",
    "```bash\n",
    "# On HPC login node\n",
    "tensorboard --logdir ./outputs/train12_flickr8k_text2img/tensorboard --port 6006\n",
    "\n",
    "# On your local machine\n",
    "ssh -L 6006:localhost:6006 your_username@hpc_address\n",
    "```\n",
    "Then open http://localhost:6006\n",
    "\n",
    "### Key Features:\n",
    "- âœ… **Multi-GPU Training**: Automatic data parallelism with Accelerate\n",
    "- âœ… **TensorBoard Logging**: All plots and samples logged to TensorBoard\n",
    "- âœ… **Metrics Tracking**: FID and CLIP Score computed at every checkpoint\n",
    "- âœ… **Automatic Checkpointing**: Resume training from last checkpoint\n",
    "- âœ… **Mixed Precision (FP16)**: Faster training with less memory\n",
    "- âœ… **Text-to-Image Generation**: Generate images from text prompts\n",
    "- âœ… **Classifier-free Guidance**: Control generation quality with guidance scale\n",
    "\n",
    "### Metrics Monitoring:\n",
    "In TensorBoard, you can view:\n",
    "- **Training Loss**: Per-batch and per-epoch loss curves\n",
    "- **FID Score**: Image quality metric (lower is better)\n",
    "- **CLIP Score**: Text-image alignment metric (higher is better)\n",
    "- **Sample Images**: Generated images at each checkpoint\n",
    "\n",
    "### Performance Tips:\n",
    "- **2 GPUs**: ~2x speedup, effective batch size = 32 (16 per GPU)\n",
    "- **4 GPUs**: ~4x speedup, effective batch size = 64 (16 per GPU)\n",
    "- **Adjust batch_size**: If OOM errors occur, reduce batch_size to 8 or 4\n",
    "- **Monitor with TensorBoard**: Watch training progress in real-time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
