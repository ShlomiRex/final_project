{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34863069",
   "metadata": {},
   "source": [
    "# Phase 1: Pretrained VQ-VAE Experimentation\n",
    "\n",
    "This notebook validates the LatentGPT pipeline using pretrained VQGAN from HuggingFace.\n",
    "\n",
    "**Goals:**\n",
    "1. Test VQ-VAE encoding/decoding\n",
    "2. Initialize LatentGPT transformer\n",
    "3. Test text encoding with CLIP\n",
    "4. Experiment with generation\n",
    "5. Track with MLflow\n",
    "\n",
    "**MLflow Experiment:** `latent-gpt-pretrained-vqvae`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee92b1",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d00a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path(\"/home/doshlom4/work/final_project\")\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d03f8",
   "metadata": {},
   "source": [
    "## 2. MLflow Configuration\n",
    "\n",
    "Set up MLflow tracking for experiment organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# MLflow configuration\n",
    "MLFLOW_TRACKING_URI = os.environ.get(\"MLFLOW_TRACKING_URI\", \"http://127.0.0.1:5000\")\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "# Define our experiments\n",
    "EXPERIMENTS = {\n",
    "    \"vqvae_training\": \"vqvae-training\",\n",
    "    \"pretrained_vqvae\": \"latent-gpt-pretrained-vqvae\",\n",
    "    \"custom_vqvae\": \"latent-gpt-custom-vqvae\",\n",
    "}\n",
    "\n",
    "print(f\"MLflow Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "print(f\"\\nConfigured Experiments:\")\n",
    "for key, name in EXPERIMENTS.items():\n",
    "    print(f\"  - {key}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb62a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current experiment for Phase 1\n",
    "CURRENT_EXPERIMENT = EXPERIMENTS[\"pretrained_vqvae\"]\n",
    "mlflow.set_experiment(CURRENT_EXPERIMENT)\n",
    "\n",
    "print(f\"Active experiment: {CURRENT_EXPERIMENT}\")\n",
    "print(\"\\nNote: Make sure MLflow server is running:\")\n",
    "print(\"  sbatch slurm/mlflow_server.sh\")\n",
    "print(\"  # or locally: mlflow server --port 5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe72f4",
   "metadata": {},
   "source": [
    "## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82489a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config import Config\n",
    "\n",
    "# Load configuration for Phase 1 experiments\n",
    "config = Config.from_yaml(PROJECT_ROOT / \"configs\" / \"vqvae_pretrained.yaml\")\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  VQ-VAE: {config.vqvae.checkpoint}\")\n",
    "print(f\"  Codebook size: {config.vqvae.codebook_size}\")\n",
    "print(f\"  Downsample factor: {config.vqvae.downsample_factor}\")\n",
    "print(f\"  Transformer layers: {config.transformer.num_layers}\")\n",
    "print(f\"  Transformer hidden: {config.transformer.hidden_size}\")\n",
    "print(f\"  Image size: {config.data.image_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d3aac",
   "metadata": {},
   "source": [
    "## 4. Load Models\n",
    "\n",
    "Load the pretrained VQ-VAE and CLIP text encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.vqvae import VQVAEWrapper\n",
    "from src.models.clip_encoder import CLIPEncoder\n",
    "\n",
    "# Load pretrained VQ-VAE\n",
    "print(\"Loading VQ-VAE...\")\n",
    "vqvae = VQVAEWrapper.from_pretrained(\n",
    "    checkpoint=config.vqvae.checkpoint,\n",
    "    device=device\n",
    ")\n",
    "print(f\"VQ-VAE loaded: vocab_size={vqvae.vocab_size}\")\n",
    "\n",
    "# Load CLIP encoder\n",
    "print(\"\\nLoading CLIP encoder...\")\n",
    "clip_encoder = CLIPEncoder(\n",
    "    model_name=config.text_encoder.model_name,\n",
    "    device=device\n",
    ")\n",
    "print(f\"CLIP hidden size: {clip_encoder.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a09408",
   "metadata": {},
   "source": [
    "## 5. Test VQ-VAE Encoding/Decoding\n",
    "\n",
    "Verify that the VQ-VAE can encode and decode images correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb7a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random test image\n",
    "test_image = torch.randn(1, 3, config.data.image_size, config.data.image_size).to(device)\n",
    "test_image = (test_image - test_image.min()) / (test_image.max() - test_image.min())  # Normalize to [0, 1]\n",
    "\n",
    "# Encode to tokens\n",
    "tokens = vqvae.encode(test_image)\n",
    "print(f\"Input image shape: {test_image.shape}\")\n",
    "print(f\"Encoded tokens shape: {tokens.shape}\")\n",
    "print(f\"Token range: [{tokens.min().item()}, {tokens.max().item()}]\")\n",
    "\n",
    "# Decode back to image\n",
    "reconstructed = vqvae.decode(tokens)\n",
    "print(f\"Reconstructed image shape: {reconstructed.shape}\")\n",
    "\n",
    "# Calculate reconstruction error\n",
    "mse = torch.nn.functional.mse_loss(reconstructed, test_image)\n",
    "print(f\"Reconstruction MSE: {mse.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original vs reconstructed\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(test_image[0].permute(1, 2, 0).cpu().numpy())\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(reconstructed[0].permute(1, 2, 0).detach().cpu().clamp(0, 1).numpy())\n",
    "axes[1].set_title(\"Reconstructed\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443637d2",
   "metadata": {},
   "source": [
    "## 6. Test CLIP Text Encoding\n",
    "\n",
    "Verify that CLIP can encode text prompts correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0165809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text encoding\n",
    "test_prompts = [\n",
    "    \"A beautiful sunset over the ocean\",\n",
    "    \"A cat sitting on a couch\",\n",
    "    \"A mountain landscape with snow\",\n",
    "]\n",
    "\n",
    "# Encode prompts\n",
    "text_embeddings = clip_encoder.encode_text(test_prompts)\n",
    "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"Expected: [batch_size, seq_len, hidden_size] = [{len(test_prompts)}, *, {clip_encoder.hidden_size}]\")\n",
    "\n",
    "# Test null embedding for CFG\n",
    "null_embedding = clip_encoder.get_null_embedding(batch_size=2)\n",
    "print(f\"\\nNull embedding shape: {null_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d7a997",
   "metadata": {},
   "source": [
    "## 7. Initialize LatentGPT Transformer\n",
    "\n",
    "Create a small transformer for testing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d274b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.latent_gpt import LatentGPT\n",
    "\n",
    "# For notebook experimentation, use a smaller model\n",
    "small_config = config.transformer\n",
    "small_config.num_layers = 4  # Reduce layers for testing\n",
    "small_config.hidden_size = 256  # Reduce hidden size for testing\n",
    "\n",
    "# Create model\n",
    "model = LatentGPT(\n",
    "    vocab_size=vqvae.vocab_size,\n",
    "    max_seq_len=config.transformer.max_seq_len,\n",
    "    hidden_size=small_config.hidden_size,\n",
    "    num_layers=small_config.num_layers,\n",
    "    num_heads=4,  # Reduce heads for testing\n",
    "    context_dim=clip_encoder.hidden_size,\n",
    "    dropout=config.transformer.dropout,\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72cf4a",
   "metadata": {},
   "source": [
    "## 8. Test Forward Pass\n",
    "\n",
    "Verify the model can process tokens and text conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ffb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass with random tokens\n",
    "batch_size = 2\n",
    "seq_len = 64  # Small sequence for testing\n",
    "\n",
    "# Random token sequence\n",
    "dummy_tokens = torch.randint(0, vqvae.vocab_size, (batch_size, seq_len)).to(device)\n",
    "\n",
    "# Get text conditioning\n",
    "dummy_context = clip_encoder.encode_text([\"A test prompt\"] * batch_size)\n",
    "\n",
    "# Forward pass\n",
    "logits, loss = model(dummy_tokens, context=dummy_context)\n",
    "\n",
    "print(f\"Input tokens shape: {dummy_tokens.shape}\")\n",
    "print(f\"Context shape: {dummy_context.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"Training loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361cb3b6",
   "metadata": {},
   "source": [
    "## 9. Load Flickr30k Dataset\n",
    "\n",
    "Load a few samples from Flickr30k for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebd47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.flickr30k import Flickr30kDataset, create_dataloader\n",
    "\n",
    "# Dataset cache path from existing project\n",
    "cache_dir = PROJECT_ROOT / \"stable_diffusion\" / \"notebooks-old\" / \"dataset_cache\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = Flickr30kDataset(\n",
    "    split=\"test\",\n",
    "    image_size=config.data.image_size,\n",
    "    cache_dir=str(cache_dir),\n",
    "    cfg_dropout=0.0,  # No dropout for testing\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Get a sample\n",
    "sample = dataset[0]\n",
    "print(f\"Image shape: {sample['image'].shape}\")\n",
    "print(f\"Caption: {sample['caption'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "dataloader = create_dataloader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = batch[\"image\"][i].permute(1, 2, 0).numpy()\n",
    "    img = (img - img.min()) / (img.max() - img.min())  # Normalize for display\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(batch[\"caption\"][i][:50] + \"...\", fontsize=10)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f3893",
   "metadata": {},
   "source": [
    "## 10. Test Generation Pipeline\n",
    "\n",
    "Test the full generation pipeline with the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sequence length for target resolution\n",
    "downsample_factor = config.vqvae.downsample_factor\n",
    "latent_size = config.data.image_size // downsample_factor\n",
    "target_seq_len = latent_size * latent_size\n",
    "\n",
    "print(f\"Image size: {config.data.image_size}x{config.data.image_size}\")\n",
    "print(f\"Latent size: {latent_size}x{latent_size}\")\n",
    "print(f\"Target sequence length: {target_seq_len}\")\n",
    "\n",
    "# Generate tokens (note: model is untrained, so output will be random)\n",
    "prompt = \"A beautiful sunset over the ocean\"\n",
    "context = clip_encoder.encode_text([prompt])\n",
    "\n",
    "print(f\"\\nGenerating {target_seq_len} tokens for prompt: '{prompt}'\")\n",
    "print(\"Note: Model is untrained, output will be random noise\")\n",
    "\n",
    "generated_tokens = model.generate(\n",
    "    context=context,\n",
    "    max_new_tokens=min(target_seq_len, 64),  # Limit for testing speed\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "print(f\"Generated tokens shape: {generated_tokens.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c7a20",
   "metadata": {},
   "source": [
    "## 11. MLflow Logging Example\n",
    "\n",
    "Example of how to log a training run to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e85463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from src.utils.logging import setup_mlflow, log_config\n",
    "\n",
    "# Start a test run\n",
    "run_name = f\"notebook_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    # Set tags for this run\n",
    "    mlflow.set_tags({\n",
    "        \"resolution\": str(config.data.image_size),\n",
    "        \"conditioning\": \"text_conditional\",\n",
    "        \"vqvae_source\": \"pretrained_hf\",\n",
    "        \"phase\": \"1\",\n",
    "        \"experiment_type\": \"notebook_test\",\n",
    "    })\n",
    "    \n",
    "    # Log configuration\n",
    "    mlflow.log_params({\n",
    "        \"vqvae_checkpoint\": config.vqvae.checkpoint,\n",
    "        \"codebook_size\": config.vqvae.codebook_size,\n",
    "        \"downsample_factor\": config.vqvae.downsample_factor,\n",
    "        \"transformer_layers\": small_config.num_layers,\n",
    "        \"transformer_hidden\": small_config.hidden_size,\n",
    "        \"model_params_millions\": num_params / 1e6,\n",
    "        \"image_size\": config.data.image_size,\n",
    "    })\n",
    "    \n",
    "    # Log some example metrics\n",
    "    for step in range(5):\n",
    "        mlflow.log_metrics({\n",
    "            \"loss\": 10.0 - step * 1.5,\n",
    "            \"perplexity\": 1000.0 - step * 150,\n",
    "        }, step=step)\n",
    "    \n",
    "    print(f\"Logged test run: {run_name}\")\n",
    "    print(f\"View at: {MLFLOW_TRACKING_URI}/#/experiments/{EXPERIMENTS[CURRENT_EXPERIMENT]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de708d0",
   "metadata": {},
   "source": [
    "## 12. Mini Training Loop\n",
    "\n",
    "Quick training loop to validate the pipeline works end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Mini training loop (just a few steps to validate)\n",
    "num_steps = 10\n",
    "model.train()\n",
    "\n",
    "print(\"Running mini training loop...\")\n",
    "losses = []\n",
    "\n",
    "for step in tqdm(range(num_steps)):\n",
    "    # Get a batch\n",
    "    batch = next(iter(dataloader))\n",
    "    images = batch[\"image\"].to(device)\n",
    "    captions = batch[\"caption\"]\n",
    "    \n",
    "    # Encode images to tokens\n",
    "    with torch.no_grad():\n",
    "        tokens = vqvae.encode(images)\n",
    "        tokens = tokens.view(tokens.size(0), -1)  # Flatten to sequence\n",
    "        context = clip_encoder.encode_text(captions)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits, loss = model(tokens, context=context)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1935dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, marker='o')\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Mini Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca3dcb",
   "metadata": {},
   "source": [
    "## 13. Next Steps\n",
    "\n",
    "Once the pipeline is validated:\n",
    "\n",
    "1. **For full training**, use the Python scripts with multi-GPU:\n",
    "   ```bash\n",
    "   # Start MLflow server\n",
    "   sbatch slurm/mlflow_server.sh\n",
    "   \n",
    "   # Submit training job\n",
    "   sbatch slurm/train_8gpu.sh\n",
    "   ```\n",
    "\n",
    "2. **Monitor training** in MLflow UI\n",
    "\n",
    "3. **After Phase 1**, move to Phase 2 (custom VQ-VAE) and Phase 3 (full transformer training)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
