{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3444581b",
   "metadata": {},
   "source": [
    "# Experiment 3: Train WikiArt Art Style Classifier\n",
    "\n",
    "This notebook trains a ResNet-18 based classifier to recognize 27 art styles from WikiArt.\n",
    "The classifier is used to evaluate how well generated images match the prompted art style.\n",
    "\n",
    "**Architecture:**\n",
    "- ResNet-18 backbone (pretrained on ImageNet)\n",
    "- Custom 512-dim embedding layer\n",
    "- Classification head for 27 art styles\n",
    "\n",
    "**Training:**\n",
    "- HuggingFace WikiArt dataset\n",
    "- Train/validation split (80/20)\n",
    "- Data augmentation (flip, rotation, color jitter)\n",
    "- 20 epochs, learning rate scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb635542",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b88cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project configuration - use absolute paths\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path(\"/home/doshlom4/work/final_project\")\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37713658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration and classifier\n",
    "from config import (\n",
    "    WIKIART_STYLES,\n",
    "    DATASET_CACHE_DIR,\n",
    "    CHECKPOINTS_DIR,\n",
    ")\n",
    "\n",
    "from wikiart_classifier import (\n",
    "    WikiArtClassifier,\n",
    "    train_wikiart_classifier,\n",
    "    evaluate_wikiart_classifier,\n",
    "    compute_per_class_accuracy,\n",
    "    get_wikiart_classifier_checkpoint_path,\n",
    "    get_wikiart_transforms,\n",
    ")\n",
    "\n",
    "# Deep learning frameworks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# HuggingFace datasets\n",
    "from datasets import load_dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    \"num_epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"image_size\": 128,\n",
    "    \"train_split\": 0.8,\n",
    "    \"num_workers\": 4,\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nNumber of art styles: {len(WIKIART_STYLES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d2e7a2",
   "metadata": {},
   "source": [
    "## 2. Load WikiArt Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c183be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiArt dataset\n",
    "print(\"Loading WikiArt dataset from HuggingFace...\")\n",
    "wikiart_hf = load_dataset(\n",
    "    \"huggan/wikiart\",\n",
    "    split=\"train\",\n",
    "    cache_dir=str(DATASET_CACHE_DIR / \"huggingface\")\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded: {len(wikiart_hf)} images\")\n",
    "print(f\"Features: {wikiart_hf.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect style column\n",
    "sample = wikiart_hf[0]\n",
    "style_column = 'style' if 'style' in sample else 'label'\n",
    "print(f\"Style column: {style_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bbc1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images per style\n",
    "style_counts = {i: 0 for i in range(len(WIKIART_STYLES))}\n",
    "\n",
    "for item in tqdm(wikiart_hf, desc=\"Counting styles\"):\n",
    "    style_idx = item[style_column]\n",
    "    if style_idx < len(WIKIART_STYLES):\n",
    "        style_counts[style_idx] += 1\n",
    "\n",
    "print(\"\\nImages per style:\")\n",
    "for style_idx in sorted(style_counts.keys()):\n",
    "    print(f\"  {WIKIART_STYLES[style_idx]}: {style_counts[style_idx]}\")\n",
    "\n",
    "total_valid = sum(style_counts.values())\n",
    "print(f\"\\nTotal valid images: {total_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ca7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "style_names = [s.replace('_', ' ')[:15] for s in WIKIART_STYLES]\n",
    "counts = [style_counts[i] for i in range(len(WIKIART_STYLES))]\n",
    "\n",
    "plt.bar(range(len(WIKIART_STYLES)), counts, color='steelblue')\n",
    "plt.xticks(range(len(WIKIART_STYLES)), style_names, rotation=45, ha='right', fontsize=8)\n",
    "plt.xlabel('Art Style')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('WikiArt Dataset: Images per Style')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c370606",
   "metadata": {},
   "source": [
    "## 3. Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e888a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((TRAINING_CONFIG[\"image_size\"], TRAINING_CONFIG[\"image_size\"])),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((TRAINING_CONFIG[\"image_size\"], TRAINING_CONFIG[\"image_size\"])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "print(\"Transforms defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09caf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset wrapper for HuggingFace dataset\n",
    "class WikiArtHFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset wrapper for HuggingFace WikiArt dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_dataset, indices, transform=None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Detect style column\n",
    "        sample = hf_dataset[0]\n",
    "        self.style_column = 'style' if 'style' in sample else 'label'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        item = self.hf_dataset[int(real_idx)]\n",
    "        \n",
    "        image = item['image']\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        label = item[self.style_column]\n",
    "        \n",
    "        # Skip if label out of range\n",
    "        if label >= len(WIKIART_STYLES):\n",
    "            label = label % len(WIKIART_STYLES)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to valid indices (only styles 0-26)\n",
    "valid_indices = []\n",
    "for i, item in enumerate(tqdm(wikiart_hf, desc=\"Filtering valid samples\")):\n",
    "    style_idx = item[style_column]\n",
    "    if style_idx < len(WIKIART_STYLES):\n",
    "        valid_indices.append(i)\n",
    "\n",
    "print(f\"\\nValid samples: {len(valid_indices)}\")\n",
    "\n",
    "# Shuffle and split\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(valid_indices)\n",
    "\n",
    "split_idx = int(len(valid_indices) * TRAINING_CONFIG[\"train_split\"])\n",
    "train_indices = valid_indices[:split_idx]\n",
    "test_indices = valid_indices[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(train_indices)}\")\n",
    "print(f\"Test samples: {len(test_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4506ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = WikiArtHFDataset(wikiart_hf, train_indices, transform=train_transform)\n",
    "test_dataset = WikiArtHFDataset(wikiart_hf, test_indices, transform=test_transform)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=TRAINING_CONFIG[\"num_workers\"],\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=TRAINING_CONFIG[\"num_workers\"],\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ea712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# Get a batch\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Denormalize for display\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = images[i] * std + mean\n",
    "    img = img.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(WIKIART_STYLES[labels[i].item()][:20], fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Training Samples', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544142dc",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61251b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca436d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = WikiArtClassifier(\n",
    "    num_classes=len(WIKIART_STYLES),\n",
    "    embedding_dim=512,\n",
    "    pretrained=True\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nâœ“ Created WikiArt Classifier\")\n",
    "print(f\"  Total parameters: {num_params:,}\")\n",
    "print(f\"  Trainable parameters: {num_trainable:,}\")\n",
    "print(f\"  Number of classes: {model.num_classes}\")\n",
    "print(f\"  Embedding dimension: {model.embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97bf00d",
   "metadata": {},
   "source": [
    "## 5. Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "save_path = get_wikiart_classifier_checkpoint_path()\n",
    "\n",
    "print(f\"Checkpoint will be saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548fa938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Starting WikiArt Classifier Training\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Epochs: {TRAINING_CONFIG['num_epochs']}\")\n",
    "print(f\"Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "model, history = train_wikiart_classifier(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    num_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
    "    lr=TRAINING_CONFIG[\"learning_rate\"],\n",
    "    save_path=save_path,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Best test accuracy: {max(history['test_acc']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd251ad1",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5888a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(range(1, len(history['train_loss']) + 1), history['train_loss'], 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(range(1, len(history['train_acc']) + 1), history['train_acc'], 'b-', linewidth=2, label='Train')\n",
    "axes[1].plot(range(1, len(history['test_acc']) + 1), history['test_acc'], 'r-', linewidth=2, label='Test')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Test Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('WikiArt Classifier Training Progress', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a52a5",
   "metadata": {},
   "source": [
    "## 7. Evaluate Per-Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a34254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-class accuracy\n",
    "per_class_acc = compute_per_class_accuracy(model, test_loader, device)\n",
    "\n",
    "print(\"Per-class accuracy:\")\n",
    "for style_name, acc in per_class_acc.items():\n",
    "    print(f\"  {style_name}: {acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a062f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class accuracy\n",
    "plt.figure(figsize=(14, 6))\n",
    "style_names = [s.replace('_', ' ')[:15] for s in WIKIART_STYLES]\n",
    "accuracies = [per_class_acc[s] for s in WIKIART_STYLES]\n",
    "\n",
    "colors = ['green' if acc >= 50 else 'orange' if acc >= 30 else 'red' for acc in accuracies]\n",
    "\n",
    "plt.bar(range(len(WIKIART_STYLES)), accuracies, color=colors)\n",
    "plt.xticks(range(len(WIKIART_STYLES)), style_names, rotation=45, ha='right', fontsize=8)\n",
    "plt.axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='50%')\n",
    "plt.xlabel('Art Style')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('WikiArt Classifier: Per-Class Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage accuracy: {np.mean(accuracies):.1f}%\")\n",
    "print(f\"Best style: {WIKIART_STYLES[np.argmax(accuracies)]} ({max(accuracies):.1f}%)\")\n",
    "print(f\"Worst style: {WIKIART_STYLES[np.argmin(accuracies)]} ({min(accuracies):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f2cd9d",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c44f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Collect all predictions\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Computing confusion matrix\"):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = outputs.max(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Normalize\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 14))\n",
    "style_labels = [s.replace('_', ' ')[:12] for s in WIKIART_STYLES]\n",
    "sns.heatmap(cm_normalized, annot=False, cmap='Blues', \n",
    "            xticklabels=style_labels, yticklabels=style_labels)\n",
    "plt.xlabel('Predicted Style')\n",
    "plt.ylabel('True Style')\n",
    "plt.title('WikiArt Classifier Confusion Matrix (Normalized)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def184b",
   "metadata": {},
   "source": [
    "## 9. Test on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on sample images from test set\n",
    "model.eval()\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "# Get random samples\n",
    "sample_indices = np.random.choice(len(test_dataset), 12, replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    image, label = test_dataset[idx]\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        img_tensor = image.unsqueeze(0).to(device)\n",
    "        output = model(img_tensor)\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        confidence, pred = probs.max(1)\n",
    "    \n",
    "    # Denormalize for display\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img_display = (image * std + mean).permute(1, 2, 0).numpy().clip(0, 1)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    ax.imshow(img_display)\n",
    "    \n",
    "    true_style = WIKIART_STYLES[label][:15]\n",
    "    pred_style = WIKIART_STYLES[pred.item()][:15]\n",
    "    conf = confidence.item() * 100\n",
    "    \n",
    "    color = 'green' if pred.item() == label else 'red'\n",
    "    ax.set_title(f'True: {true_style}\\nPred: {pred_style} ({conf:.0f}%)', \n",
    "                 fontsize=8, color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('WikiArt Classifier Predictions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f1857",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook trained a WikiArt art style classifier:\n",
    "\n",
    "**Model:**\n",
    "- ResNet-18 backbone (pretrained on ImageNet)\n",
    "- Custom classification head for 27 art styles\n",
    "- 512-dim embedding layer for feature extraction\n",
    "\n",
    "**Training:**\n",
    "- HuggingFace WikiArt dataset\n",
    "- 80/20 train/test split\n",
    "- Data augmentation (flip, rotation, color jitter)\n",
    "- Saved best model checkpoint\n",
    "\n",
    "**Usage:**\n",
    "The classifier will be used in `metrics1_evaluate_wikiart.ipynb` to compute:\n",
    "1. Classification accuracy on generated images (prompt adherence)\n",
    "2. Feature extraction for additional metrics\n",
    "\n",
    "**Next steps:**\n",
    "- `metrics1_evaluate_wikiart.ipynb` - Compute FID and classification accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Stable Diffusion - CUDA 11.8)",
   "language": "python",
   "name": "stable_diffusion_cuda118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
