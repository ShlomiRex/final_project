{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bb5d38",
   "metadata": {},
   "source": [
    "# CelebA-HQ Latent Diffusion Training with Classifier-Free Guidance\n",
    "\n",
    "This notebook trains a text-conditioned diffusion model on CelebA-HQ using **latent diffusion** (operates in VAE latent space instead of pixel space for faster training).\n",
    "\n",
    "## Key Differences from Pixel-Space Training:\n",
    "1. **VAE Encoding**: Images are encoded to 32√ó32√ó4 latents before training\n",
    "2. **Faster Training**: 8x smaller spatial dimensions = much faster\n",
    "3. **Latent U-Net**: 4-channel input/output (latent channels)\n",
    "4. **Same CFG**: Classifier-free guidance works the same way\n",
    "\n",
    "## Training Configuration:\n",
    "- **Image size**: 256√ó256 ‚Üí 32√ó32 latents (8x compression)\n",
    "- **Model**: Custom UNet2DConditionModel (latent space)\n",
    "- **VAE**: Pretrained SD VAE (frozen, `stabilityai/sd-vae-ft-mse`)\n",
    "- **Text encoder**: CLIP ViT-B/32 (512-dim embeddings)\n",
    "- **CFG**: 10% unconditional dropout\n",
    "- **Batch size**: 32\n",
    "- **Epochs**: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af32fff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/doshlom4/work/final_project\n"
     ]
    }
   ],
   "source": [
    "# Setup: Define project root and add to path\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path(\"/home/doshlom4/work/final_project\")\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c147c349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doshlom4/work/conda/envs/shlomid_conda_12_11_2025/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Import custom modules\n",
    "from config import (\n",
    "    TRAIN_CELEBA_LDM_CONFIG,\n",
    "    EXPERIMENT_4_CONFIG,\n",
    "    DATASET_CACHE_DIR,\n",
    "    CHECKPOINTS_DIR,\n",
    "    UNET_CELEBA_LDM_CHECKPOINT_PREFIX,\n",
    "    CELEBA_DATASET_NAME,\n",
    "    CLIP_MODEL_NAME,\n",
    "    ensure_experiment_4_dirs,\n",
    ")\n",
    "from models.custom_unet_celeba_ldm import CustomUNet2DConditionModelCelebaLDM\n",
    "from models.vae_wrapper import VAEWrapper\n",
    "from custom_datasets.celeba_hq_dataset import CelebAHQWithCaptions\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97a0509f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Epochs: 100\n",
      "  Learning rate: 1e-05\n",
      "  Batch size: 32\n",
      "  Image size: 256√ó256\n",
      "  Latent size: 32√ó32\n",
      "  CFG dropout: 10%\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "NUM_EPOCHS = TRAIN_CELEBA_LDM_CONFIG[\"num_epochs\"]\n",
    "LEARNING_RATE = TRAIN_CELEBA_LDM_CONFIG[\"learning_rate\"]\n",
    "BATCH_SIZE = TRAIN_CELEBA_LDM_CONFIG[\"batch_size\"]\n",
    "NUM_TRAIN_TIMESTEPS = TRAIN_CELEBA_LDM_CONFIG[\"num_train_timesteps\"]\n",
    "BETA_SCHEDULE = TRAIN_CELEBA_LDM_CONFIG[\"beta_schedule\"]\n",
    "CHECKPOINT_EVERY_N_EPOCHS = TRAIN_CELEBA_LDM_CONFIG[\"checkpoint_every_n_epochs\"]\n",
    "CFG_DROPOUT_PROB = TRAIN_CELEBA_LDM_CONFIG[\"cfg_dropout_prob\"]\n",
    "\n",
    "IMAGE_SIZE = EXPERIMENT_4_CONFIG[\"image_size\"]\n",
    "LATENT_SIZE = EXPERIMENT_4_CONFIG[\"latent_size\"]\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}√ó{IMAGE_SIZE}\")\n",
    "print(f\"  Latent size: {LATENT_SIZE}√ó{LATENT_SIZE}\")\n",
    "print(f\"  CFG dropout: {CFG_DROPOUT_PROB*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b08206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "CUDA version: 11.8\n",
      "\n",
      "‚úì Directories created\n"
     ]
    }
   ],
   "source": [
    "# Setup device and create directories\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Create experiment directories\n",
    "ensure_experiment_4_dirs()\n",
    "print(f\"\\n‚úì Directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acac199",
   "metadata": {},
   "source": [
    "## Step 1: Load CelebA Dataset\n",
    "\n",
    "Load the CelebA dataset with attributes from local cache. The dataset contains 200K celebrity face images at 178√ó218 resolution with 40 binary attributes per image (gender, age, hair color, facial hair, accessories, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0866aac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CelebA dataset...\n",
      "Dataset: flwrlabs/celeba\n",
      "Cache directory: /home/doshlom4/work/final_project/dataset_cache\n",
      "\n",
      "üì• Loading 'flwrlabs/celeba' from cache...\n",
      "‚úì Loaded 162770 images\n",
      "   Image size: (178, 218)\n",
      "   Dataset features: ['image', 'celeb_id', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n",
      "   Attributes: 40 binary attributes\n",
      "\n",
      "‚úì Dataset ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Load CelebA dataset with attributes\n",
    "print(\"Loading CelebA dataset...\")\n",
    "print(f\"Dataset: {CELEBA_DATASET_NAME}\")\n",
    "print(f\"Cache directory: {DATASET_CACHE_DIR}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    print(f\"üì• Loading '{CELEBA_DATASET_NAME}' from cache...\")\n",
    "    \n",
    "    # Load dataset from cached location\n",
    "    celeba_hq = load_dataset(\n",
    "        CELEBA_DATASET_NAME,\n",
    "        cache_dir=str(DATASET_CACHE_DIR / \"huggingface\"),\n",
    "        split=\"train\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Loaded {len(celeba_hq)} images\")\n",
    "    print(f\"   Image size: {celeba_hq[0]['image'].size}\")\n",
    "    print(f\"   Dataset features: {list(celeba_hq.features.keys())}\")\n",
    "    print(f\"   Attributes: {len([k for k in celeba_hq.features.keys() if k not in ['image', 'celeb_id']])} binary attributes\")\n",
    "    print(f\"\\n‚úì Dataset ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Failed to load dataset: {e}\")\n",
    "    celeba_hq = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e0df77",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Models\n",
    "\n",
    "Initialize all models:\n",
    "1. **VAE** (frozen, pretrained) - for encoding/decoding\n",
    "2. **UNet** (trainable) - noise prediction in latent space\n",
    "3. **CLIP Text Encoder** (frozen) - text embeddings\n",
    "4. **Noise Scheduler** - DDPM with squared cosine schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3138c4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VAE...\n",
      "Loading VAE from: stabilityai/sd-vae-ft-mse\n",
      "VAE loaded and frozen:\n",
      "  - Latent channels: 4\n",
      "  - Downsample factor: 8x\n",
      "  - Scale factor: 0.18215\n",
      "‚úì VAE loaded and frozen\n"
     ]
    }
   ],
   "source": [
    "# Initialize VAE (frozen, pretrained)\n",
    "print(\"Loading VAE...\")\n",
    "vae = VAEWrapper().to(device)\n",
    "print(\"‚úì VAE loaded and frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cb6229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing UNet...\n",
      "Number of trainable parameters: 109,370,116 (109.4M)\n",
      "‚úì UNet initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize UNet (trainable)\n",
    "print(\"Initializing UNet...\")\n",
    "unet = CustomUNet2DConditionModelCelebaLDM().to(device)\n",
    "unet.print_parameter_count()\n",
    "print(\"‚úì UNet initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a21800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP...\n",
      "‚úì CLIP loaded and frozen\n"
     ]
    }
   ],
   "source": [
    "# Initialize CLIP text encoder and tokenizer (frozen)\n",
    "print(\"Loading CLIP...\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    CLIP_MODEL_NAME,\n",
    "    cache_dir=str(DATASET_CACHE_DIR / \"huggingface\"),\n",
    ").to(device)\n",
    "text_encoder.eval()\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    CLIP_MODEL_NAME,\n",
    "    cache_dir=str(DATASET_CACHE_DIR / \"huggingface\"),\n",
    ")\n",
    "print(\"‚úì CLIP loaded and frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8b37a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Noise scheduler initialized (1000 timesteps)\n"
     ]
    }
   ],
   "source": [
    "# Initialize noise scheduler\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=NUM_TRAIN_TIMESTEPS,\n",
    "    beta_schedule=BETA_SCHEDULE,\n",
    ")\n",
    "print(f\"‚úì Noise scheduler initialized ({NUM_TRAIN_TIMESTEPS} timesteps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b3c866",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Dataset\n",
    "\n",
    "Create transforms and dataset. Images are:\n",
    "1. Resized to 256√ó256\n",
    "2. Converted to tensor\n",
    "3. Normalized to [-1, 1] (required for VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc9b3fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CelebA-HQ Dataset initialized:\n",
      "  - Total images: 162770\n",
      "  - Available attributes: 40\n",
      "  - Attributes used for prompting: 16\n",
      "‚úì Dataset ready: 162770 images\n",
      "‚úì DataLoader ready: 5087 batches\n"
     ]
    }
   ],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Create dataset (only if celeba_hq was loaded)\n",
    "if celeba_hq is not None:\n",
    "    dataset = CelebAHQWithCaptions(\n",
    "        hf_dataset=celeba_hq,\n",
    "        transform=transform,\n",
    "    )\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Dataset ready: {len(dataset)} images\")\n",
    "    print(f\"‚úì DataLoader ready: {len(dataloader)} batches\")\n",
    "else:\n",
    "    print(\"‚ö† Skipping dataset creation (dataset not loaded)\")\n",
    "    dataloader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691c5da",
   "metadata": {},
   "source": [
    "## Step 4: Training Setup\n",
    "\n",
    "Initialize optimizer and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2599e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Optimizer initialized (AdamW, lr=1e-05)\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
    "print(f\"‚úì Optimizer initialized (AdamW, lr={LEARNING_RATE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b763802",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop\n",
    "\n",
    "Key steps in each training iteration:\n",
    "1. **Encode images to latents** using frozen VAE\n",
    "2. **Add noise** to latents (in latent space, not pixel space)\n",
    "3. **Get text embeddings** from CLIP\n",
    "4. **Apply CFG dropout**: Replace 10% of prompts with empty string\n",
    "5. **Predict noise** using UNet\n",
    "6. **Compute loss** and update weights\n",
    "\n",
    "**Note**: This is much faster than pixel-space training because we operate on 32√ó32 latents instead of 256√ó256 pixels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f04e391b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  15%|‚ñà‚ñç        | 743/5087 [03:33<20:48,  3.48it/s, loss=0.4304, avg_loss=0.4162]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(noise_pred, noise)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Step 8: Backprop and optimize\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     65\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/work/conda/envs/shlomid_conda_12_11_2025/lib/python3.10/site-packages/torch/_compile.py:51\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     49\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/conda/envs/shlomid_conda_12_11_2025/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/work/conda/envs/shlomid_conda_12_11_2025/lib/python3.10/site-packages/torch/optim/optimizer.py:967\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[0;32m--> 967\u001b[0m         p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    unet.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    for batch_idx, (images, captions, attributes) in enumerate(progress_bar):\n",
    "        images = images.to(device)  # (B, 3, 256, 256)\n",
    "        \n",
    "        # Step 1: Encode images to latents using VAE\n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(images)  # (B, 4, 32, 32)\n",
    "        \n",
    "        # Step 2: Sample noise to add to latents\n",
    "        noise = torch.randn_like(latents)\n",
    "        \n",
    "        # Step 3: Sample random timesteps\n",
    "        timesteps = torch.randint(\n",
    "            0, NUM_TRAIN_TIMESTEPS, (latents.shape[0],),\n",
    "            device=device,\n",
    "        ).long()\n",
    "        \n",
    "        # Step 4: Add noise to latents according to timesteps\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        # Step 5: Get text embeddings\n",
    "        # Apply CFG dropout: randomly replace captions with empty string\n",
    "        cfg_mask = torch.rand(len(captions)) < CFG_DROPOUT_PROB\n",
    "        captions_cfg = [\n",
    "            \"\" if cfg_mask[i] else captions[i]\n",
    "            for i in range(len(captions))\n",
    "        ]\n",
    "        \n",
    "        # Tokenize and encode\n",
    "        text_inputs = tokenizer(\n",
    "            captions_cfg,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder_hidden_states = text_encoder(\n",
    "                text_inputs.input_ids.to(device)\n",
    "            )[0]  # (B, 77, 512)\n",
    "        \n",
    "        # Step 6: Predict noise using UNet\n",
    "        noise_pred = unet(\n",
    "            noisy_latents,\n",
    "            timesteps,\n",
    "            encoder_hidden_states,\n",
    "        ).sample\n",
    "        \n",
    "        # Step 7: Compute loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # Step 8: Backprop and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "        global_step += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"avg_loss\": f\"{epoch_loss/(batch_idx+1):.4f}\",\n",
    "        })\n",
    "    \n",
    "    # End of epoch\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Average loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % CHECKPOINT_EVERY_N_EPOCHS == 0:\n",
    "        checkpoint_path = CHECKPOINTS_DIR / f\"{UNET_CELEBA_LDM_CHECKPOINT_PREFIX}{epoch}.pt\"\n",
    "        \n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"global_step\": global_step,\n",
    "            \"unet_state_dict\": unet.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": avg_epoch_loss,\n",
    "            \"config\": {\n",
    "                \"image_size\": IMAGE_SIZE,\n",
    "                \"latent_size\": LATENT_SIZE,\n",
    "                \"vae_model\": \"stabilityai/sd-vae-ft-mse\",\n",
    "                \"vae_scale_factor\": vae.scale_factor,\n",
    "            },\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        print(f\"‚úì Saved checkpoint: {checkpoint_path.name}\")\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bda24",
   "metadata": {},
   "source": [
    "## Step 6: Save Final Model\n",
    "\n",
    "Save the final trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b253dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final checkpoint\n",
    "final_checkpoint_path = CHECKPOINTS_DIR / f\"{UNET_CELEBA_LDM_CHECKPOINT_PREFIX}final.pt\"\n",
    "\n",
    "torch.save({\n",
    "    \"epoch\": NUM_EPOCHS - 1,\n",
    "    \"global_step\": global_step,\n",
    "    \"unet_state_dict\": unet.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"config\": {\n",
    "        \"image_size\": IMAGE_SIZE,\n",
    "        \"latent_size\": LATENT_SIZE,\n",
    "        \"vae_model\": \"stabilityai/sd-vae-ft-mse\",\n",
    "        \"vae_scale_factor\": vae.scale_factor,\n",
    "    },\n",
    "}, final_checkpoint_path)\n",
    "\n",
    "print(f\"‚úì Saved final checkpoint: {final_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f09b51",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Inference**: Use `inference1_t2i_celeba_hq_ldm_cfg.ipynb` to generate images\n",
    "2. **Metrics**: Use `metrics1_evaluate_celeba_hq.ipynb` to compute FID and CLIP scores\n",
    "3. **Classifier**: Train attribute classifier in `train2_train_celeba_attribute_classifier.ipynb`\n",
    "\n",
    "The trained model can generate 256√ó256 face images from text prompts like:\n",
    "- \"A photo of a young woman with blond hair, smiling\"\n",
    "- \"A portrait of an older man with eyeglasses\"\n",
    "- \"A young person with black hair and no accessories\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Stable Diffusion - CUDA 11.8)",
   "language": "python",
   "name": "stable_diffusion_cuda118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
