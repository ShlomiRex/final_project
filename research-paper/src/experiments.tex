\section{Experiments}

\subsection{Experimental Setup}

\subsubsection{Hardware Environment}

\begin{itemize}
    \item \textbf{Environment:} HPC cluster with GPU nodes (specifications vary by node type)
    \item \textbf{Cluster resources (node available):}
    \begin{itemize}
        \item \textbf{CPU}: Intel Xeon Gold 6330 (56 cores @ 2.00 GHz)
        \item \textbf{GPUs}: 8x NVIDIA A100 80GB PCIe (80GB VRAM)
        \item \textbf{Memory (RAM)}: Node total 256 GB
    \end{itemize}
    \item \textbf{Requested resources (SLURM allocation used for experiments):}
    \begin{itemize}
        \item \textbf{CPUs}: 4 logical cores requested via `--cpus-per-task=4` (experiments used 4 cores)
        \item \textbf{Memory}: 32 GB requested via `--mem=32G`
        \item \textbf{GPUs}: 1 GPU (NVIDIA A100) requested via `--gres=gpu:1`
    \end{itemize}
\end{itemize}

\subsubsection{Software Environment}

\begin{itemize}
    \item \textbf{CUDA:} Version 11.8
    \item \textbf{Python:} 3.10.19 (in different experiments I used 3.11 as well)
    \item \textbf{Deep Learning Framework:} PyTorch 2.7.1+cu118
\end{itemize}

\subsubsection{Distributed Training Considerations}

Although the HPC environment supports multi-GPU and multi-node jobs, distributed training via SLURM proved unreliable in practice due to recurring configuration and environment issues. Additionally, distributed training with Python scripts requires external experiment tracking tools (e.g., MLflow) to monitor training progress, involving significant setup overhead: configuring image logging, artifact storage locations, and experiment tracking infrastructure. In contrast, Jupyter notebooks provide immediate visual feedback on training progress, loss curves, and generated samples without additional tooling. To keep the study focused and reproducible with minimal overhead, all experiments were conducted on a single NVIDIA A100 80GB GPU using Jupyter notebooks. Future work may revisit distributed training using a non-interactive script and a unified environment submitted as SLURM batch jobs, with proper experiment tracking infrastructure in place.

\input{experiment_mnist_t2i_cfg}
\input{experiment_cifar10_t2i_cfg}
\input{experiment_wikiart_t2i_cfg}