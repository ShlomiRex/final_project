\begin{abstract}

We developed a text-to-image generation model based on Stable Diffusion, a diffusion-based generative framework that synthesizes images by iteratively refining random noise into coherent visual content. 

The model utilizes CLIP embeddings as the text conditioning mechanism, enabling alignment between textual descriptions and generated images. Our approach operates in latent space for computational efficiency while leveraging the inherent stability and robustness of the diffusion process. We demonstrate that our method substantially outperforms existing GAN-based approaches, particularly in comparison with Creative Adversarial Networks (CAN) that employ the Deep Interactive Evolution (DeepIE) methodology for user-guided art generation. Our findings establish diffusion-based models as a superior paradigm for text-conditioned image synthesis, offering enhanced stability, sample quality, and quantitative evaluation metrics compared to adversarial approaches.

\end{abstract}