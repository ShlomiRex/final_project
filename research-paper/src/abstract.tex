\begin{abstract}

We developed a text-to-image generation model based on Stable Diffusion, a diffusion-based generative framework that synthesizes images by iteratively refining random noise into coherent visual content. The model utilizes CLIP embeddings as the text conditioning mechanism, enabling alignment between textual descriptions and generated imagery. Our approach operates in latent space for computational efficiency while leveraging the inherent stability and robustness of the diffusion process. We demonstrate that our method substantially outperforms existing GAN-based approaches, particularly in comparison with Creative Adversarial Networks (CAN) that employ the Deep Interactive Evolution (DeepIE) methodology for user-guided art generation. The superior performance highlights the advantages of diffusion-based models over adversarial methods for text-conditioned image synthesis.

\end{abstract}