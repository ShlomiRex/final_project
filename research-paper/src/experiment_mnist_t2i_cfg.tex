\subsection{MNIST Text-to-Image Generation with Classifier-Free Guidance}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth,keepaspectratio]{../figures/experiments/experiment_1/generated_cfg_with_prompt.png}
    \caption[Generated images of digits 0-9 with CFG (w=5) and text prompts]{Generated images for all digits (0-9) using CFG with guidance scale $w=5$ and text prompts "A handwritten digit \{0-9\}". Demonstrates improved generation quality and text-image alignment compared to non-CFG baseline \ref{fig:mnist_no_cfg}.}
    \label{fig:mnist_cfg_with_prompt}
\end{figure}

\subsubsection{Objective}
This experiment tests the fundamental principles of text-to-image generation using a diffusion-based architecture. By training on MNIST handwritten digits as a minimal-scale dataset, we investigate how the stable diffusion model handles image synthesis from text prompts and systematically evaluate the impact of Classifier-Free Guidance (CFG) on generation quality and text-image alignment.

\subsubsection{Model Architecture}

\textbf{Text Encoder (Frozen):}
\begin{itemize}
    \item \textbf{Model:} CLIP (openai/clip-vit-base-patch32)
    \item \textbf{Embedding dimension:} 512
    \item \textbf{Tokenizer max length:} 8 tokens (reduced from default 77)\footnote{Prompts are tokenized then padded or truncated to exactly 8 tokens. This keeps shapes fixed (batch, 8, 512) for CLIP embeddings and reduces unnecessary padding/compute versus the 77-token default. Longer prompts would be clipped beyond 8 tokens, which is acceptable here because prompts are intentionally short (e.g., "A handwritten digit 5").}
    \item \textbf{Training:} Weights are frozen
\end{itemize}

\textbf{Denoising Network (U-Net):}

The key design choice in the U-Net architecture is to train directly in pixel space without a VAE, which is viable for MNIST's low resolution (28$\times$28).

\begin{itemize}
    \item \textbf{Architecture:} Custom UNet2DConditionModel
    \item \textbf{Input/Output:} 1 channel (grayscale), 28$\times$28 pixels
    \item \textbf{Block channels:} (32, 64, 64, 32)
    \item \textbf{Layers per block:} 2
    \item \textbf{Down blocks:} DownBlock2D → CrossAttnDownBlock2D → CrossAttnDownBlock2D → DownBlock2D
    \item \textbf{Up blocks:} UpBlock2D → CrossAttnUpBlock2D → CrossAttnUpBlock2D → UpBlock2D
    \item \textbf{Cross-attention dimension:} 512 (matches CLIP embedding size)
    \item \textbf{Total trainable parameters:} 3,140,385
\end{itemize}

\subsubsection{Dataset}

\textbf{MNIST Handwritten Digits:}
\begin{itemize}
    \item \textbf{Training images:} 60,000
    \item \textbf{Resolution:} 28$\times$28 pixels, grayscale
    \item \textbf{Classes:} 10 digit classes (0-9)
    \item \textbf{Text captions:} Automatically generated as "A handwritten digit \{label\}" (e.g., "A handwritten digit 5")
    \item \textbf{Preprocessing:} Conversion to tensors with values normalized to [0, 1]
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.55\textwidth,keepaspectratio]{../figures/experiments/experiment_1/mnist_training_dataset.png}
    \caption[Sample handwritten digits (0-9) from MNIST training set]{Sample images from MNIST training dataset showing handwritten digits (0-9) with corresponding class labels.}
    \label{fig:mnist_training_dataset}
\end{figure}

\subsubsection{Training Configuration}

\begin{itemize}
    \item \textbf{Batch size:} 512
    \item \textbf{Learning rate:} $10^{-3}$
    \item \textbf{Optimizer:} AdamW: \texttt{torch.optim.AdamW(unet.parameters(), lr=1e-3)}
    \item \textbf{Epochs:} 20 continuous training epochs with checkpoints saved at epochs 5, 10, 15, and 20
    \item \textbf{Noise scheduler:} DDPM with squared cosine beta schedule: \texttt{DDPMScheduler(num\_train\_timesteps=1000, beta\_schedule="squaredcos\_cap\_v2")}
    \item \textbf{Timesteps:} 1,000
    \item \textbf{Loss function:} Mean Squared Error (MSE) between predicted and actual noise
\end{itemize}

\textbf{Training Pipeline per Batch:}
\begin{enumerate}
    \item Convert digit labels to text captions using CLIP tokenizer. Format: \texttt{"A handwritten digit \{label\}"} where $label \in \{0, 1, ..., 9\}$
    \item Encode captions to semantic embeddings via frozen CLIP text encoder [batch, 8, 512]
    \item Sample random timestep $t \sim \text{Uniform}(0, 1000)$ for each image
    \item Add Gaussian noise to images: $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$
    \item Predict noise: $\epsilon_\theta(x_t, t, c_\text{text})$ using UNet with cross-attention conditioning
    \item Calculate MSE loss: $\mathcal{L} = \|\epsilon - \epsilon_\theta(x_t, t, c_\text{text})\|^2$
    \item Backpropagate and update UNet parameters only (CLIP remains frozen)
\end{enumerate}

\subsubsection{Inference and Classifier-Free Guidance}

\textbf{Basic Sampling (Without CFG):}
\begin{itemize}
    \item Initialize random noise tensor: $x_T \sim \mathcal{N}(0, I)$
    \item Encode text prompt via CLIP
    \item Iteratively denoise for 50 steps using DDPM scheduler
    \item Post-process: normalize output to [0, 255] for visualization
\end{itemize}

Figure~\ref{fig:mnist_no_cfg} shows generated images of digits 0-9 without CFG, demonstrating the baseline generation quality using only conditional text prompts.

\textbf{Classifier-Free Guidance (CFG):}

CFG enables stronger text conditioning by computing both conditional and unconditional predictions:

\begin{enumerate}
    \item \textbf{Dual embeddings:}
    \begin{itemize}
        \item Conditional: Text prompt → CLIP embedding $c_\text{text}$
        \item Unconditional: Empty string "" → CLIP embedding $c_\text{null}$
    \end{itemize}
    \item \textbf{Guidance formula:}
    \begin{equation}
        \tilde{\boldsymbol{\epsilon}} = \boldsymbol{\epsilon}_\text{uncond} + w \cdot (\boldsymbol{\epsilon}_\text{cond} - \boldsymbol{\epsilon}_\text{uncond})
        \label{eq:cfg}
    \end{equation}
    where $w$ is the guidance scale, $\boldsymbol{\epsilon}_\text{uncond} = \text{UNet}(x_t, t, c_\text{null})$, and $\boldsymbol{\epsilon}_\text{cond} = \text{UNet}(x_t, t, c_\text{text})$.
    
    \textbf{Implementation in Experiment 1:}
        \footnote{The \texttt{.chunk(2)} operation splits the concatenated noise predictions (which contains both unconditional and conditional predictions stacked together) into two equal tensors. The guided noise prediction is then computed using Equation~\ref{eq:cfg}, which is passed to the scheduler's step function to iteratively denoise the latent representation.}
    \begin{verbatim}
    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
    noise_pred = noise_pred_uncond + guidance_scale * \
                 (noise_pred_text - noise_pred_uncond)
    \end{verbatim}
    
    \item \textbf{Effect:} Higher $w$ → stronger adherence to text prompt
\end{enumerate}

Figure~\ref{fig:mnist_no_cfg} shows generated images of digits 0-9 without CFG, demonstrating the baseline generation quality using only conditional text prompts. Figure~\ref{fig:mnist_cfg_empty_prompt} shows unconditional generation using CFG with empty prompts ($w=5$), producing varied outputs ranging from recognizable digits to ambiguous patterns due to the absence of text guidance.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth,keepaspectratio]{../figures/experiments/experiment_1/generated_no_cfg_with_prompt.png}
    \caption[Effect of guidance scale on MNIST digit generation quality]{Generated images of digits 0-9 without CFG. Digits 4 and 5 show some artifacts, indicating CFG enhances generation quality and text-image alignment (see Figure~\ref{fig:mnist_cfg_with_prompt}).}
    \label{fig:mnist_no_cfg}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\textwidth,keepaspectratio]{../figures/experiments/experiment_1/generated_cfg_empty_prompt.png}
    \caption[Unconditional generation with CFG (w=5) and empty prompts]{Unconditional generation using CFG with guidance scale $w=5$ and empty prompts. Without text conditioning, the model produces varied outputs including recognizable and unrecognized digits.}
    \label{fig:mnist_cfg_empty_prompt}
\end{figure}

\textbf{Inference Parameters:}
\begin{itemize}
    \item \textbf{Scheduler:} DDPM with squared cosine schedule
    \item \textbf{Number of inference steps:} 50
    \item \textbf{Random seed:} 422 (for reproducibility)
    \item \textbf{Guidance scales tested:} $w \in \{0, 5, 10, 20, 50, 100\}$
\end{itemize}

\subsubsection{Results and Analysis}

\textbf{Training Convergence:}

Figure~\ref{fig:mnist_loss} demonstrates the training convergence of the diffusion model over 20 epochs. The curve demonstrates rapid initial convergence followed by plateau, indicating successful optimization of the diffusion model’s noise prediction task.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth,keepaspectratio]{../figures/experiments/experiment_1/mnist_loss_graph.png}
    \caption[Training loss convergence over 20 epochs]{Training loss curve of experiment 1.}
    \label{fig:mnist_loss}
\end{figure}


\textbf{Extended Training Results:}


% Place the extended training results figure here to ensure it appears within the MNIST experiment section
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth,keepaspectratio]{../figures/experiments/experiment_1/mnist_generation_20epochs.png}
    \caption[Improved MNIST generation after extended training (20 epochs)]{Generation results after 20 epochs of training for all MNIST digit classes (0-9, rows) across different guidance scales ($w \in \{0, 1, 2, 3, 5, 10, 20\}$, columns). All images generated using 50 inference steps. Extended training produces sharper digits with improved text-prompt alignment and reduced artifacts, particularly noticeable at higher guidance scales.}
    \label{fig:mnist_generation_20epochs}
\end{figure}

To evaluate the impact of extended training, we trained the model for 20 epochs and regenerated samples across all digit classes (see Figure~\ref{fig:mnist_generation_20epochs}).

\subsubsection{Key Findings}

\textbf{Noise Characteristics with CFG:} Comparing Figures~\ref{fig:mnist_no_cfg} and~\ref{fig:mnist_cfg_with_prompt}, we observe that CFG with moderate guidance scale ($w=5$) produces slightly noisier outputs compared to the non-CFG baseline, despite using identical text prompts. This phenomenon can be attributed to several factors:

\begin{itemize}
    \item \textbf{Training-inference mismatch:} The model was trained without CFG, performing standard conditional generation. At inference time, CFG introduces a distribution shift by extrapolating beyond the learned conditional distribution using the formula: $\tilde{\boldsymbol{\epsilon}} = \boldsymbol{\epsilon}_\text{uncond} + w \cdot (\boldsymbol{\epsilon}_\text{cond} - \boldsymbol{\epsilon}_\text{uncond})$. This extrapolation can amplify noise present in the predictions.
    
    \item \textbf{Noise amplification:} The CFG mechanism amplifies the \emph{difference} between conditional and unconditional predictions. When $w > 1$, even small noise components in either prediction are magnified, leading to grainier textures in the output.
    
    \item \textbf{Sub-optimal guidance scale:} While $w=5$ improves text-prompt adherence, it may not be the optimal value for this model-dataset combination. The slight noise increase suggests a trade-off between prompt alignment and output smoothness.
    
    \item \textbf{Iterative error accumulation:} Over 50 denoising steps, the guided predictions may accumulate small errors differently than standard sampling, particularly when operating outside the training distribution.
\end{itemize}

This observation highlights an important consideration when applying CFG: the guidance scale must be carefully tuned to balance text-prompt adherence against output quality. For production systems, this suggests either: (1) training explicitly with CFG to minimize the training-inference gap, or (2) systematic hyperparameter search to identify the optimal guidance scale that maximizes both prompt alignment and visual quality.
