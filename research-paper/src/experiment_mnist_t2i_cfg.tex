\subsection{MNIST Text-to-Image Generation with Classifier-Free Guidance}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth,keepaspectratio]{../figures/experiments/mnist_prompts.png}
    \caption[Effect of guidance scale on MNIST digit generation quality]{Generated images for prompt "A handwritten digit 3" across different guidance scales. Higher guidance scales produce sharper, more confident outputs with stronger adherence to the text prompt.}
    \label{fig:mnist_prompts}
\end{figure}

\subsubsection{Objective}
This experiment tests the fundamental principles of text-to-image generation using a diffusion-based architecture. By training on MNIST handwritten digits as a minimal-scale dataset, we investigate how the stable diffusion model handles image synthesis from text prompts and systematically evaluate the impact of Classifier-Free Guidance (CFG) on generation quality and text-image alignment.

\subsubsection{Model Architecture}

\textbf{Text Encoder (Frozen):}
\begin{itemize}
    \item \textbf{Model:} CLIP (openai/clip-vit-base-patch32)
    \item \textbf{Embedding dimension:} 512
    \item \textbf{Tokenizer max length:} 8 tokens (reduced from default 77)\footnote{Prompts are tokenized then padded or truncated to exactly 8 tokens. This keeps shapes fixed (batch, 8, 512) for CLIP embeddings and reduces unnecessary padding/compute versus the 77-token default. Longer prompts would be clipped beyond 8 tokens, which is acceptable here because prompts are intentionally short (e.g., "A handwritten digit 5").}
    \item \textbf{Training:} Weights are frozen
\end{itemize}

\textbf{Denoising Network (U-Net):}

The key design choice in the U-Net architecture is to train directly in pixel space without a VAE, which is viable for MNIST's low resolution (28$\times$28).

\begin{itemize}
    \item \textbf{Architecture:} Custom UNet2DConditionModel
    \item \textbf{Input/Output:} 1 channel (grayscale), 28$\times$28 pixels
    \item \textbf{Block channels:} (32, 64, 64, 32)
    \item \textbf{Layers per block:} 2
    \item \textbf{Down blocks:} DownBlock2D → CrossAttnDownBlock2D → CrossAttnDownBlock2D → DownBlock2D
    \item \textbf{Up blocks:} UpBlock2D → CrossAttnUpBlock2D → CrossAttnUpBlock2D → UpBlock2D
    \item \textbf{Cross-attention dimension:} 512 (matches CLIP embedding size)
    \item \textbf{Total trainable parameters:} 3,140,385
\end{itemize}

\subsubsection{Dataset}

\textbf{MNIST Handwritten Digits:}
\begin{itemize}
    \item \textbf{Training images:} 60,000
    \item \textbf{Resolution:} 28$\times$28 pixels, grayscale
    \item \textbf{Classes:} 10 digit classes (0-9)
    \item \textbf{Text captions:} Automatically generated as "A handwritten digit \{label\}" (e.g., "A handwritten digit 5")
    \item \textbf{Preprocessing:} Conversion to tensors with values normalized to [0, 1]
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.55\textwidth,keepaspectratio]{../figures/experiments/mnist_training_dataset.png}
    \caption[Sample handwritten digits (0-9) from MNIST training set]{Sample images from MNIST training dataset showing handwritten digits (0-9) with corresponding class labels.}
    \label{fig:mnist_training_dataset}
\end{figure}

\subsubsection{Training Configuration}

\begin{itemize}
    \item \textbf{Batch size:} 512
    \item \textbf{Learning rate:} $10^{-3}$
    \item \textbf{Optimizer:} AdamW: \texttt{torch.optim.AdamW(unet.parameters(), lr=1e-3)}
    \item \textbf{Epochs:} 20 continuous training epochs with checkpoints saved at epochs 5, 10, 15, and 20
    \item \textbf{Noise scheduler:} DDPM with squared cosine beta schedule: \texttt{DDPMScheduler(num\_train\_timesteps=1000, beta\_schedule="squaredcos\_cap\_v2")}
    \item \textbf{Timesteps:} 1,000
    \item \textbf{Loss function:} Mean Squared Error (MSE) between predicted and actual noise
\end{itemize}

\textbf{Training Pipeline per Batch:}
\begin{enumerate}
    \item Convert digit labels to text captions using CLIP tokenizer. Format: \texttt{"A handwritten digit \{label\}"} where $label \in \{0, 1, ..., 9\}$
    \item Encode captions to semantic embeddings via frozen CLIP text encoder [batch, 8, 512]
    \item Sample random timestep $t \sim \text{Uniform}(0, 1000)$ for each image
    \item Add Gaussian noise to images: $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$
    \item Predict noise: $\epsilon_\theta(x_t, t, c_\text{text})$ using UNet with cross-attention conditioning
    \item Calculate MSE loss: $\mathcal{L} = \|\epsilon - \epsilon_\theta(x_t, t, c_\text{text})\|^2$
    \item Backpropagate and update UNet parameters only (CLIP remains frozen)
\end{enumerate}

\subsubsection{Inference and Classifier-Free Guidance}

\textbf{Basic Sampling (Without CFG):}
\begin{itemize}
    \item Initialize random noise tensor: $x_T \sim \mathcal{N}(0, I)$
    \item Encode text prompt via CLIP
    \item Iteratively denoise for 50 steps using DDPM scheduler
    \item Post-process: normalize output to [0, 255] for visualization
\end{itemize}

\textbf{Classifier-Free Guidance (CFG):}

CFG enables stronger text conditioning by computing both conditional and unconditional predictions:

\begin{enumerate}
    \item \textbf{Dual embeddings:}
    \begin{itemize}
        \item Conditional: Text prompt → CLIP embedding $c_\text{text}$
        \item Unconditional: Empty string "" → CLIP embedding $c_\text{null}$
    \end{itemize}
    \item \textbf{Guidance formula:}
    \begin{equation}
        \tilde{\boldsymbol{\epsilon}} = \boldsymbol{\epsilon}_\text{uncond} + w \cdot (\boldsymbol{\epsilon}_\text{cond} - \boldsymbol{\epsilon}_\text{uncond})
        \label{eq:cfg}
    \end{equation}
    where $w$ is the guidance scale, $\boldsymbol{\epsilon}_\text{uncond} = \text{UNet}(x_t, t, c_\text{null})$, and $\boldsymbol{\epsilon}_\text{cond} = \text{UNet}(x_t, t, c_\text{text})$.
    
    \textbf{Implementation in Experiment 1:}
        \footnote{The \texttt{.chunk(2)} operation splits the concatenated noise predictions (which contains both unconditional and conditional predictions stacked together) into two equal tensors. The guided noise prediction is then computed using Equation~\ref{eq:cfg}, which is passed to the scheduler's step function to iteratively denoise the latent representation.}
    \begin{verbatim}
    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
    noise_pred = noise_pred_uncond + guidance_scale * \
                 (noise_pred_text - noise_pred_uncond)
    \end{verbatim}
    
    \item \textbf{Effect:} Higher $w$ → stronger adherence to text prompt
\end{enumerate}

\textbf{Inference Parameters:}
\begin{itemize}
    \item \textbf{Scheduler:} DDPM with squared cosine schedule
    \item \textbf{Number of inference steps:} 50
    \item \textbf{Random seed:} 422 (for reproducibility)
    \item \textbf{Guidance scales tested:} $w \in \{0, 5, 10, 20, 50, 100\}$
\end{itemize}

\subsubsection{Results}

\textbf{Training Convergence:}

Figure~\ref{fig:mnist_loss} demonstrates the training convergence of the diffusion model over 20 epochs. The curve demonstrates rapid initial convergence followed by plateau, indicating successful optimization of the diffusion model’s noise prediction task.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth,keepaspectratio]{../figures/experiments/mnist_loss_graph.png}
    \caption[Training loss convergence over 20 epochs]{Training loss curve of experiment 1.}
    \label{fig:mnist_loss}
\end{figure}

\textbf{Generation Quality Assessment:}

Figure~\ref{fig:mnist_generation} presents a comprehensive evaluation of the model's generation capabilities across all digit classes and various guidance scales. This systematic comparison demonstrates how the guidance scale parameter $w$ influences both image quality and text-prompt adherence across different digit classes.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.70\textwidth,keepaspectratio]{../figures/experiments/mnist_generation.png}
    \caption[MNIST generation for all digits (0-9) across guidance scales $w=0$ to $w=20$]{Comprehensive generation results for all MNIST digit classes (0-9, rows) across different guidance scales ($w \in \{0, 1, 2, 3, 5, 10, 20\}$, columns). Each row shows generations for the prompt "A handwritten digit X" where X corresponds to the digit class. All images generated using 50 inference steps.}
    \label{fig:mnist_generation}
\end{figure}

\textbf{Extended Training Results:}

To evaluate the impact of extended training, we trained the model for 20 epochs and regenerated samples across all digit classes. Figure~\ref{fig:mnist_generation_20epochs} shows the improved generation quality achieved with extended training, demonstrating better convergence and more consistent digit generation across all classes.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth,keepaspectratio]{../figures/experiments/mnist_generation_20epochs.png}
    \caption[Improved MNIST generation after extended training (20 epochs)]{Generation results after 20 epochs of training for all MNIST digit classes (0-9, rows) across different guidance scales ($w \in \{0, 1, 2, 3, 5, 10, 20\}$, columns). All images generated using 50 inference steps. Compared to Figure~\ref{fig:mnist_generation}, the extended training produces sharper digits with improved text-prompt alignment and reduced artifacts, particularly noticeable at higher guidance scales.}
    \label{fig:mnist_generation_20epochs}
\end{figure}

\subsubsection{Guidance-Scale ($w$) Ablation Study}

We systematically evaluated the impact of guidance scale on generation quality:

\textbf{Tested Guidance Scales:}
\begin{itemize}
    \item $w = 0$: Unconditional generation (no text guidance)
    \item $w = 5$: Weak guidance
    \item $w = 10$: Moderate guidance
    \item $w = 20$: Strong guidance
    \item $w = 50$: Very strong guidance
    \item $w = 100$: Maximum guidance
\end{itemize}

\textbf{Evaluation Protocol:}
\begin{enumerate}
    \item Generate images with fixed prompt: "A handwritten digit \{0-9\}"
    \item Use fixed random seed (422) set within the generation function, ensuring consistent initial noise for each guidance scale comparison
    \item Apply 50 denoising steps per image
    \item Qualitatively assess:
    \begin{itemize}
        \item Image clarity and sharpness
        \item Adherence to text prompt (correct digit class)
        \item Presence of artifacts or distortions
        \item Overall sample quality
    \end{itemize}
\end{enumerate}

\textbf{Expected Results:}
\begin{itemize}
    \item $w = 0$: Random digit generation (unconditional)
    \item $w = 5$-$10$: Balanced quality and prompt following
    \item $w = 20$-$50$: Strong prompt adherence with sharp outputs
    \item $w = 100$: Potential over-saturation and artifacts
\end{itemize}

\subsubsection{Visualization}

Generated images displayed using matplotlib with:
\begin{itemize}
    \item Grayscale colormap
    \item Grid layout for guidance scale comparison (1 row $\times$ 6 columns)
    \item Individual subplot titles showing guidance scale values
    \item Tight layout to prevent overlap
\end{itemize}
