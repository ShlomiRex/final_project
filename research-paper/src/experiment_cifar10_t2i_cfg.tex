\subsection{CIFAR-10 Text-to-Image Generation with Classifier-Free Guidance}

\subsubsection{Objective}
Building upon the success of the MNIST experiment, this experiment extends the text-to-image diffusion framework to CIFAR-10, a more challenging dataset with natural color images. CIFAR-10 contains 32$\times$32 RGB images across 10 object classes, presenting significantly greater complexity than grayscale handwritten digits. This experiment evaluates whether the same architectural principles and classifier-free guidance approach can scale to more realistic image generation tasks.

\subsubsection{Model Architecture}

\textbf{Text Encoder (Frozen):}
\begin{itemize}
    \item \textbf{Model:} CLIP (openai/clip-vit-base-patch32)
    \item \textbf{Embedding dimension:} 512
    \item \textbf{Tokenizer max length:} 77 tokens (standard CLIP length)
    \item \textbf{Training:} Weights are frozen
\end{itemize}

\textbf{Denoising Network (U-Net):}

The U-Net architecture is scaled up compared to the MNIST experiment to handle the increased complexity of natural color images.

\begin{itemize}
    \item \textbf{Architecture:} Custom UNet2DConditionModel for CIFAR-10
    \item \textbf{Input/Output:} 3 channels (RGB), 32$\times$32 pixels
    \item \textbf{Block channels:} (128, 256, 256, 512) --- larger than MNIST to capture natural image features
    \item \textbf{Layers per block:} 2
    \item \textbf{Down blocks:} DownBlock2D $\rightarrow$ CrossAttnDownBlock2D $\rightarrow$ CrossAttnDownBlock2D $\rightarrow$ DownBlock2D
    \item \textbf{Up blocks:} UpBlock2D $\rightarrow$ CrossAttnUpBlock2D $\rightarrow$ CrossAttnUpBlock2D $\rightarrow$ UpBlock2D
    \item \textbf{Cross-attention dimension:} 512 (matches CLIP embedding size)
    \item \textbf{Attention head dimension:} 32
    \item \textbf{Total trainable parameters:} $\sim$45 million (significantly larger than MNIST model)
\end{itemize}

\subsubsection{Dataset}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth,keepaspectratio]{../figures/experiments/experiment_2/training_samples.png}
    \caption[Example CIFAR-10 training images]{Example images from the CIFAR-10 training set.}
    \label{fig:cifar10_training_samples}
\end{figure}

\textbf{CIFAR-10 Dataset:}
\begin{itemize}
    \item \textbf{Training images:} 50,000
    \item \textbf{Test images:} 10,000
    \item \textbf{Resolution:} 32$\times$32 pixels, RGB color
    \item \textbf{Classes:} 10 object categories:
    \begin{multicols}{3}
    \begin{enumerate}
        \item airplane
        \item automobile
        \item bird
        \item cat
        \item deer
        \item dog
        \item frog
        \item horse
        \item ship
        \item truck
    \end{enumerate}
    \end{multicols}
    \item \textbf{Text captions:} Automatically generated as ``A photo of a \{class\_name\}'' (e.g., ``A photo of a cat'')
    \item \textbf{Preprocessing:} Normalized to [-1, 1] range for diffusion training
\end{itemize}

\subsubsection{Training Configuration}

\begin{itemize}
    \item \textbf{Batch size:} 128 (reduced from MNIST due to larger model and RGB images)
    \item \textbf{Learning rate:} $10^{-4}$
    \item \textbf{Optimizer:} AdamW with weight decay 0.01
    \item \textbf{Epochs:} 50
    \item \textbf{Noise scheduler:} DDPM with linear beta schedule
    \item \textbf{Beta range:} $\beta_{\text{start}} = 0.0001$, $\beta_{\text{end}} = 0.02$
    \item \textbf{Timesteps:} 1,000
    \item \textbf{Loss function:} Mean Squared Error (MSE) between predicted and actual noise
    \item \textbf{Unconditional dropout:} 10\% (for classifier-free guidance training)
\end{itemize}

\textbf{Training Pipeline per Batch:}
\begin{enumerate}
    \item Convert class labels to text captions using the prompt template ``A photo of a \{class\_name\}'' (e.g., ``A photo of a cat''), then tokenize using the CLIP tokenizer
    \item Encode captions to semantic embeddings via frozen CLIP text encoder [batch, 77, 512]
    \item With 10\% probability, replace text embedding with null embedding (empty string) for CFG training
    \item Sample random timestep $t \sim \text{Uniform}(0, 1000)$ for each image
    \item Add Gaussian noise to images: $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$
    \item Predict noise: $\epsilon_\theta(x_t, t, c_{\text{text}})$ using UNet with cross-attention conditioning
    \item Calculate MSE loss: $\mathcal{L} = \|\epsilon - \epsilon_\theta(x_t, t, c_{\text{text}})\|^2$
    \item Backpropagate and update UNet parameters only (CLIP remains frozen)
\end{enumerate}

\subsubsection{Inference Configuration}

\begin{itemize}
    \item \textbf{Scheduler:} DDPM with linear beta schedule
    \item \textbf{Number of inference steps:} 50
    \item \textbf{Guidance scales tested:} $w \in \{0, 2, 5, 10\}$
\end{itemize}

\subsubsection{Evaluation Metrics}

Two complementary metrics were used to evaluate generation quality:

\textbf{1. Fr√©chet Inception Distance (FID):}
\begin{itemize}
    \item Measures distributional similarity between generated and real images
    \item Computed using pytorch-fid with Inception-v3 features (2048 dimensions)
    \item Lower FID indicates better image quality and diversity
    \item 1,000 generated images compared against 1,000 real CIFAR-10 test images
\end{itemize}

\textbf{2. Classification Accuracy:}
\begin{itemize}
    \item Measures prompt adherence using a ResNet-18 classifier adapted from ImageNet (not fine-tuned on CIFAR-10). The classifier's first convolution and final layer were modified for 32$\times$32 images and 10 classes, but no additional training was performed. Thus, the absolute accuracy values are underestimated, but relative differences across guidance scales remain meaningful.
    \item Generated images classified and compared to intended class from prompt
    \item Higher accuracy indicates better text-image alignment
    \item 100 images per class (1,000 total) evaluated per guidance scale
\end{itemize}


\subsubsection{Results}

To qualitatively assess the effect of guidance scale on prompt adherence and sample diversity, Figures~\ref{fig:cifar10_samples_w0}--\ref{fig:cifar10_samples_w10} show $10\times10$ grids of generated CIFAR-10 images for all 10 classes at guidance scales $w=0,2,5,10$. Each row corresponds to a class, and each column shows a different generated sample. As the guidance scale increases, the images become more class-specific and less diverse. The unconditional case ($w=0$) produces class-agnostic images, while higher guidance scales yield more recognizable class features but reduced variation within each class.
Notably, higher guidance scales also tend to introduce more visual artifacts, such as unnaturally bright images and overconfident or exaggerated features, reflecting the model's increased emphasis on matching the prompt at the expense of image realism.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth,keepaspectratio]{../figures/experiments/experiment_2/samples_w_0.png}
    \caption[Generated CIFAR-10 images for all 10 classes at guidance scale $w=0$]{Generated CIFAR-10 images for all 10 classes ($10\times10$ grid) using guidance scale $w=0$ (unconditional). Each row shows 10 samples for a class.}
    \label{fig:cifar10_samples_w0}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth,keepaspectratio]{../figures/experiments/experiment_2/samples_w_2.png}
    \caption[Generated CIFAR-10 images for all 10 classes at guidance scale $w=2$]{Generated CIFAR-10 images for all 10 classes ($10\times10$ grid) using guidance scale $w=2$. Each row shows 10 samples for a class.}
    \label{fig:cifar10_samples_w2}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth,keepaspectratio]{../figures/experiments/experiment_2/samples_w_5.png}
    \caption[Generated CIFAR-10 images for all 10 classes at guidance scale $w=5$]{Generated CIFAR-10 images for all 10 classes ($10\times10$ grid) using guidance scale $w=5$. Each row shows 10 samples for a class.}
    \label{fig:cifar10_samples_w5}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth,keepaspectratio]{../figures/experiments/experiment_2/samples_w_10.png}
    \caption[Generated CIFAR-10 images for all 10 classes at guidance scale $w=10$]{Generated CIFAR-10 images for all 10 classes ($10\times10$ grid) using guidance scale $w=10$. Each row shows 10 samples for a class.}
    \label{fig:cifar10_samples_w10}
\end{figure}

% Showcase representative generated samples at guidance scale 5
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth,keepaspectratio]{../figures/experiments/experiment_2/samples.png}
    \caption[Representative CIFAR-10 samples at $w=5$]{Representative generated CIFAR-10 images using guidance scale $w=5$ (prompt template: ``A photo of a \{class\_name\}''). The figure showcases a diverse set of samples across all classes, illustrating the model's ability to generate class-conditional images with moderate prompt adherence and visual quality.}
    \label{fig:cifar10_samples_representative}
\end{figure}

\begin{table}[h]
    \centering
    \caption{CIFAR-10 Generation Metrics Across Guidance Scales. Accuracy measures prompt adherence.}
    \label{tab:cifar10_metrics}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Guidance Scale ($w$)} & \textbf{FID Score} $\downarrow$ & \textbf{Accuracy (\%)}$^{\ast}$ $\uparrow$ \\
        \hline
        0 (unconditional) & 77.05 & 9.10 \\
        \hline
        2 & \textbf{56.28} & 15.40 \\
        \hline
        5 & 63.13 & 15.00 \\
        \hline
        10 & 77.19 & \textbf{16.50} \\
        \hline
    \end{tabular}
    
    \vspace{0.5em}
    \footnotesize{$^\ast$Accuracy is measured using a ResNet-18 classifier adapted from ImageNet but not fine-tuned on CIFAR-10. The classifier's first convolution and final layer were modified for 32$\times$32 images and 10 classes, but no additional training was performed. Thus, the absolute accuracy values are underestimated, but the relative trend shows that higher guidance scales yield better prompt adherence, as expected.}
\end{table}
