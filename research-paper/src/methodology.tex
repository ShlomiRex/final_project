\section{Methodology}
\label{sec:methodology}

\sloppy % Allow LaTeX to adjust spacing to avoid Overfull \hbox warnings

This section outlines the methodology for training text-conditioned diffusion models, detailing the model architecture, training procedure, and inference strategies.

\subsection{Model Architecture}

Our approach builds upon the Denoising Diffusion Probabilistic Model (DDPM) framework, extended with cross-attention conditioning for text-to-image generation.

\subsubsection{UNet2DConditionModel}
We employ a custom U-Net architecture with cross-attention layers for text conditioning:

\begin{itemize}
    \item \textbf{Input/Output:} 1-channel grayscale images at 28×28 resolution
    \item \textbf{Block structure:} Reduced parameter configuration with \texttt{block\_out\_channels=(32, \allowbreak 64, \allowbreak 64, \allowbreak 32)}
    \item \textbf{Layers per block:} 2 layers for computational efficiency
    \item \textbf{Down-sampling blocks:} DownBlock2D → CrossAttnDownBlock2D → CrossAttnDownBlock2D → DownBlock2D
    \item \textbf{Up-sampling blocks:} UpBlock2D → CrossAttnUpBlock2D → CrossAttnUpBlock2D → UpBlock2D
    \item \textbf{Cross-attention dimension:} 512 (matching CLIP hidden size)
\end{itemize}

\subsubsection{Text Encoder}
We use the pretrained CLIP ViT-B/32 model (\texttt{openai/clip-vit-base-patch32}) for text encoding:

\begin{itemize}
    \item \textbf{Hidden size:} 512 dimensions
    \item \textbf{Max sequence length:} 8 tokens (reduced from default 77 for MNIST digit descriptions)
    \item \textbf{Training strategy:} Frozen weights during diffusion model training
\end{itemize}

\subsection{Training Procedure}

\subsubsection{Dataset and Preprocessing}
We use the MNIST dataset with automatic text caption generation:
\begin{itemize}
    \item \textbf{Captions:} "A handwritten digit \{label\}" where label $\in$ \{0, 1, ..., 9\}
    \item \textbf{Image preprocessing:} ToTensor() transformation (normalization to [0, 1])
    \item \textbf{Batch size:} 512 samples
\end{itemize}

\subsubsection{Noise Schedule}
We employ the squared cosine schedule (\texttt{squaredcos\_cap\_v2}) with 1000 diffusion timesteps, which provides:
\begin{itemize}
    \item Smoother noise progression
    \item Better preservation of signal at early timesteps
    \item Improved training stability
\end{itemize}

\subsubsection{Training Algorithm}
For each training iteration:

\begin{enumerate}
    \item Encode text captions using CLIP tokenizer and text encoder: $\mathbf{c} = \text{TextEncoder}(\text{Tokenize}(caption))$
    \item Sample random timestep $t \sim \mathcal{U}(0, 999)$
    \item Add noise to images: $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}$ where $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$
    \item Predict noise: $\boldsymbol{\epsilon}_\theta = \text{UNet}(\mathbf{x}_t, t, \mathbf{c})$
    \item Compute MSE loss: $\mathcal{L} = \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta\|^2$
    \item Update UNet parameters via backpropagation
\end{enumerate}

\textbf{Optimizer:} AdamW with learning rate $\lambda = 10^{-3}$

\subsection{Inference with Classifier-Free Guidance}

\subsubsection{Classifier-Free Guidance (CFG)}
To improve text-image alignment during generation, we implement classifier-free guidance:

\begin{enumerate}
    \item Encode both conditional text prompt and empty string "": 
    $$\mathbf{c}_{\text{text}} = \text{TextEncoder}(\text{prompt}), \quad \mathbf{c}_{\emptyset} = \text{TextEncoder}("")$$
    \item For each denoising step, predict noise for both conditions:
    $$\boldsymbol{\epsilon}_{\text{cond}} = \text{UNet}(\mathbf{x}_t, t, \mathbf{c}_{\text{text}})$$
    $$\boldsymbol{\epsilon}_{\text{uncond}} = \text{UNet}(\mathbf{x}_t, t, \mathbf{c}_{\emptyset})$$
    \item Combine predictions with guidance scale $w$:
    $$\tilde{\boldsymbol{\epsilon}} = \boldsymbol{\epsilon}_{\text{uncond}} + w \cdot (\boldsymbol{\epsilon}_{\text{cond}} - \boldsymbol{\epsilon}_{\text{uncond}})$$
    \item Update latents using the scheduler's step function
\end{enumerate}

\textbf{Guidance scale $w$:} Controls the strength of text conditioning. Higher values increase adherence to the text prompt but may reduce sample diversity.

\subsubsection{Sampling Parameters}
\begin{itemize}
    \item \textbf{Number of inference steps:} 50 (reduced from 1000 for faster generation)
    \item \textbf{Scheduler:} DDPM with squared cosine schedule
    \item \textbf{Initial noise:} $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$ with fixed random seed for reproducibility
\end{itemize}

\subsection{Limitations}
\begin{itemize}
    \item \textbf{Resolution:} Limited to 28×28 pixels due to MNIST dataset constraints
    \item \textbf{Domain:} Single-domain (handwritten digits) rather than general image generation
    \item \textbf{Computational resources:} Training conducted on single GPU for baseline experiments
\end{itemize}